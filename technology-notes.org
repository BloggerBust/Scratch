* Node
- process.env.NODE_ENV :: environment variable for configuring what environment node should run in i.e. dev, prod etc.
** npm
   - node package-manager
   - npm help <command> :: brings up help documentation on the specified command
** Node Modules
*** Bower
  - Install: $npm install -g bower
  - Help:
    - $ bower help
    - help on specific command : $ bower help command_name
  - Bower Lookup:
   Lets you see the git uri for a specific package
    - $bower lookup lodash
  - Bower Install: 
    - current version: $ bower install lodash
    - specific version: $ bower install lodash#2.2.1
    - save package to bower.json dependencies: $ bower install lodash --save
    - install package to dev only: $ bower install lodash --save-dev
    - install from cache (offline mode): $ bower install lodash -o
    - install from local repository: $ bower install relative-path/ProjectDirectoryName
    - install regular dependencies: $ bower install --production
    - install to named folder: $ bower install named_dir=lodash
  - Bower Uninstall: 
    - $ bower uninstall lodash
    - $ Remove from dependencies: bower uninstall lodash --save 
    - $ Remove from dev dependencies: bower uninstall lodash --save-dev
  - Bower Package Info: $ bower info lodash
  - Bower Update:
    - Update all packages: $bower update
    - Update single package: Just use Bower Install. It will update already installed packages.
  - Bower List:
    - list already installed packages: $bower list
  - Prune:
    Removes all packages that are not indicated in the bower.json file or are not sub-dependencies of existing dependencies.
    - $ bower prune
  - Bower Registery Search:
    - find all packages with the specified word: $bower search lodash
    - find package by keyword: go to the bower site http://bower.io/search/
  - Bower Init:
    - Used to create the bower json file: $ bower init
  - Bower RC:
    - Create a file called .bowerrc that contains a json with a single property named "directory"
      {
        "directory": "js/lib"
      }
      This specifies where bower should install packages.
    - Have bower install to more than one directory by creating more than one init point. So you could have a sub directory called test and throw a bower.json and .bowerrc file in there for managing dependencies in directory test/js/lib.
  - Bower Cache
    - List what is in the cache: $bower cache list
    - Clean the cache: $bower cache clean
  - Bower register
    To register a repository
    - $ git register https://github.com/TrevorWilsonSS/[reponame]
    - It will then ask if it can register the package with bower.herokuapp.com.
*** nodemon
    - Run a node application reloads modified files on the fly.
    - usage :: $ nodemon main-file.js
*** Stylus
    - A CSS preprocessor
    - styl :: file extension
*** Jade
  - The view engine used by express applications
  - jade :: file extension
  - doctype :: specifies that the jade file will contain HTML5
  - // :: a single line comment
*** toastr
*** ExpressJS
*** Morgan
    - Http request logger middleware
*** Body-parser
    - Body parser middleware
*** Mongoose
  - Fascilitates implementation of mongo-db in node applications
  - makes implementation of mongo much easier.
  - works off of schemas. Since mongo-db is a schema list document database, implementing schemas with mongoose is often seen as a bad idea.
    
* Angular JS
** Isolate Scope
- Local Scope Properties: Used to allow information flow between isolate and parent scope. Note that the alternate name option applies to all local scope properties, but is only illustrated with the @ below.
  - @: one-way binding of string values
    - directive usage:
      scope: {
    name: '@'
    value: '@someOtherAttrName'
    }
    - consumer usage:
    <div my-isolate-scope-with-name name: '{{customer.name}}' someOtherAttrName='{{constomer.value}}'></div>
  - =: two-way binding of objects
    - directive usage:
   scope: {
   customer: '='    
   },
    template: '<ul><li ng-repeat="prop in customer">{{prop}}</li></ul>
    - consumer usage:
      <div my-isolate-scope-with-model customer="customer"></div>
  - &: function binding for call-backs
    - directive usage:
      scope{
      action:'&'
      }
    - consumer usage:
      <div my-isolate-scope-with-function action="doStuff()" />
* Mongo-db
  - A schema list document database
  - No schema to define.
  - No relationship between collections of objects.
  - Objects can be flat or structured.
  - Two documents in same collection can be different from each other since no schema governs the collection.
    + Scalability
      - Single document write scope. Documents live in a collection, but updating a document occurs one at a time.
      - No need to extend locks accross collections because there are no relationships to enforce.
      - Eventual consistency. Mongo does not lock accross multiple mongo servers. A repleca set in mongo contains a single server that will handle all writes and a collection of secondary servers that will be replecated to. There is a lag of time from when a write occurs in the Primary DB to when the value is made observable by others by being replicated in a secondary db; hence, eventual consistancy.
      - Can choose consistancy model: 
        - Can choose to wait for primary write server to persist data
        - To wait for all replica servers to sync with the primary server following the write.
        - To wait for a majority of replica servers to sync with the primary server following the write.
        - Choose to hand over document to primary and not care wether it persisted or not.
      - Capped Collections:
        - Fixed size :: no time to allocate space
        - Auto override all documents
    + Mongod
      - The daemon.
      - Default Port: 27017
      - mongod help :: help documentation for commandline options
    + Mongos
      - The sharding server.
  - Mongo Client
    - mongo :: starts the client
    - help :: brings up client help documentation
    - exit :: quits the client
    - show dbs :: lists existing dbs
    - db :: shows the current database
    - use foo :: switches to database few and creates it if it does not exist.
    - db.getMongo() :: returns host and port for that server instance.
  - Replica Sets
    - Advantages :: scalability and automatic recovery.
    - Types :: Primary, Secondary, Arbiter
    - Primary
      - One and only primary instance. 
    - Secondary
      - Readonly
      - one to many
      - Data is replicated from primary. Gaurantees eventual consistancy.
      - If Primary database fails, one of the seconary databases will take over and become the primary. This is descided by an election.
      - Nothing special happens if a secondary db fails. If one secondary fails and there are others than no big deal. Haveing multiple Secondaries protects against single server failure.
    - Arbiter
      - sole purpose is to break ties on primary db elections.
      - is not a database. It contains no data.
    - Minimal replica set :: Primary DB, 1 Secondary DB, 0 or 1 Arbiters
    - Dev can run a replica set on a single machine. Production should run each mongo server per machine
    - Creation of single machine replica set:
      - Each mongo server requires its own db directory. i.e. db{1,2,3}
      - Each mongo server must run on a different port
      - mongod --dbpath ./db{n} --port unique_port_num --replSet "<replicaSetName>"
** TODO Install Mongo-db Minimal ReplicaSet
   - mongod -f "e:\dev\experiments\MultiVision\conf\mongod1.conf" --replSet "Experiments" --install
   - Windows instructions to get the replica set running as a service.
* jquery
* git
** Steps to create a new Github Rep
 1. Login to github and create the repository
 2. Copy the ssh path to the new repository
 3. Follow this command pattern:
    - $ mkdir myProj
    - $ cd myProj
    - $ git init
    - touch README.md
    - touch .gitignore
    - git add -A
    - git commit -m "my first checkin"
    - git remote add origin [paste copied git repo uri here]
    - git push -u origin master :: //MUST BE IN INTERACTIVE SHELL TO PROVIDE CREDENTIALS
    
** Steps to tag and push a new release
 1. cd myProj
 2. command pattern:
    - git tag 0.0.1 :: Should be the same version as entered in bower.json "version" property
    - git push --tags
 3. Now visit github, click on the project and then click on the release link to view the release.

* Linux 
** Commands :: remember to always check the man pages.
  - lsblk :: list block devices.
  - lddtree --help :: there are no man pages for this command
  - lddtree --copy-to-tree=/source/path /target/path :: from app-misc/pax-utils USE="python"
  - find /usr/portage -name '*.ebuild' -o -name '*.eclass' | xargs grep MAKE_CONF_VARIABLE :: finds all package references to MAKE_CONF_VARIABLE
  - lspci | grep -i vga :: detect video controller
*** Detect Motherboard, Bios and CPU
   - dmidecode -t 4 | grep ID :: The CPU ID
   - dmidecode -t 0 :: Bios info
   - dmidecode -t 4 :: Processor info
   - dmidecode -t 11 :: Original Equipment Manufacturer (OEM) info
** I/O Stream Numbers
   | Handle | Name   | Description     |
   |      0 | stdin  | Standard input  |
   |      1 | stdout | Standard output |
   |      2 | stderr | Standard error  |
   - =$ program-name 2> error.log= :: Redirect standard error stream to a file
   - =$ program-name &>file= :: Redirect the standard error (stderr) and stdout to file
     =$ program-name > file-name 2>&1= :: Alternate redirect the standard error (stderr) and stdout to file

** Boot Process
  1. Boot loader loads Linux.
  2. Linux assumes control of the system.
  3. Linux prepares its memory structures and drivers
  4. Hands control to Init.
  5. Init makes sure that at the end of the boot process, all necessary services are running and the user is able to log in.
  6. Init launches udev daemon which will further load up and prepare the system based on the detected devices.
  7. Udev mounts the remaining file systems waiting to be mounted.
  8. Udev starts the remaining services waiting to be started.
      
** Initramfs
*** The Initial RAM File System
  - Based on tmpfs
  - Because it is a size-flexible, in-memory lightweight file system it does not use a seperate block device so no caching was done. It does not have the overhead of an entire file system.
  - Contains the tools and scripts needed to mount the file systems before the init binary on the real root file system is called.
    - The tools can be the decryption abstraction layers (for encrypted file systems), logical volume mangers, software raid, bluetooth driver based file system loaders, etc.
  - All files, tools, libraries, configuration settings (if applicable), etc are put into a cpio archive.
*** From creation to execution
  1. The cpio archive is compressed using gzip and stored in the /boot partition along side the linux kernel.
  2. The boot loader will let the linux kernel know where the cpio archive is at boot time so that the kernel can load the initramfs.
  3. The Linux kernel will create a tmpfs file system, extract the contents of the cpio archive into it, and then launch the init script located in the root of the tmpfs file system.
  4. The init script will then perform what ever tasks are necessary to ensure that it will be able to mount the real root file system.
    - It may have to decrypt the real root file system, other vital file systems, and mount them among possibly other things depending on what is needed.
  5. The init script from the initramfs will then switch the root towards the real root file system
  6. Lastly the initramfs init script will call /sbin/init (the init script on the real root file system)
  7. The boot process will continue as normal.
** Gentoo
*** Use Variables
    - If a use variable has a * by it that means that it changed since the last build.
*** Hardened Profile
**** PIC (Position Independent Code)
     - Functions and data are accessed through an indirect table called the Global Offset Table (GOT).
     - The purpose of indirect addressing is to fascillitate the access of functions and data independently of the corresponding load address. Only the symbols in the text segment exported in the GOT need updating at run-time deending on the current load address of the various shared libraries in the address space of the running process.
     - Similarly, procedure calls to globally defined functions are redirected through the "Procedure Linkage Table" (PLT) residing in the data segment of the core image. This avoids runtime modifications of the text segment.
     - The Linker-editor allocates the GOT and PLT when combining the PIC object files into an image for mapping into the process address space.
     - The Linker-editor collects all symbols that may be needed by the run-time link-editor and stores these along with the image's text and data bits.
     - Objects compiled as PIC allow the OS to load the object at any address in preperation for execution with slight overhead.
     - The libtool builds PIC objects for use in shared libraries and non-PIC objects for use in static libraries. PIC compilation is required for objects in a shared library.
     - libtool compiles PIC objects with '*.lo' extension and non-PIC objects with '*.o' extension.
     - In practice PIC objects can be linked into static archive and often non-PIC objects can be similarly linked into shared archives both with execution and load speed overhead.
     - If the shared object is built from code that is not PIC then the text segment will usually require a large number of relocations to be performed at runtime. The system overhead from the run-time linker required to handle this can cause serious performance degradation.
     - =# readelf -d foo= :: If the output contains a TEXTREL entry then text relocations exist.
**** PaX
     - Purpose is to protect against a class of exploits that give an attacker arbitrary read / write access to the attacked task's address space. These exploits include buffer and heap overflows and similar attacks. PaX is the first line of defense offered by Hardened Gentoo.
     - Implements the least privilege protections for memory pages. i.e computer programs should only  be allowed to do what they have to do in order to be able to execute properly and nothing more.
     - The exploit techniques that PaX defends against include:
       1. Introduce / execute arbitrary code
       2. execute existing code out of original program order
       3. execute existing code in original program order with arbitrary data
     - References:
       - Gentoo Hardened Introduction :: https://wiki.gentoo.org/wiki/Hardened/Introduction_to_Hardened_Gentoo#Technologies_Offered
       - Project site :: http://pax.grsecurity.net/
       - Quickstart :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart
     - Adds security enhancement to the area between both kernel and userland.
     - Patch to the kernel that provides hardening in the following ways:
       1. Judicious enforcement of non-executable memory
       2. Address Space Layout Randomization (ASLR)
          - Compiling with Position Independent Executable (PIE) allows ASLR to randomaize even the base address.
       3. Miscellaneous hardening on stack and memory handling
          - Erases stack frame when returning from a system call
          - refusing to dereference user-land pointers in some context
          - detecting overflows of certain reference counters
          - correcting overflows of some integer counters
          - enforcing the size on copies between kernel and user land
          - providing extra entropy
     - PaX Modes
       - SOFTMODE
         - PaX protection will not be enforced by default for those features which can be turned on or off at runtime.
         - The "permit by default" mode.
         - The user must explicitly mark executables to enforce PaX protection.
       - non-SOFTMODE
         - PaX protections are immediately activated.
         - The "forbid by default" mode.
         - The user must explicitly mark binaries to relax PaX protection selectively.
     - PaX Configurable Features
       - Enforce non-executable pages :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Enforce_non-executable_pages
       - Enhanced Address Space Layout Randomization (ASLR) :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Enhanced_Address_Space_Layout_Randomization_.28ASLR.29
       - Miscellaneous Memory Protection :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Miscellaneous_Memory_Protection
     - PaX patches support three ways of doing PaX markings:
       1. EI_PAX
          - This option is nolonger supported
          - Places PaX flags in bytes 14 and 15 of the e_ident field of an ELF objects header.
       2. PT_PAX
          - Places the flags in an ELF object's program header called PAX_FLAGS.
          - Flags are in the body of the object and so if the object is moved or copied the flags are also.
          - The object must have the PAX_FLAGS program header to work. Most Linux distrobutions do not build their executables and libraries with this program header.
       3. XATTR_PAX
          - This is the preferred approach.
          - PaX flags are palced in the file system's extended attributes.
          - Does not modify the ELF object.
          - The file system and utilities used to copy, move and archive files must support xattrs.
            - Must support user.pax.* namespace in which the PaX flags are placed.
            - Do not enable the entire user.* namespace because it may open attackvectors.
            - Must support security.*, trusted.* namespaces.
     - Building a PaX Kernel
       - Reference :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Building_a_PaX_Kernel

*** ZFS
    - References:
      - https://wiki.gentoo.org/wiki/ZFS/Features :: Detailed list of features
      - https://wiki.gentoo.org/wiki/ZFS :: Gentoo wiki guide
      - https://pthree.org/2012/12/04/zfs-administration-part-i-vdevs/ :: Administraiton Guide
      - http://docs.oracle.com/cd/E19253-01/819-5461/ :: Official Oracle Docs
    - ARC :: Adaptive Replacement Cache
    - ARC page replacement agolrithm is used instead of the Last Recently used page replacement algorithm.
    - Minumum and Maximum memory usage allocated to ARC varies based on system memory.
      - Default Min :: 1/32 of all memory, or 64 MB, whichever is more.
      - Default Max :: the larger of 1/2 of system memory or 64 MB.
      - Linux accounts for memory used by arc differently than memory used by the page cache. Memory used by ARC is included under "used" not "cached" in the output used by the 'free' program. This can give the impression that ARC will use all of system memory if given opportunity.
      - ARC memory usage is tunable via zfs_arc_min and zfs_arc_max. These properties may be set in 3 ways:
        1. at runtime.
           - =root # echo 536870912 >> /sys/module/zfs/parameters/zfs_arc_max=
           - Changes through sysfs do not persist across boots.
           - The value in sysfs will be 0 when the value has not been manually configured.
           - The current setting can be viewed by looking at c_max in /proc/spl/kstat/zfs/arcstats
        2. via /etc/modprobe.d/zfs.conf
           - =root # "options zfs zfs_arc_max=536870912" >> /etc/modprobe.d/zfs.conf=
        3. Kernel command line
           - =zfs.zfs_arc_max=536870912=
      - Zpool Version Update
        - When sys-fs/zfs is updated likely the version of ZFS has been incremented. The status of zpools will indicate a warning that a newer version is available and that the zpools can be upgraded.
        - =root # zpool upgrade -v= :: Display current version on zpool
        - =root # zpool upgrade zfs_test= :: upgrade the version of zpool zfs_test.
        - =root # zpool upgrade -a= :: upgrade the version of all zpools in the system.
**** Virtual Devices (VDEVs)
     - A meta-device representing one or more physical devices.
     - 7 types of VDEVs:
       - Disk (default) :: The physical drives in your system.
       - File :: The absolute path of pre-allocated files/images.
       - Mirror :: Standard software RAID-1 mirror.
       - Spare :: Hard drives marked as a "hot spare" for ZFS software RAID.
       - Cache :: Device used for a level 2 adaptive read cache (L2ARC).
       - Log :: A seperate log (SLOG) called the "ZFS Intent Log" or ZIL.
     - VDEVs are dynamically striped.
     - Caveats
       - Devices cannot be removed from a VDEV
       - RAID-(n) is faster than RAID-(n+1)
       - Hot spares are not dynamically added unless configured to.
       - A zpool will not dynamically resize when larger disks fill the pool unless you enable the setting (off by default) BEFORE the first disk replacement.
       - Will know about "advanced format" 4K sector drives iif the drive reports such.
       - Duplication is EXTREMELY EXPENSIVE, will cause performance degredation if not enough RAM is available.
       - Duplication is pool-wide, not local to the filesystem.
       - Compression is EXTREMLY CHEAP on the CPU, yet it is disabled by default.
       - ZFS suffers a great deal from fragmentation. Full ZPOOLS will experience performance degredation
     - Creation
       - A Simple pool
         #+BEGIN_SRC sh
           # zpool create tank sde sdf
           # zpool status tank
 pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  sde       ONLINE       0     0     0
	  sdf       ONLINE       0     0     0
	  sdg       ONLINE       0     0     0
	  sdh       ONLINE       0     0     0

errors: No known data errors
         #+END_SRC
       - A simple mirrored zpool
         #+BEGIN_SRC sh
           # zpool create tank mirror sde sdf sdg sdh
           # zpool status tank
 pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    sde     ONLINE       0     0     0
	    sdf     ONLINE       0     0     0
	    sdg     ONLINE       0     0     0
	    sdh     ONLINE       0     0     0

errors: No known data errors
         #+END_SRC
       - Nested VDEVs
         #+BEGIN_SRC sh
           # zpool create tank mirror sde sdf mirror sdg sdh
           # zpool status
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    sde     ONLINE       0     0     0
	    sdf     ONLINE       0     0     0
	  mirror-1  ONLINE       0     0     0
	    sdg     ONLINE       0     0     0
	    sdh     ONLINE       0     0     0

errors: No known data errors
         
         #+END_SRC
       - File VDEVs (useful for experiments)
         - When creating cannot use relative paths.
         - The image file must be preallocated, not sparse or thin provisioned.
         #+BEGIN_SRC sh
         # for i in {1..4}; do dd if=/dev/zero of=/tmp/file$i bs=1G count=4 &> /dev/null; done
         # zpool create tank /tmp/file1 /tmp/file2 /tmp/file3 /tmp/file4
         # zpool status tank
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME          STATE     READ WRITE CKSUM
	tank          ONLINE       0     0     0
	  /tmp/file1  ONLINE       0     0     0
	  /tmp/file2  ONLINE       0     0     0
	  /tmp/file3  ONLINE       0     0     0
	  /tmp/file4  ONLINE       0     0     0

errors: No known data errors
         
         #+END_SRC
       - Hybrid pools
         - "tank" pool is composed of "mirror-0" and "mirror-1" VDEVs for long-term persistent storage. 
         - Neither the "logs" pool nor the "cache" pool are long-term storage for the pool, thus creating a "hybrid pool" steup.
         #+BEGIN_SRC sh
           # zpool create tank mirror /tmp/file1 /tmp/file2 mirror /tmp/file3 /tmp/file4 log mirror sde sdf cache sdg sdh
           # zpool status tank
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME            STATE     READ WRITE CKSUM
	tank            ONLINE       0     0     0
	  mirror-0      ONLINE       0     0     0
	    /tmp/file1  ONLINE       0     0     0
	    /tmp/file2  ONLINE       0     0     0
	  mirror-1      ONLINE       0     0     0
	    /tmp/file3  ONLINE       0     0     0
	    /tmp/file4  ONLINE       0     0     0
	logs
	  mirror-2      ONLINE       0     0     0
	    sde         ONLINE       0     0     0
	    sdf         ONLINE       0     0     0
	cache
	  sdg           ONLINE       0     0     0
	  sdh           ONLINE       0     0     0

errors: No known data errors
         
         #+END_SRC
         - In practice, use the device id found in /dev/disk/by-id when identifying the devices for the "logs" and "cache" pools. They may be assigned different device names from one boot to another, unlike devices in the main pool.
**** RAIDZ
     - References:
       - http://en.wikipedia.org/wiki/RAID
     - Disk Striping
       - References
         - http://en.wikipedia.org/wiki/Data_striping
       - The process of segmenting a body of logically sequential data into data blocks so that consecutive data blocks are spread accross multiple storage devices.
       - Storage systems vary in the way striping is performed. Data may be stripped at the byte, block or partition level, and may be stripped accross all or some of the disks in a cluster.
       - Main advantage is higher performance. By spreading data accross multiple devices that can be accessed concurrently, total throughput is increased.
       - Balances I/O load accross an array of disks.
     - Standard Parity RAID (i.e. RAID-5)
       - References:
         - http://blog.open-e.com/how-does-raid-5-work/
       - Consists of block level striping with distributed parity. The stripe width is thus statically set at creation.
       - Minumum of 3 disks. Data is stripped accross two disks. A pairity bit is calculated such that the XOR of all three stripes in the set calculate to zero. The parity bit is then written to the chosen 3rd disk.
       - No single disk is dedicated to parity. Two disk are chosen to be striped and the third is chosen for parity such that the distribution of parity bits is even accross all drives.
       - Resilant against a failure of any one disk. The data on the failed disk can be recalculated via the remaining disks and thus restored.
       - RAID-5 write hole
         - Caused by inturrupted destaging of writes to disk, such as a power failure.
         - Solutions to the RAID-5 write hole are either slow (software based) or expensive (hardware based).
         - RAID-5 has fallen out of favor as a result.
       - If the data being written to the strip is less than the stripe-size, much time is wasted reading the data on the rest of the stripe to ensure that the parity satisfies the constraint that the XOR of all three stripes in the set calculate to zero.
         - Thus data is read and written that is not pertinent to the application doing the work.
         - Expensive NVRAM hardware RAID cards can hide the latency from the end user.
       - The RAID-5 write hole and the performance impact of writing data smaller than the stripe size to disk motivated the ZFS team to re-think parity-based RAID.
     - ZFS RAIDZ
       - Stripe width dynamically allocated.
         - Every block stransactionally flushed to disk is its own stripe width.
         - Every RAIDZ write is a full stripe write.
         - The parity bit is flushed with the stripe simultaneously. This completely eliminates the possability of a write hole.
           - In the event of a power failure, either the latest data was flushed to disk or it wasn't, but the disks will not be inconsistent.
         - Cannot calculate parity simply by the rule -- every disk XORs to zero because the stripe size is dynamic with respect to the size of the datablock being written to disk.
           - ZFS metadata is used to determine the RAIDZ geometry on every read.
           - Reading file system metadata to contruct the RAID stripe means reading live running data only, not dead inpertinent or unallocated data.
           - No need for expensive NVRAM to buffer writes or for battery backup to protect against write hole.
     - Self-healing RAID
       - ZFS can detect silent errors and fix them on the fly (Not possible if RAID and filesystem are seperate)
         - When an application requests data ZFS constructs the stripe and compares each block against a default checksum in the metadata. If the read stripe does not match the checksum, ZFS finds the bad block, reads the parity and fixes it through combinatorial reconstruction and returns good data to the application.
         - If the stripe width is longer than the disks in the array and there is a disk failure there would not be enough data in the parity for combinatorial reconstruction, thus ZFS can mirror data in the stripe to prevent this from happening. :: Bad wording. I re-worded the original to try to make sense of it. Is this what was meant???
     - RAIDZ-{1,2,3}
       - RAIDZ-n, where n is the number of parity bits distributed accross all the disks in the array.
       - Stripe width is variable. Disk Array Size < Stripe width || Disk Array Size == Stripe Width || Stripe Width < Disk Array Size.
       - RAIDZ-n allows for n disk failures.
       - RAIDZ-n requires Disk Array Size = n + 2.
       - RAIDZ-n capacity = Disk Array Size * Capacity of smallest disk - n parity storage
     - Hybrid RAIDZ
       - A stripe of multiple RAIDZ VDEVS.
       - Increase performance at the cost of available disk storage.
       - Stripe width is variable within each nested RAIDZ VDEV.
       - Each RAIDZ level follows the same rules outlined in RAIDZ-{1,2,3} above.
         - A stripe of 3 RAIDZ-n VDEVS can suffer a total of 3*n disks. n per VDEV.
     - RAIDZ Benchmark :: Recommended to use the tool IOZone 3 to benchmark and stress the array.
       - Mirrors always outperform RAIDZ levels.
       - performs(RAIDZ-n) > performs(RAIDZ-{n-1})
       - More parity bits mean longer read / write times.
     
**** ZFS Intent Log (ZIL and SLOG)
     - References:
      https://pthree.org/2012/12/06/zfs-administration-part-iii-the-zfs-intent-log/
     - The Zil and the Seperate Intent Log (SLOG) log what is currently in system memory so that the system may recover in the event of a power loss. This can be setup at anytime so I am going to skip it for now.
**** The Adjustable Replacement Cache (ARC)
     - The purpose of this is to cache data for quick retrieval to increase performance. This is a good idea, but can be added later so I am going to skip this for now.
**** Exporting and Importing Zpools
     - Exporting Storage Pools
       - When exporting the following happens:
         - Causes the kernel to flush all pending data to disk
         - Writes data to the disk acknowledging that the export was done
         - Removes all knowledge that the storage pool existed in the system
       - Exporting the storage pool is necessary before importing the storage pool into a new system. Also, unwritten data may not have been flushed to disk.
       - == root # zpool export tank ==
         - Will attempt to unmount all ZFS datasets as well as the pool
         - If the zpool refuses to export you can add -f to force the export.
     - Importing Storage Pools
       - Once the drives have been physically installed they may be imported.
       - == root # zpool import tank == :: Imports the zpool tank
       - Once imported it is a good idea to check the status of the zpool
       - == root # zpool status tank
         - A status of ONLINE means that everything is healthy
         - Status FAULTED :: means one or more drives appear faulty to the system
         - Refer to documentation for troubleshooting status codes
       - Import multiple zpools by listing them after the import command delimited by white space, or by passing the -a flag for importing all known zpools.
         - == root # zpool import tank1 tank2 tank3
         - == root # zpool import -a
     - Recovering a Destroyed Pool
       - Destroying a pool does not wipe the data on the disks. The pool can be discovered.
       - == (server A) root # zpool destroy tank == :: Does not wipe data
         == (server B) root # zpool import -D == :: Lists destroyed pools.
         - Run the import command again specifying the pool name to bring it fully online.
         - If more than one storage pool is found with the same name then the unique identifier of the storage pool must be used as the argument to import to bring it fully online.
     - Upgrading Storage Pools
       - Once a zpool has been upgraded servers running older versions of zpool will not be able to import it.
       - There is no way to downgrade.
       - == root # zpool upgrade -v == :: Outputs the version of ZFS pool that the system is currently running. Display a list of supported zpool versions and some descriptions for each.
       - == root # zpool upgrade -a == :: Upgrades the zpool and enables all supported features. See man page to upgrade to a specific version and feature set.
       - On shutdown the zfs init script may just unmount the pools and not export them in which case the zpools would not be able to be imported to by another system. If this is the case you will have to explicitly export the zpools first.

**** Scrub and Resilver
     - Standard Validation
       - integrity checking tools require the disks to be offline.
       - The filesystem knows nothing about the underlying data structures such as LVM or RAID.
       - Software RAID has no idea what disk contains good or bad data, so either could be served (silent data errors).
       - Nothing can be done about silent data errors.
     - ZFS Scrubbing
       - Scrubbing the disk is used to detect and correct silent data errors.
       - Scrubbing disks can be done on a live running system with no downtime.
       - The scrub involves checking every block in the storage pool against its known checksum using the "fletcher4" (default) 256-bit algorithm.
       - Must be performed explicitly.
       - Recommended that scrubbing is performed on a regularly scheduled interval (good job for chron).
       - == root # scrub tank == :: performs a scrub of the pool.
       - Can check status during a scrub.
       - Scrubs impact performance of disks.
       - == root # scrub -s tank == Stops a scrub in progress.
       - == 0 2 * * 0 /sbin/zpool scrub tank == crontab to perform scrub every Sunday at 2 in the morning.
     - Self Healing Data
       - Requires redundancy. i.e. mirror
       - Will not only detect corrupt data on a scrub, but will correct the bad blocks if good data exists on a different disk.
       - I think that this zfs can self heal in response to realtime interaction with an application and is not limited to explicit scrubs. (verify this)
     - Resilvering Data
       - References:
         - http://docs.oracle.com/cd/E19082-01/817-2271/gbbvf/index.html :: regarding damaged devices with ZFS
       - Same concept as rebuilding or resyncing data onto a new disk in the array.
       - With Software RAID there is no distinction between which blocks are live and which are not. The rebuild starts at the begining of the disk and does not stop until it reaches the end of the disk.
       - ZFS knows about the RAID structure and has a smarter algorithm for rebuilding the data.
         - Does not sync the free disk blocks, only live blocks. If storage pool is only partially filled this can save significant time.
         - In ZFS the process of rebuilding, resyncing or reconstructing is kown is Resilvering.
         - If zpool status is "DEGRADED" then a disk needs to be replaced. Identify the disk that needs to be replaced with teh following command
           #+BEGIN_SRC 
           # for i in a b c d e f g; do echo -n "/dev/sd$i: "; hdparm -I /dev/sd$i | awk '/Serial Number/ {print $3}'; done
/dev/sda: OCZ-9724MG8BII8G3255
/dev/sdb: OCZ-69ZO5475MT43KNTU
/dev/sdc: WD-WCAPD3307153
/dev/sdd: JP2940HD0K9RJC
/dev/sde: /dev/sde: No such file or directory
/dev/sdf: JP2940HD0SB8RC
/dev/sdg: S1D1C3WR
           #+END_SRC
           The above example shows that /dev/sde needs to be replaced since the command could not find the device.
         - After identifying the dead disk go to the storage array and find which serial number was not printed. The one that was not printed needs to be replaced. Pull the disk and replace it with a new one.
         - Restart the system and see if /dev/sde is repopulated by running the command again.
         - Then issue the following command to replace sde with the new disk at /dev/sde (note the new disk may not be /dev/sde in which case the command would be slightly different to reflect that)
           == root # zpool replace tank sde sde == Rebuilds the data blocks on the new disk until it is in a completely healthy state. Check the status to know when the process has completed.
     - Identifying Pool Problems
       - == zpool status -x == :: the x flag only displays the status of pools that are exhibiting errors or are unavailable.
       - The zpool status fields:
         + pool- The name of the pool.
         + state- The current health of the pool. This information refers only to the ability of the pool to provide the necessary replication level.
         + status- A description of what is wrong with the pool. This field is omitted if no problems are found.
         + action- A recommended action for repairing the errors. This field is an abbreviated form directing the user to one of the following sections. This field is omitted if no problems are found.
         + see- A reference to a knowledge article containing detailed repair information. Online articles are updated more often than this guide can be updated, and should always be referenced for the most up-to-date repair procedures. This field is omitted if no problems are found.
         + scrub- Identifies the current status of a scrub operation, which might include the date and time that the last scrub was completed, a scrub in progress, or if no scrubbing was requested.
         + errors- Identifies known data errors or the absence of known data errors.
         + config- Describes the configuration layout of the devices comprising the pool, as well as their state and any errors generated from the devices. The state can be one of the following: ONLINE, FAULTED, DEGRADED, UNAVAILABLE, or OFFLINE. If the state is anything but ONLINE, the fault tolerance of the pool has been compromised.
       - The columns in the status output, "READ", "WRITE" and "CHKSUM" are defined as follows:

         + NAME- The name of each VDEV in the pool, presented in a nested order.
         + STATE- The state of each VDEV in the pool. The state can be any of the states found in "config" above.
         + READ- I/O errors occurred while issuing a read request.
         + WRITE- I/O errors occurred while issuing a write request.
         + CHKSUM- Checksum errors. The device returned corrupted data as the result of a read request.

* TODO
  - Install Cron
  - Schedual Zpool scrub
