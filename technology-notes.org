* Node
- process.env.NODE_ENV :: environment variable for configuring what environment node should run in i.e. dev, prod etc.
** npm
   - node package-manager
   - npm help <command> :: brings up help documentation on the specified command
   - npm init :: interactivly creates package.json file
   - npm install <package> --save :: installs the package to the project and saves the dependency in the package.json file
   - npm install <package> -g :: save it globally. This allows the package to be run directly from the commandline as apposed to via node
** Node Modules
*** Bower
  - Install: $npm install -g bower
  - Help:
    - $ bower help
    - help on specific command : $ bower help command_name
  - Bower Lookup:
   Lets you see the git uri for a specific package
    - $bower lookup lodash
  - Bower Install: 
    - current version: $ bower install lodash
    - specific version: $ bower install lodash#2.2.1
    - save package to bower.json dependencies: $ bower install lodash --save
    - install package to dev only: $ bower install lodash --save-dev
    - install from cache (offline mode): $ bower install lodash -o
    - install from local repository: $ bower install relative-path/ProjectDirectoryName
    - install regular dependencies: $ bower install --production
    - install to named folder: $ bower install named_dir=lodash
  - Bower Uninstall: 
    - $ bower uninstall lodash
    - $ Remove from dependencies: bower uninstall lodash --save 
    - $ Remove from dev dependencies: bower uninstall lodash --save-dev
  - Bower Package Info: $ bower info lodash
  - Bower Update:
    - Update all packages: $bower update
    - Update single package: Just use Bower Install. It will update already installed packages.
  - Bower List:
    - list already installed packages: $bower list
  - Prune:
    Removes all packages that are not indicated in the bower.json file or are not sub-dependencies of existing dependencies.
    - $ bower prune
  - Bower Registery Search:
    - find all packages with the specified word: $bower search lodash
    - find package by keyword: go to the bower site http://bower.io/search/
  - Bower Init:
    - Used to create the bower json file: $ bower init
  - Bower RC:
    - Create a file called .bowerrc that contains a json with a single property named "directory"
      {
        "directory": "js/lib"
      }
      This specifies where bower should install packages.
    - Have bower install to more than one directory by creating more than one init point. So you could have a sub directory called test and throw a bower.json and .bowerrc file in there for managing dependencies in directory test/js/lib.
  - Bower Cache
    - List what is in the cache: $bower cache list
    - Clean the cache: $bower cache clean
  - Bower register
    To register a repository
    - $ git register https://github.com/TrevorWilsonSS/[reponame]
    - It will then ask if it can register the package with bower.herokuapp.com.
*** nodemon
    - Run a node application reloads modified files on the fly.
    - usage :: $ nodemon main-file.js         
*** gulp
    - install
      - first install globally and then install locally with dev dependency
      - npm install gulp -g
      - npm install gulp --save-dev
    - gulpfile.js :: gulp expects this file to exist as the main entry point
    - Simple gulp example to monitor for code changes
      #+BEGIN_SRC javascript
      var gulp = require('gulp'),
      nodemon = require('gulp-nodemon');
      
      //create the default task and use it to construct nodemon
      gulp.task('default', function(){
        nodemon({
          script: 'app.js', //what is it going to run
          ext: 'js', //what file  extensions to monitor for changes
          env: {
            PORT: 8000 //sets the process.env.PORT
          },
          ignore: [
            './node_modules/**' //ignore everything under node_modules. That is what the double ** means.
            ]
        })
        .on('restart', function(){
          console.log('we have restarted');
        });
      });
      #+END_SRC 
*** gulp-nodemon
    - a nodemon plugin for gulp
*** Stylus
    - A CSS preprocessor
    - styl :: file extension
*** Jade
  - The view engine used by express applications
  - jade :: file extension
  - doctype :: specifies that the jade file will contain HTML5
  - // :: a single line comment
*** toastr
*** ExpressJS
    - Steps to setup express server
      #+BEGIN_SRC javascript
      var express = require('express')
      var app = express(); // construct an express server instance
      var port = process.env.PORT || 3000; //use the environment port if it is defined, otherwise default to port 3000
      // setup a route handler
      app.get('/', function(req, res){
        req.send('welcome to my API'); //write a message back to the requester
      });

      //listen on the port for requests
      app.listen(port, function(){
        console.log('listening on port ' + port + '...');
      });
      #+END_SRC
    - node <server-file> :: Start an node express app
    - Setup routes
      - app.get can be used to setup simple routes
      - express.Router() can be used to setup more advanced routes.
        - The following snippet creates the endpoint /api/books
        #+BEGIN_SRC javascript
        //setup route handler
        var bookRouter = express.Router();

        //setup the routes
        bookRouter.route('/Books')
        .get(function(req, res){
          var data = {hello: 'This is my API'};
          res.json(data);
        });

        app.use('/api/', bookRouter);
        #+END_SRC
*** Morgan
    - Http request logger middleware
*** Body-parser
    - Body parser middleware
*** Mongoose
    - API docs :: http://mongoosejs.com/docs/api.html
    - Similar to an ORM
    - Install
      - npm install mongoose --save
    - Fascilitates implementation of mongo-db in node applications
    - makes implementation of mongo much easier.
    - works off of schemas. Since mongo-db is a schema-less document database, implementing schemas with mongoose is often seen as a bad idea.
    
    - mongoose.connect :: opens connection, if it does not exist then it will be created
      - mongoose.connect('mongodb://<host>/<data-base>');
    - <model>.find(function(err, list<item>) :: find all records for a model
    - <model>.find(query, function(err, list<item>) :: find all records for a model filtered by query. Query can come from req.query.
    - <model>.findById(req.params.<routeId>, function(err, <item>)
* Angular JS
** Isolate Scope
- Local Scope Properties: Used to allow information flow between isolate and parent scope. Note that the alternate name option applies to all local scope properties, but is only illustrated with the @ below.
  - @: one-way binding of string values
    - directive usage:
      scope: {
    name: '@'
    value: '@someOtherAttrName'
    }
    - consumer usage:
    <div my-isolate-scope-with-name name: '{{customer.name}}' someOtherAttrName='{{constomer.value}}'></div>
  - =: two-way binding of objects
    - directive usage:
   scope: {
   customer: '='    
   },
    template: '<ul><li ng-repeat="prop in customer">{{prop}}</li></ul>
    - consumer usage:
      <div my-isolate-scope-with-model customer="customer"></div>
  - &: function binding for call-backs
    - directive usage:
      scope{
      action:'&'
      }
    - consumer usage:
      <div my-isolate-scope-with-function action="doStuff()" />
** Migrating AngularJS 1.x to Angular 2+
*** Main Topics Covered
**** Components
**** Bootstrapping
**** TypeScript and ES6
**** Using Classes in Angular 1
*** Reasons to Migrate
    - Angular 2+ is much faster and simpler
    - Supports:
      - Lazy Loading
      - Server-side rendering
      - Multiple Rendering Targets
*** Preperation
    - Preparing your code
      - Primer
        - Directives
          - There are three types of Directives in boty AngularJS and Angular
            - Component
              - Represented by an Element
                - Restrict set to 'E'
              - Has a template
              - Should use an isolate scope
              - Most common
            - Decorator
              - Represented by an Attribute
                - Restrict set to 'A'
              - No Template
              - Should use Shared Scope
              - Typically use the link function for manipulating the DOM
              - Uncommon                
            - Structural
              - Add / remove nodes from DOM
              - ng-repeat, ng-if, ng-switch are examples
              - Almost never used
        - Angular 1 to 2 Mapping
          | Angular 1           | Upgradeable     | Angular 2+ | Downgradeable | Description                                            |
          | Controllers         | Yes (converted) | Component  | Yes           | Encapsulates html / logic                              |
          | Component Directive | Yes             | Component  | Yes           | Encapsulates html / logic                              |
          | Services            | Yes (formated)  | Services   | Yes           | Exactly what you think it is / reusable business logic |
          | Decorator Directive | No              | Directives | No            | Give DOM element more functionality                    |
          | Filters             | No              | Pipes      | No            | Format, filter, sort etc.                              |
        - Upgrade / Downgrade
          - By using the migration adapter things can be migrated one at a time without duplication while keeping the whole app functional
          - Angular 1.x things can be used by Angular 2+ things by going through an upgrade process
          - Angular 2 things can be used by angular 1.x things by going through a downgrade process
          - No code changes to be upgraded / downgraded by the adapter
          - Ownership of hibryd app
            - The top most node is always Angular 1.x
            - If Angular 2+ is used in Angular 1.x then the top evel dom element represeting the angular 2 component is scoped to angular 1.x.
              - Directives applied to this top level element must be Angular 1.x
              - Child nodes are scoped to Angular 2+
                - Directives applied to this node must be Angular 2+
              - vice versa
          - Upgradeability
            - Controllers must be converted to Component Directives to be Upgradable
            - Services must be formatted a certain way to be upgradeable
            - Decorator Directives cannot be upgraded. must be re-written as Angular 2+ Directives
            - Filters cannot be upgraded. Must be re-written as Angular 2+ Pipes
          - Change Detection
            - In Angular 1.x this is known as a digest
            - An event such as a click event in Angular 1.x will fire a change detection and then that will propogate to a change detection in Angular 2+ and vice versa
            - There is a performance cost to running two change detection strategies. So a fully upgraded app will perform better than a hybrid.
            - Angular 1.x change detection (digest) wll always be the bottle neck. Angular 2+ is much faster.
    - Angular 2+ migration plan
      - Phase 1
        - Make changes to existing code
          1. Follow the style guide (google Angular Style Guide to find it)
             - https://github.com/johnpapa/angular-styleguide
             - During preperation we want to look at the angular 1 style guide mainly since we are still in angular 1.x
               - The most important things to follow in the style guide are:
                 - Single Responsibility
                   - Every object should be in its own file
                 - Controllers
                   - Use Controller As View Syntax
                   - Directive Controller
                     - Goto each controller in the application one at a time
                     - set controllerAs: '$ctrl' above the controller property
                     - set bindToController: true
                   - Stand alone Controller
                     - Goto the routing for that controller
                     - set controllerAs: '$ctrl' above the controller property
                   - Remove $scope argument from controller function and replace any use of $scope with the this variable
                   - Go into the template and append $ctrl to the beginging of any binding to the controller
                 - Directives
          2. Update to latest version of Angular 1.x
             - must be at least Angular 1.3
             - better to be upgraded to Angular 1.5+
               - Angular 1.5 has features to make it easier to migrate to Angular 2+
             - probably will be as simple as including the script to the latest version of angular.min.js and angular-route.min.js.
             - Confirm that everything still works and that angular.version outputs the correct version in the browser console.
          3. Use Angular 1.5+ Components for all new development during this phase
             - 1.5 Components are a type of directive that has short hand notation for a directive that uses isolate scope.
             - This step is optional unless the thing is a controller and you want to be able to use the upgrade process of the ng-upgrade component (migration adapter)
             - Makes the process of migrating to Angular 2 in Phase 2 easier.
          4. Switch existing controllers to components
             - This step will help, but is not required
             - Makes the process of migrating to Angular 2 in Phase 2 easier.
          5. Remove incompatible features from directives
             - Incompatible Features Include
               - Compile property
               - Terminal property
               - Priority property
               - Replace property
          6. Switch Component Directives to the new 1.5x Components
             - Open the directive file
             - Replace directive function with component
             - The second argument is not a function, it is just the object to return.
             - Remove the restrict property. As a component directive it will be restricted to 'E' implicitly.
             - Rename scope property to bindings
             - Remove ControllerAs - unless you don't want to use default $ctrl
             - Remove bindToController - unless you don't want to use default $ctrl
             - Open up template and make sure that it is using the correct accessor. If you removed ControllerAs and bindToController then that means that the accessor should be the default which is $ctrl.
          7. Implement manual bootstrapping
             - Required to migrate to angular 2 and to run a hubrid application.
          8. Add TypeScript & a Build
             - Javascript will go through a build step before running in the browser
          9. Start using ES6
          10. Switch Controllers to ES6 Classes
              - If we switched stand alone controllers to components, then those components which are really like a type of directive, will have controllers. We can now switch those Component Controllers to ES6 Classes
              - If we have any stand alone controllers that we never wanted to turn into components for some reason then we should switch those to ES6 Classes.
          11. Switch Services to ES6 Classes
      - Karma Unit Tests
        - change $controller to $componentController
        - The $componentController adds a new parameter. The second parameter is the arguments for the controller as before. The new third parameter is the bindings for the component.
      - Phase 2
        1. Add Angular 2+ to the build
        2. Migrate 1 thing at a time to Angular 2
           - If we migrate something like a Filter which cannot be upgraded, and it is needed in both Angular 1 and Angular 2 then in those cases we will need to create a Pipe which duplicates the object until the filter is no longer needed by the Angular 1.x code and can then be removed.
    - Adding AngularJS 1.5 Components (Step 3 / 4 of migration plan)
      - Step 3: Doing all new development with components
      - Step 4: Switching controllers to components
        - Rename the file where the ctrl lives by dropping the Ctrl postfix
        - Open the file and change the controller method to component.
        - Rename the first parameter, the name of the component, by dropping the Ctrl postfix
        - The second parameter is currently a function, but the component is expecting an object.
          - Add a few lines above the current second parameter (an object)
          - In the space create a json object
            {
              templateUrl: 'path/nameOfTemplate.html',
              bindings: {},
              controller: function(){
                // the controller code goes here...
              }
            }
              
          - The bindings property works just like the angular controller scope property.
          - The controller property should be assigned to the function argument that used to be the second argument for the controller.
        - Update the script reference to refer to the newly named component file
        - Update the routing
          - Remove the specification for the controller, since it is not a component and not a controller
          - Remove the TemplateUrl property since that is now part of the definition of the component.
          - Add a property called template. The template is for the top level element of the component. Recall that components are restricted to elements. In the simplest case the template property can simply equal the html element for the component wrapped in quotes.
          - Remove the controllerAs property. It nolonger applies to the route since this is a route for a component now. Since $Ctrl is the default ControllerAs variable we do not need it, but if we wanted to have a different variable then we would need to add the controllerAs property as a property of the second argument of the component itself.
          - If a resolve property is being used to inject a property into the controller it must be changed as this is not possible with components.
            - The injected property must be removed from the controller. Let's call that property myResolveable
            - myResolveable should be added to the bindings property using two-way binding i.e. myResolveable: '='
            - In the route template add an attribute named my-resolveable to the components element and set it to equal "$resolve.myResolveable". The attribute property is the same as the property in bindings, unless you override the name, but with html case and the $resolve property that it is set to is the same property found in the resolve property of the route defenition.
              #+BEGIN_SRC 
              .when('/path/myComponent', {
                template: '<my-component my-resolveable="$resolve.myResolveable"></my-component>',
                resolve: {
                  myResolveable: routeResolvers.myResolveable
                }
              })
              #+END_SRC
            - Since myResolveable is now a property in the bindings and not injected in the defenition of the controller it must now be accessed using the this accessor. So everywhere it is used add the this accessor.
              
    - Preparing Directives and Bootstrapping
    - Adding Typescript and ES6
    - Using ES6
    - Switching to Classes
* Mongo-db
  - A schema list document database
  - No schema to define.
  - No relationship between collections of objects.
  - Objects can be flat or structured.
  - Two documents in same collection can be different from each other since no schema governs the collection.
    + Scalability
      - Single document write scope. Documents live in a collection, but updating a document occurs one at a time.
      - No need to extend locks accross collections because there are no relationships to enforce.
      - Eventual consistency. Mongo does not lock accross multiple mongo servers. A repleca set in mongo contains a single server that will handle all writes and a collection of secondary servers that will be replecated to. There is a lag of time from when a write occurs in the Primary DB to when the value is made observable by others by being replicated in a secondary db; hence, eventual consistancy.
      - Can choose consistancy model: 
        - Can choose to wait for primary write server to persist data
        - To wait for all replica servers to sync with the primary server following the write.
        - To wait for a majority of replica servers to sync with the primary server following the write.
        - Choose to hand over document to primary and not care wether it persisted or not.
      - Capped Collections:
        - Fixed size :: no time to allocate space
        - Auto override all documents
    + Mongod
      - The daemon.
      - Default Port: 27017
      - mongod help :: help documentation for commandline options      
    + Mongos
      - The sharding server.
  - Mongo Client
    - mongo :: starts the client
      - if running from emacs -nw then use commit-run to run client. Otherwise you will have problems with buffer.
    - help :: brings up client help documentation
    - exit :: quits the client
    - show dbs :: lists existing dbs
    - db :: shows the current database. If no database was specified then you will be on the test database.
    - use foo :: switches to database foo and creates it if it does not exist.
    - db.getMongo() :: returns host and port for that server instance.   
  - Replica Sets
    - Advantages :: scalability and automatic recovery.
    - Types :: Primary, Secondary, Arbiter
    - Primary
      - One and only primary instance.
      - read/write
    - Secondary
      - Readonly
      - one to many
      - Data is replicated from primary. Gaurantees eventual consistancy.
      - If Primary database fails, one of the seconary databases will take over and become the primary. This is descided by an election.
      - Nothing special happens if a secondary db fails. If one secondary fails and there are others than no big deal. Haveing multiple Secondaries protects against single server failure.
    - Arbiter
      - sole purpose is to break ties on primary db elections.
      - is not a database. It contains no data.
    - Minimal replica set :: Primary DB, 1 Secondary DB, 0 or 1 Arbiters
    - Dev can run a replica set on a single machine. Production should run each mongo server per machine
    - Creation of single machine replica set:
      - Each mongo server requires its own db directory. i.e. db{1,2,3}
      - Each mongo server must run on a different port
      - mongod --dbpath ./db{n} --port unique_port_num --replSet "<replicaSetName>"
      - The mongoShell is a javascript interpreter
      - connect to one of the instances and create the replicaSet javascript object:
        - > var replicaSetConf = {id='demo', members: [
          {_id:0, host: 'localhost:30000', priority: 10}, //high priority ensures this will become the primary
          {_id:1, host: 'localhost:40000'}, //no priority ensures this will become a secondary
          {_id:2, host: 'localhost:50000', arbiterOnly=true}] //this will become the arbiter
        - > rs.initiate(replicaSetConf)
        - demo:PRIMARY> //The prompt will change to replicaSetName:Type>
      - Cannot begin to read from secondary replicaset unless you issue the db.setSlaveOk()
    - Shell
      - A javascript interpreter
        - Supports scripting in javascript
      - useful for admin tasks
      - Spelunk data
      - fix or modify documents
      - Inspect mongod health
      - Single Command
        - mongo host/dbname --eval "db.runCommand({logRotate:1}" :: the --eval flag evaluates a javascript argument and prevents an interactive shell
      - run from script
        - create a javascript file with the script you wish to run.
        - mongo host nameOfScript.js :: Also prevents interactive shell from running.
        - mongo host nameOfScript.js --shell :: Remains in interactive shell after running script.
      - Some admin tasks
        - mongo localhost/admin --eval "db.runCommand({logRotate:1})" :: configures mongo to rotate log file
        - use printjson to print out json results
          - mongo localhost/admin --eval "printjson(db.runCommand({logRotate:1}))" :: prints the result in json format
        - Can override database commands
          - DB.prototype.dropDatabase = function(){ print("Don't do it man!");}
            db.dropDatabase = DB.prototype.dropDatabase;
          - Now I can stick that in a script i.e. safter.js and then run it creating a shell that won't let me accidentally drop a database while I am working in there.
            - mongo safer.js --shell
        - mongo shell has special keybindings. Read about them in the man page. 
        - Edit a multiline function
          #+BEGIN_SRC javascript
          myFunc = function(x) //defines a new function
          edit myFunc //This will open the editor defined by environment variable EDITOR. Make changes and save.
          myFunc() //executes the newly modified function
          #+END_SRC
        - pwd() :: print working directory
        - load('safter.js') :: load the script safter.js into the current shell
        - Anything placed in ~/.mongodb-rc will be executed on start of shell.
        - mongo --norc :: starts shell without reading ~/.mongodb-rc
        - db.serverStatus() :: Lots of server status info
        - db.serverCmdLineOpts() :: Outputs the command line options that the shell was started with.
** Storage Engine
   - Uses memory mapped files
   - mongo creates a huge array that it treats as if it were all just in memory. The array references files and core OS library handles serializing the data to disk and reading it into memory.
     - OS lib is highly optimized and reliable
   - data in mongo is stored in a binary JSON format called BSON
     - reference :: http://bsonspec.org
     - BSON requires very little marshalling to convert from memory to disk and vice versa.
   - Lots of optimization with little overhead results in a very fast storage engine
** Saving Documents
   - What is required to store data in mongo?
     1. An document must have an id field named _id
        - If the document is missing the _id field then one will be provided.
     2. Document must be no larger than 16 MB.
   - Collections
     - > show collections :: shows any collections in the database.
     - Defines scope of interaction with documents.
       - Cannot issue command accross multiple collections
   - Save a document
     - db.<colleciton-name>.save(<json object>);
     - db.foo.save({_id:1, x:10});
     - The first time you save to a collection if it does not exist it will be created.
     - Once created show collections will find the new collection.
       - db.system.indexes :: a special collection indexing the ids from all other collections in the database.
     - If an object is saved without an _id field then mongo will generate one:
       > db.users.save({name:'Bob'})
         WriteResult({ "nInserted" : 1 })
       > db.users.find()
         { "_id" : ObjectId("56d7c92eb6b9acc077b385a4"), "name" : "Bob" }
     - ObjectId() :: generates a new object id.
       - ObjectId().getTimestamp() :: No need to store a timestamp field as the object id contains a timestamp.
       - ObjectIds create an ascending insertion order, that is they sort in order that they were created.
     - What happens when two different documents are saved with the same id?
       - The first save creates the new object with the given _id.
       - Since the second save has the same _id it simply updates the document.
     - What happens if we try to insert two documents with the same _id into the same collection
       > db.foo.insert({_id:1, name:'bob'});
       > db.foo.insert({_id:1, age:12});
       - results in a write error.
       - So insert can be used to help with data integrity since it will prevent clobering an existing record if one already exists.
     - Calling insert multiple times without providing an _id field behaves in the same way as save does. The shell will generate new _id fields each time.
   - read a document
     - db.<collection-name>.find()
     - db.foo.find()
       - > db.system.indexes.find()
          { "v" : 1, "key" : { "_id" : 1 }, "name" : "_id_", "ns" : "test.foo" } // here the database is test and the collection is foo, hence test.foo
     - db.foo.find().pretty()
       - beautifies the output
     - valid types for _id field:
       - integer
       - real number
       - string
       - UTC date
       - complex structure
     - invalid types for _id field:
       - An array.
     - db.foo.findOne({_id:1}) :: finds the record with _id = 1 in the collection foo
   - Update a document
     - The update command is atomic within a document. That is, no two clients may update the same document at the same time.
     - Two update commands issued concurrently will be executed one after the other.
     - Syntax: db.<collection-name>.update(<query>, <update>, <options>);
       - <query> :: determines which document
       - <update> :: What change
       - <options> :: Optional parameter. Values: One, Many, Upsert
         - One :: The first one found. Default option.
         - Many :: Any documents matching query
         - Upsert :: If not found will create a new document, otherwise will update the existing document.
     - > db.foo.update({_id:1}, {$inc: {x:1}}); :: uses mongo's increment operator to atomicallyl update the document
     - How to update a document to have an additional field:
       - db.<collection-name>.update({_id:1}, {$set: {y:3}}) :: uses $set operator
     - How to update a document to have a field removed:
       - db.<collection-name>.update({_id:1}, {$unset: {y:''}});
     - $rename :: rename operator
       - db.<collection-name>.update({_id:1}, {$rename: {<old-name>: <new-name>}})
     - $push :: pushes values into an array. If one does not exist then one is created
       - db.<collection-name>.update({_id:1}, {$push: {things: 'one'}});
     - $addtoSet :: like push, but treats the target as a set so that adding more than one item of the same value will not cause it to be added more than onece to the document.
     - $pull :: removes all instances of an element from an array
     - $pop :: removes the first or last element in array. 1 for last -1 for first.
          > db.<collection-name>.update($pop: {<field>:<-1 | 1>}});
     - multi:true :: Allows an update to affect more than one record
       - db.<collection-name>.update({}, {$push: {things:4}}, {multi:true}); :: An empty query {} means no filtering.
     - findAndModify :: updates a specific record. Only the first one found is modified. Returns the record before the change was made.
       - Syntax:
         - db.<collection-name>.findAndModify( {
           query: <document>, //query parameter determines what records are returned.
           update: <document>, //what change?
           upsert: <boolean>, //create or update document matching criteria. Not to be used with remove
           remove: <boolean>, //remove document matching criteria. Not to be used with upsert.
           new: <boolean>, // If true then findAndModify returns the record after the change is made.
           sort: <document>, //query order
           fields: <document>}) //What fields to include in the returned document.

** Retrieving Documents
   - Query criteria
     - db.<collection-name>.find(
       <query>,                    // filter
       <projection>)               // Which fields should be returned. This is optional.
     - Projection contains the names of the fields to include or exclude from the returned document.
       - {<field>: 1} //include field
       - {<field>: 0} //exclude field
       - _id is included by default.
       - You cannot mix including and excluding fields. This may change in the future.
     - Reference
       - https://docs.mongodb.org/manual/reference/operator/query/ :: complete list of query and projection operators
     - Query criteria is case sensative
     - Dot notation
       - used to provide query parameters deep within the document structure
         - db.animals.find({'info.canFly': true})
     - Things to watch out for
       - Field existance and nulls
         - Don't use null to test for existance. i.e. db.animals.find({'info.canFly': null}) will return things with canFly and without.
           - db.aminals.find('info.canFly': {$exists: true}}) will return documents only where canFly is defined.
       - comma seperated criteria are all AND'ed
   - Field Selection
   - Cursor
     - Returned from a find command when multiple documents are returned.
       - var cursor = db.<collection-name>.find({}, <query>)
       - cursor operations
         - size() :: returns cursor size
         - hasNext() :: returns true if there are items to iterate over
         - forEach(function(<document>){}) :: iterates cursor items
         - sort({<field>:<sort-direction>) :: sorts the cursor results.
           - sort-direction > 0 ascending, sort-direction < 0 descending
           - sorting is done on server side. The field being sorted by does not need to be included in a projection.
           - May sort by multiple fields at different levels in the document hierarchy
             db.animal.find({}, {name:1, 'info.type':1}).sort({'info.type':1, 'name': -1})
         - limit(<max-num>) :: takes up to max num of documents.
         - skip(<num-skip>) :: number of docs to skip. This combined with limit is useful for paging.
         - findOne() :: returns exactly one document (not a cursor, but a document)
** Mongodb Indexing
   - Indexes
     - Without indexes all documents in a collection would have to be loaded into memory and scanned for specific query criteria
     - An index is a table of field to document mappings allowing mongo to quickly find all documents where a field matches certain criteria.
     - Indexes allow only documents matching the criteria to be loaded into memory.
     - Types of Indexes
       - Regular B-Tree
         - Single or multiple field
         - single or multiple values
       - Geo
         - Omptimized for geo location
         - Does not have to be geography. The algorithm calculates proximity of points to centre.
         - Allows to find things near something
           - sort by nearness or proximity to a certain point.a
       - Text
         - Allows for full text searching
       - Hashed
         - Pertains mainly to sharding
         - Allows index on a certain field, but reduces fragmentation of the indexes. This allows documents to be spread more evenly across shards.
       - TTL
         - Time To Live index
         - Support expiring documents.
         - Can specify a date time on a document to be an expiry datetime.
         - Mongo will automatically remove the document from the collection when it expires.
     - Supports indexing arbitrary nested feilds
     - multi-key index :: When indexing an array each element of the array will be indexed as a value
   - Indexing a Collection
     - Commands
       - ensureIndex :: creates an index
         ({
         <key>:<sort-direction>},           //which fields, what order and if it is Geo or Text
         {<options>})
         - options:
           Name :: Name of index
           Build Now :: Build immediately and block every other operation or build it in the background
           Unique :: Is the index unique? Whether to make this uniqueness a constraint.
           Spars :: Is it a sparse index?
           TTL :: Is it a TTL index
           Language :: What language settings should be used
         - ensureIndex may contain a comma seperated list of keys of the form <field1>:1,<field2>:1
       - db.system.indexes.find({ns:<namespace>}, {key:<key>})
         - use to find an index.
         - namespace is <database>.<collection>
       - How to find out of mongo uses an index for a query?
         - db.<collection>.find(<query>).explain()
         - explain outputs a description of how mongo is going about resolving the query.
         - if cursor is BasicCursor then no indexing is used.
         - scanned property tells how many documents are scanned to resolve the query.
         - explain tells many useful things about what mongodb is doing to resolve query.
       - To alter an index you must drop it first.
         - db.<collection>.dropIndex(<indexName>)
         - Not allowed to drop index on _id
       - Unique Index
         - db.<collection-name>.ensureIndex({<field-name>:1}, {unique:true});
         - db.system.indexes.find({ns:'<database>.<collection-name>'}).pretty() will output with the index <field-name> will have the 'unique':true property
         - Although the _id index is unique mongo does not print it out with the unique property.
       - Sparse Index
         - db.<collection-name>.ensureIndex({<field-name>:1}, {sparse:true});
         - By default when you create an index a key value is created for every document in the collection even for documents that do not contain the feild being indexed.
         - A sparse index will create an index only for those documents in the collection that contain the field being indexed.
         - If a query is performed that uses a sparse index it will omit all documents that do not have that index and that may not be what you want. It is important to make sure that your indexing strategies and your queries match up all of the documents that you want.
       - Multi Index (Compoind Index)
         - db.<collection-name>.ensureIndex({field-name1>:1,<field-name2>:1})
         - db.<collection-name>.find({<field-name1>:'val1',<field-name2>:'val2).explain() :: shows that index is used
         - db.<collection-name>.find({<field-name2>:'val2',<field-name1>:'val1).explain() :: shows that index is used
         - db.<collection-name>.find({<field-name1>:'val1').explain() :: shows that index is used
         - db.<collection-name>.find({<field-name2>:'val2').explain() :: shows that index is NOT used. Why? Because the index was created specifying <field-name1> then <field-name2> and any query must include, in any order, the field names as they were declared from left to right.
         - May contain up to 31 fields in it. The name of the field is the concatonation of the field names and their sorting direction.
         - Mongo imposes a 128 character limit on index names so it is possible that this limit could be exceeded by compound indexing.
         - The 128 character limit includes the name of the colleciton.
         - Is a problem when you have lots of field names included in a compound index. It can also be a problem when you have lots of deeply nested indexes.
         - See Index Name for a resolution to this issue
       - Sort direction
         - Query sort direction is same as index sort direction :: The index will be used
         - Query sort direction opposite of index sort direction :: The index will be used
         - Query sort direction neither same nor opposite as index sort direction :: The index will not be used
           - db.<collection-name>.ensureIndex({<field-name1>:1,<field-name2>:1})
             db.<collection-name>.find().sort({field-name1>:1,<field-name2>:-1}) :: since the query is sorted with field1 ascending and field2 descending it is neither the same nor the opposite as what the index supports so the index cannot be used.
       - Covering Index
         - If a query uses an index and if all the information the query requests is covered by the index, then there is no need to fetch the actual document as the index itself can be used to return the requested information.
         - To make mongodb use a covering index you will have to make sure that you omit the _id unless it is actually part of the index. Otherwise it will have to fetch the actual document in order to get the _id field.
         - explain() will return the property 'indexOnly':true if a covering index strategy is used when returning values for a given query.
       - Dead Weight Index
         - Mongodb will let you create an index on a field when no document in the collection exists with that field.
         - It will keep track of this index and one day if a document is created with that field then it will be covered by that index.
         - For this reason it is possible to misspell an index name and mongodb will happily create it. You may not realize that you created the wrong index. Always use explain to double check that your indexes are being used in queries as you expect them to be.
       - Background index building
         - db.<collection-name>.ensureIndex({<field-name>:<sort-direction>},{background:true})
         - By default ensureIndex will index the collection in the forground blocking all other queries. In a large database this can take a long time.
         - By setting background option to true mongodb will index a collection in the background allowing read and write operations to continue
         - Indexing in the background can take much longer.
         - Indexing in the background can result in larger indexing structures then indexing in the foreground.
       - Index Name
         - db.<collection-name>.ensureIndex({<field-name>:<sort-direction>},{name:<index-name>});
         - By default the index name is the concatonation of all the field names and sort directions specified during indexing.
         - <index-name> provides a name for the index overriding the default index naming scheme.
              
** TODO Install Mongo-db Minimal ReplicaSet
   - mongod -f "e:\dev\experiments\MultiVision\conf\mongod1.conf" --replSet "Experiments" --install
   - Windows instructions to get the replica set running as a service.
* REST
  - Representational State Transfer
  - ReST services should be written around nouns not verbs
  - Design the service around a Uniform Interface. There are 3 peices that should always operate the same way from one service to the next
    1. Resources
       - These are the nouns or things that the uniform interface is built around
       - Services should not use verbs or actions.
    2. HTTP Verb
       - defines the type of activity you are trying to perform on the resource
       - GET :: retrieves a resource
       - POST :: creates a resource
       - DELETE :: Removes a resource
       - PUT :: updates an entire resource
       - PATCH :: updates a piece of a resource
    3. HATEOS (Sounds like - Hayoss)
       - Hypermedia As The Engine Of Application State
       - Means that in each request will be a set of hyperlinks that you can use the navigate the API
       - Creates a self documenting API.
       - This is a manual procedure where you clone the model being returned and add links for all of the valid end points for the model

* Mocha
  - Unit testing framework
* Should
  - Assertion framework
  - BDD Style
* expect
  - BDD Style assertion framework
* sinon
  - mocking framework
* jquery
* MEAN stack
** Steps to create a walking skeleton
 - Create working directory
   - cd <working_dir>
   - npm help json :: review fields used by npm init
   - npm init
   - npm install express jade --save :: jade is view rendering engine
 - Setup git
   - git init :: or optionally git clone
   - touch .gitignore
   - git status :: take not of what git sees
   - Add anything that you do not want to be under source control to the .gitignore file
   - git add -A :: Add everything not ignored to be tracked by git.
   - git commit -a -m "first commit"
 - Install Clientside Dependencies using bower
   - npm install bower --save-dev
   - sudo npm install bower -g :: For convenience so that it is easily within scope from terminal
   - mkdir server :: server side application
     - Will contain:
       - node source code
       - jade views
       - client-side partials
   - mkdir public :: client side application
     - All of our angular application code.
   - touch .bowerrc :: tells bower where to install client-side dependencies
   - For bower package.json spec please reference
     - https://github.com/bower/spec/blob/master/json.md
   - bower init
   - bower install jquery --save
   - bower install toastr --save :: client side notifications
   - bower install angular angular-resource angular-route --save
 - Create Node Application
   - touch server.js :: main file for node application
   - npm install gulp gulp-nodemon --save-dev
   - sudo npm install gulp -g :: for convenience
   - touch gulpFile.js
     - create default task
       - construct nodemon
       - handle restart event just to print out that it has happend.
   - Inside server.js:
     - require express
     - get the port
     - construct express
       - set views path
       - set view engine
     - configure default server side route
       - have this catch all route render the index view
     - Create ./server/views/index.jade
     - Configure app to listen on port
     - configure middleware
       - npm install morgan body-parser stylus --save
       - configure logger (morgan) for express
       - configure body-parser middleware to handle marshalling json
       - configure css preprocessor middleware
       - use express static middleware to configure routing to /public. This tells angular that any requests that match the static route it should just u pand serve the content.
   - Create Layout jade file
     - mkdir server/includes
     - touch server/includes/layout.jade
     - move all content from server/views/index.jade to server/includes/layoute.jade
     - mkdir public/css
     - touch public/css/site.styl :: application stylus file
     - bower install bootstrap --save :: our css will leverage bootstrap
     - Add links in layout.jade to favicon, bootstrap, toastr and our own site.css
     - touch server/includes/scripts.jade :: here will inject all of our script tags
     - include scripts in layout.jade
     - In server.js add a second param to the default route which renders the index view.
       - The second param should include the following properties:
         - title :: The title of the application
         - scripts :: An array of paths to all of the scripts that we wish to inject into scripts.jade.
           - jquery, angular, angular-resource, angular-route
     - Inside scripts.jade
       #+BEGIN_SRC jade
       - each script in scripts
         script(type="text/javascript", src=script)
       #+END_SRC
     - Inside the layout
       - Inside head add
         - title= title
         - base(href="/")
         - any links
       - include scripts either in head or end of body
     - inside server/views/index.jade
       - extend ../includes/layout.jade
       - add block for main-content
  
 - Create Angular Application
   - mkdir public/app
   - touch public/app/app.js :: main angular application entry-point
   - Define the angular application module
   - configure the angular application module
     - Turn on html5mode
       - add base(href="/") to to the head of /server/includes/layout.jade so that routing will work properly
       - Inside public/app/app.js place $location.Provider.html5mode({enabled: true}); above the routeProvider rules.
     - Add a default rout to /partials/main with a controller called mainCtrl
     - Define the mainCtrl right in the app.js for now to verify that it works.
     - Include public/app/app.js in server/includes/scripts.jade
     - Make a partials file that our mainCtrl will request.
       - mkdir /views/partials
       - touch /views/partials/main.jade
       - Add some content that binds to a scope variable from mainCtrl so that we can prove that it works.
       - Remove section.content message from index.jade and replace it with an ng-view
       - Inside the /server/includes/layout.jade add ng-app directive to body tag
       - Add route to partials before the catch-all route.
         - the route should have form 'partials/:partialPath'
 - Hook the walking skeleton up to mongodb
   - npm install mongoose --save
   - Inside the server.js file require mongoose
   - Under the static express route connect to mongodob
   - on any database errors log that there was a connection error
   - on the connection open event log that we have established a connection.
   - Start the server and test that the connection is successfully established.
   - Create a mongoose schema called messageSchema
   - Create a Message model based on messageSchema
   - Fetch one item from the database using the Message model
   - Add message property to object being rendered to index view.
   - go to server/view/index.jade and display the message.
   - switch to mongo shell
   - db use MultiVision
   - db.messages.insert({message:'Hello Angular from mongodb'})
   - Now reload the index page and the message 'Hello Angular from mongodb' should be rendered.
 - Add tests
   - Unit tests
   - Integration Tests
     - Use Node Environment variables to set connection string
     - Inside
 - Angular User Interface
   - Create header in index.jade
     - Normally the header would go in the layout, but since this is a SPA we will stick it in the index.
     - update site.styl
   - Create footer in index.jade
     - update site.styl
 - Prepare Application for deployment
   - Add engines property to package.json
** Authentication
 - Passport Local Strategy
   - Server-side
     - npm install passport passport-local --save
     - Create a User model using mongoose
     - in server.js 
       - passport = require('passport')
       - passportLocal = require('passport-local').Strategy;
       - configure passport middleware after the call to require mongoose configuration using passportLocal
       - handle passport serializeUser
       - handle passport deserializeUSer
     - In server/config/routes.js
       - Below route to partials add a post route to '/login' and handle passport.authenticate for local strategy.
     - npm install cookie-parser express-session --save
     - In server/config/express.js
       - require cookie-parser, express-session and passport
       - add cookieParser middleware below logger middleware
       - add session middleware below bodyParser middleware
       - add passport initialize middleware
       - add passport session middleware       
     - Create an authentication service
       - mkdir server/config/auth.js
       - create module.exports.authenticate function
       - refactor logic from server/config/routes.js app.post /login and move the authentication logic to server/config/auth.js authenticate function
       - remove require('passport') from server/conf/routes.js
       - add require('auth') to server/conf/routes.js
       - have post take auth.authenticate as second argument.
       - test that everything still works.         
   - Client-Side
     - inside mvNavBarLoginCtrl
       - inject $http service
       - in $scope.signin post to the login endpoint passing the user's credentials.
       - if response.data.success then console.log that the user logged in.
       - else console log that there was an authenticaiton failure
     - Create toastr notification
       - mkdir public/app/common
       - touch public/app/common/mvNotifier.js
       - at the top of the file define a value for toastr so that it can be injected.
       - use app.factory to create mvNotifier and inject mvToastr.
       - Return an object with a notify function that takes a message argument and pasess it to mvToastr.success
       - Use the mvNotifier service in the mvNavBarLoginCtrl for both success and failure notifications.
       - add path to mvNotifier in server/config/routes.js catch all scripts property
       - test and style toastr message.
     - Create identity service
       - mkdir public/app/account/mvIdentify.js
       - use app.factory to create mvIdentity service
       - add path to mvIdentity.js in server/config/routes.js catch all scripts property
     - Create an authentication service
       - touch public/app/account/mvAuth.js
       - use app.factory to create mvAuth service for handling the client-side authentication logic.
       - The service should take $http, mvIdentity, $q
       - Refactor authentication service from mvNavBarLoginCtrl.signin to mvAuth.authenticateUser(username, password)
       - inside the mvNavBarLoginCtrl.signin call mvAuth.authenticateUser and issue the success / fail notifications.
       - add path to mvAuth.js in server/config/routes.js catch all scripts property
 - Implementing secure passwords
   - clear-text password + salt => Hashing Algorithm => Hashed Password
   - We will use a unique salt for each user therefore we will store both a user's hashed password and there salt.
   - in server/conf/mongoose.js
     - Alter userSchema by adding two properties:
       - salt: String
       - passwordHash: String
     - require('crpto')
     - Create method to generate a salt
       - Create the method salt
         - return crypto.randomBytes(128).toString('base64');
     - Create method to take a salt and password and return a hashed password
       #+BEGIN_SRC javascript
       var hashPassword = function(salt, password){
         var hmac = crypto.createHmac('sha1', salt);
         hmac.setEncoding('hex');
         hmac.write(pwd);
         hmac.end();
         return hmac.read();
       }
       #+END_SRC     
     - Create userSchema.methods json and add a funciton called authenticate to the userSchema that takes a password
       #+BEGIN_SRC javascript
       userSchema.methods= {
         authenticate: function(password){
            return hashPassword(this.salt, password)) === this.passwordHash;
         }
       }
       #+END_SRC
   - in server.js
     - In the passport middle-wear after the user is checked for existance also call user.authenticate(password)
 - Create a means of logging out
   - Add a button to logout on the navbar-login
   - inside public/app/account/mvNavBarLoginCtrl
     - Add method signout that calls mvAuth.logoutUser and then redirects the user to the home page.
   - inside public/app/account/mvAuth
     - add the loutout function.
     - It should post a dummy payload to a logout endpoint. On success it should set the currentUser to undefined.
   - inside the server/config/routes.js
     - Implement the logout endpoint.
     - call req.logout() // function added by passport
     - call res.end()
   - Test the logout functionality
 - Add current user to client session
   - Inside server/includes/layout.jade
     - include currentUser
   - Create server/includes/currentUser.jade
     - assign bootstrappedUser property to window.bootstrappedUserObject so that it is available on the client
   - Inside server/config/routes.js
     - Add bootstrappedUser property to the catchall route render index argument and assign it to req.user.
   - Inside public/app/account/mvIdentity
     - Check the $window.bootstrappedUserObject and if it exists then assign that to the currentUser variable
 - Refactor Server Authentication
   - Create server/config/passport.js
     - move passport configuration code from server.js to module.exports. This should include creating the mongoose User model.
     - server.js should now have no 3rd party require statements.
     - Test that everything still works.
 - Implementing Client-side authorization
   - Inside server/config/mongoose.js add roles property to userSchema that is an array of type String
   - In the section where the default users are added, make one user have the 'Admin' Role, one user have an ampty roled array and one user not have roles defined at all.
   - clear the users collection in mongo shell
   - restart server.
   - Even though we did not set the roles property to one of the users you can see that mongoose created the property and set it to an empty array in the mongodb users collection
   - Create a user resource object
     - using app.factory create the service public/app/accounts/mvUser.js
     - inject $resource
     - create a UserResource variable set to $resource('/api/users/:id', {_id: '@id'})
     - using prototype create function UserResource.isAdmin to see if the user is an admin by checking the roles collection on the resource.
   - Inside mvAuth
     - create a user variable that is a new mvUser and use angular.extend on user and resource.data.user.
     - set the mvIdentity.currentUser to the user resource.
     - Now mvIdentity.currentUser will have isAdmin function
   - Inside mvIdentity
     - if $window.bootstrappedUserObject is a thing then set currentUser to new mvUser and extend with $window.bootstrappedUserObject
     - Now the client user has the isAdmin function
   - Add an /admin/user link to nav-bar login dropdown menu
     - Only render the link if identity.currentUser.isAdmin() is true
 - Implement Server-side Authorization
   - Inside server/config/routes.js
     - require ('mongoose')
     - use mongoose to create the User model
     - Create the route /api/users that will return a list of all our users
     - Test that the endpoint returns all of the users in the browser and notice that it can be accessed without being logged in.
     - protect the resource so that the user will recieve a 402 if they make a request for the users resource without being logged in.
       - add another function to the /api/users route indbetween the route and the function that returns the users collection.
       - Make this middleware function take the following parameters: req,res,next
       - use req.isAuthenticated() provided by passport to see if the user is authenticated and call next() in that case
       - otherwise, set the status, send a message and call res.end()
     - Refactor the authentication check function from the previous step to the server/config/auth.js module.exports.isApiAuthenticated
       - Now use auth.isAuthenticated as the first parameter to /api/users and anywherelese that we wish to protect.
   - Inside server/config/auth.js create module.exports.requiresRole(role) 
     - Have this function return a middleware function(req,res,next)
     - if not authenticated or does not have role then return 403, otherwise proceed to next function
 - Create and protect admin page on client
   - Inside public/app/app.js create new route for admin page
     - .when('/admin/users', {templateUrl: '/partials/admin/userList', controller: 'mvUserListCtrl'});
   - mkdir public/admin
   - Create mvUserListCtrl.js
     - touch public/admin/mvUserListCtrl.js
     - The controller should use mvUser.query() to set a $scope.users variable.
   - Create userList partial
     - touch public/admin/userList.jade
     - Create a table that loops over the collection of users and outputs their first and last names
   - Inside of server/config/routs.js Add mvUserListCtrl to the index page scripts.
   - Inside of public/app/app.js add an route resolver to the /admin/users route following the controller
     #+BEGIN_SRC javascript
     resolve: {
            auth: function(mvIdentity, $q){
                if(mvIdentity.currentUser && mvIdentity.currentUser.isAdmin()){
                    return true;
                }
                else{
                    return $q.reject('not authorized');
                }
            }
        }
     #+END_SRC
     - Following app.config add an app.run that handles the $rootScope.$on('$routeChangeError', ...) event and watches for a rejection message matching the one we defined in our route resolve defined above. If it does, then redirect the client back to the home page using $location.path('/');
 - Refactor client-side routing
   - Inside app/account/mvUser
     - Add function UserResource.prototype.hasRole(role) That checks role against this.roles
     - refactor UserResource.prototype.isAdmin to call hasRole
   - Add isAuthorized function to identity service
     - Should take in a role
     - Should return true if the user is authenticated and this.currentUser.hasRole(role) returns true
   - Move authorization code from route resolver to mvAuth
     - inside public/app/account/mvAuth
       - create function isCurrentUserAuthorized(role)
       - paste in code from route resolver and alter it so that it passes along the provided route rather than hard checking for 'Admin'
       - now set the routes resolve.auth to be assigned to a function that injects mvAuth and returns mvAuth.IsCurrentUserAuthorized('Admin')
   - Refactor route resolver to a convenience object called routeRoleChecks
     #+BEGIN_SRC javascript
     var routeRoleChecks = {
        admin: {auth: function(mvAuth){
            return mvAuth.isCurrentUserAuthorized('Admin')
        }}
     };     
     #+END_SRC
     - now set routes resolve property to routeRoleChecks.admin
** Unit Testing
   - sudo npm install karma-cli -g
   - npm install karma mocha karma-mocha karma-chai-plugins --save-dev
   - bower install angular-mocks --save-dev
   - karma init
     - testing framework: mocha
     - use require.js: no
     - browser to capture automatically: firefox
     - location of source and test files:
       - public/app/**/*.js
       - test/tests/**/*.js
     - location of source to exclude:
       - public/app/app.js
     - Should karma watch files?
       - yes
   - Configure karma
     - inside karma.conf.js
       - frameworks: ['mocha', 'chai', 'sinon-chai'
       - Add to files:
        'public/vendor/angular/angular.js',
        'public/vendor/angular-resource/angular-resource.js',
        'public/vendor/angular-mocks/angular-mocks.js',
     - mkdir test
     - mkdir test/public
     - mkdir test/public/app
     - touch test/public/app/app.js
     - inside test/public/app/app.js
       - var app = angular.module('app', ['ngResource']); //not testing routing so no ng-route
       - create fake global toastr object
         var toastr = {}
     - Inside karma.conf.js
       - insert 'test/public/app/app.js' into files array after all 3rd party stuff, but before any of our own stuff
     - Test that karma runs
       - karma start
   - Write some tests
     - mvUser Service
       - mkdir test/public/app/account
       - touch test/public/app/account/mvUser.test.js
* git
** Steps to create a new Github Rep
 1. Login to github and create the repository
 2. Copy the ssh path to the new repository
 3. Follow this command pattern:
    - $ mkdir myProj
    - $ cd myProj
    - $ git init
    - touch README.md
    - touch .gitignore
    - git add -A
    - git commit -m "my first checkin"
    - git remote add origin [paste copied git repo uri here]
    - git push -u origin master :: //MUST BE IN INTERACTIVE SHELL TO PROVIDE CREDENTIALS
    
** Steps to tag and push a new release
 1. cd myProj
 2. command pattern:
    - git tag 0.0.1 :: Should be the same version as entered in bower.json "version" property
    - git push --tags
 3. Now visit github, click on the project and then click on the release link to view the release.
    
* Linux
- Getting help:
  - =# equery f iproute2 | egrep '/usr/share/(man|doc)/'= :: Will list all help docs that reference.
  - =# qlist iproute2 | grep -i doc= :: qlist is a link to q and lists all files owned by package. This will include all the docs.
** Kernel
- =# make help= :: output help
- =# make modules_install MODLIB=/lib64/{kernel-version}{cust-postfix}= :: Installs kernel modules to custom path
** Commands :: remember to always check the man pages.
  - pwd :: print working directory
  - lsblk :: list block devices.
  - lddtree --help :: there are no man pages for this command
  - lddtree --copy-to-tree=/source/path /target/path :: from app-misc/pax-utils USE="python"
  - find /usr/portage -name '*.ebuild' -o -name '*.eclass' | xargs grep MAKE_CONF_VARIABLE :: finds all package references to MAKE_CONF_VARIABLE
  - lspci | grep -i vga :: detect video controller
  - numactl --hardware :: check to see if the hardware has numa support. This will return the number of numa nodes.
  - getconf
    - used to get configuration values
    - =getconf PAGE_SIZE= :: returns the systems page size in bytes.
  - q :: invoke a portage utility applet.
  - =equery u <package>= :: Similar to euse -i <useflag>, but brings up use flag info for a specific package.
*** Detect Motherboard, Bios and CPU
   - dmidecode -t 4 | grep ID :: The CPU ID
   - dmidecode -t 0 :: Bios info
   - dmidecode -t 4 :: Processor info
   - dmidecode -t 11 :: Original Equipment Manufacturer (OEM) info
** Variables
   - $_ :: recalls the last argument
** I/O Stream Numbers
   | Handle | Name   | Description     |
   |      0 | stdin  | Standard input  |
   |      1 | stdout | Standard output |
   |      2 | stderr | Standard error  |
   - =$ program-name 2> error.log= :: Redirect standard error stream to a file
   - =$ program-name &>file= :: Redirect the standard error (stderr) and stdout to file
     =$ program-name > file-name 2>&1= :: Alternate redirect the standard error (stderr) and stdout to file

** Telly Type Terminals TTYs
   - =Ctrl-Alt-Fx= :: switch to TTYx
   - =$ chvt x= :: switch to TTYx via command line. Good for ssh.
** Boot Process
  1. Boot loader loads Linux.
  2. Linux assumes control of the system.
  3. Linux prepares its memory structures and drivers
  4. Hands control to Init.
  5. Init makes sure that at the end of the boot process, all necessary services are running and the user is able to log in.
  6. Init launches udev daemon which will further load up and prepare the system based on the detected devices.
  7. Udev mounts the remaining file systems waiting to be mounted.
  8. Udev starts the remaining services waiting to be started.
      
** Initramfs
*** The Initial RAM File System
  - Based on tmpfs
  - Because it is a size-flexible, in-memory lightweight file system it does not use a seperate block device so no caching was done. It does not have the overhead of an entire file system.
  - Contains the tools and scripts needed to mount the file systems before the init binary on the real root file system is called.
    - The tools can be the decryption abstraction layers (for encrypted file systems), logical volume mangers, software raid, bluetooth driver based file system loaders, etc.
  - All files, tools, libraries, configuration settings (if applicable), etc are put into a cpio archive.
*** From creation to execution
  1. The cpio archive is compressed using gzip and stored in the /boot partition along side the linux kernel.
  2. The boot loader will let the linux kernel know where the cpio archive is at boot time so that the kernel can load the initramfs.
  3. The Linux kernel will create a tmpfs file system, extract the contents of the cpio archive into it, and then launch the init script located in the root of the tmpfs file system.
  4. The init script will then perform what ever tasks are necessary to ensure that it will be able to mount the real root file system.
    - It may have to decrypt the real root file system, other vital file systems, and mount them among possibly other things depending on what is needed.
  5. The init script from the initramfs will then switch the root towards the real root file system
  6. Lastly the initramfs init script will call /sbin/init (the init script on the real root file system)
  7. The boot process will continue as normal.
** Networking
 - iproute2 is now default over ifconfig for network configuraiton, routing and tunneling
 - Open Systems Interconnection model (OSI model) :: https://en.wikipedia.org/wiki/OSI_model
 - Ethernet
   - conceptually behaves like a single bus that each of the network hosts connect to. In reality each host connects directly to a switch.
   - Ethernet occupies the second layer known as the data link layer. You will also here terms such as:
     - local network
     - layer 2
     - L2
     - link layer
   - Ethernet network hosts communicate by exchanging Frames.
   - Every host on an ethernet network is identified by an address called the Media Access Control (MAC) address.
   - Every host can send a frame directlyto every other host
   - A host can broadcast a frame to every other host on the same network at once by sending it to the broadcast MAC address
   - ARP and DHCP use ethernet broadcasts.
   - Sometimes ethernet is referred to as broadcast domain.
   - When a NIC recieves a frame it checks to see if the destination MAC address matches its MAC address or the broadcast MAC address. If not, then it discards it.
   - Promiscuous Mode configures a NIC to pass all frames to the operating system regardless of their MAC address.
 - MAC address
   - 48 bits
   - typically represented as a hexidecimal string.
   - ff:ff:ff:ff:ff:ff :: The broadcast MAC address
 - Network switch
   - Used to connect hosts on an ethernet
   - When a NIC recieves a Frame for a particular destination MAC address for the first time it does not know which port is connected to the destination host so it broadcasts the frame. If the destination host acknowledges the frame then the switch records the port for that MAC address in a table called the Forwarding Information Base (FIB). For future Frames the switch can look up the correct port in the FIB and send the frames direction
   - Switches can be daisy-chained. All hosts connected to these switches will be on a single ethernet.
 - Forwarding Information Base :: a table of MAC address to switch ports
 - VLANS
   - Enables a single switch to act as multiple independant switches.
   - Two hosts on the same switch, but on different VLANs do not see each others traffic.
   - Each VLAN has an associated ID. VLAN15 refers to the VLAN with ID 15.
   - The switch is responsible for ensuring that traffic is isolated accross VLAN's
   - A port that has been configured to serve traffic to a particular VLAN is called an Access Port.
   - If two switches are configured for VLAN's and are daisy-chained, then the ports for cross-connecting the switches must ben configured to allow ethernet frames from any VLAN to be forwarded to the other switch.
     - The sending switch must tag each ethernet frame with the VLAN ID so that the recieving switch can ensure that the frame is forwarded to the correct VLAN and only the correct VLAN.
   - Trunk Port :: A switch port that is configured to tag and forward ethernet frames for all VLANs.
   - IEEE 802.1Q :: The network standards that describes how VLAN tags are encoded in eithernet frames when trunking is being used.
 - Subnets and ARP
   - While NICs use MAC addresses to address hosts, TCP/IP uses IP Addresses to address hosts.
   - The address resolution protocol (ARP) bridges the gap between ethernet and IP by translating IP addresses into MAC addresses.
   - IP addresses have two parts, a network address and a host address.
   - Two hosts are on the same Subnet if they have the same Network Address
   - Two hosts can communicate directly over ethernet if they are on the same local network.
   - ARP treats all hosts in the same subnet as being on the same local network.
   - A netmask specifies how many bits of a 32-bit IP address make up the network address.
     - Since there are 8 bits in each of the 4 octets of an IP address the following network addresses can be derived:
       - 192.168.75.22/24  --> 192.168.75.0
       - 192.168.75.22/16 --> 192.168.0.0
       - 192.168.75.22/8 --> 192.0.0.0
   - Creating CIDR subnets including a multicast address or a loopback address cannot be used in an OpenStack environment
     - multicast addresses are in the range 224.0.0.0 through 239.255.255.255.
   - arp -n :: view ARP cache.
   - An ARP Request of Host A sending a message to Host B looks like:
     Host A --> To: everybody (ff:ff:ff:ff:ff:ff). I am looking for the computer who has IP address 192.168.1.7. Signed: MAC address fc:99:47:49:d4:a0.
     Host B --> To: fc:99:47:49:d4:a0. I have IP address 192.168.1.7. Signed: MAC address 54:78:1a:86:00:a5.
     Host A then sends Ethernet frames to host B.
   - Commands:
     - $ Arping :: sends an arp command.
     - $ Arp -n :: view the arp cache.
 - Dynamic Host Configuration Protocol (DHCP)
   - DHCP Server :: The system that assigns IP addresses to network hosts
   - DHCP Clients ::  Network hosts that receive IP addresses from a DHCP Server.
   - The interaction between client and server:     
     - The client sends a UDP discover request from port 68 to 255.255.255.255 port 67 --> (“I’m a client at MAC address 08:00:27:b9:88:74, I need an IP address”)
     - The server sends a UDP offer request from port 67 to port 68 on the client. --> (“OK 08:00:27:b9:88:74, I’m offering IP address 10.10.0.112”)
     - The client sends a request (“Server 10.10.0.131, I would like to have IP 10.10.0.112”)
     - The server sends an acknowledgement (“OK 08:00:27:b9:88:74, IP 10.10.0.112 is yours”)
   - Due to the discovery taking advantage of the broadcast address, a DHCP server must be on the same local network as a DHCP client.
   - OpenStack uses dnsmasq as the DHCP server.
   - Troubleshooting tip:
     - dnsmasq writes to syslog. When an instance is not reachable over a network check the log that all 4 steps of the DHCP protocol have been satisfied.
 - Internet Protocol IP
   - Specifies how to route packets between hosts on different local networks.
   - Relies on special networked hosts called Routers or Gateways.
   - Router :: A host connected to at least 2 local networks with an IP address for each local network it is connected to. A router can forward IP packets from one local network to another.
   - Layer 3 (L3) of the OSI model also known as the Network Layer.
   - Every network device has a routing table
   - The routing table maintains a list of subnets for each local network the host is directly connected to
   - The routing table maintains a list of routers that are on local networks that the host is directly connected to.
   - A host sending a packet to an IP address consults its routing table. 
     - If the destination host is on the local network, the data is delivered to the destination host.
     - If the destination host is on a remote network, the data is forwarded to a local gateway.
   - A DHCP server typically transmits the IP address of the default gateway to the DHCP client along with the client’s IP address and a netmask
 - TCP/UDP/ICMP
   - User Datagram Protocol (UDP)
     - Another L4 protocol
     - Connectionless protocol
       - No connection required for two applications to exchange data
     - Unreliable protocol
       - The OS does not attempt to detect lost packets
       - Order is not gauranteed
     - Uses ports to distinguish between different applications running on the same system.
       - UDP ports are independant of TCP ports.
     - Examples of UDP based protocols: DHCP, Domain Name System (DNS), Network Time Protocol (NTP), Virtual Extensible Local Area Network (VxLan)
     - Supports one to many communication
     - An application can send a UDP packet to a set of receivers using IP multicast (i.e. VXLAN uses IP multicast)
       - All involved routers must be configured to support IP multicast
     - Internet Control Message Protocol is a protocol for sending messages over an IP network.
       - ping and mtr both use ICMP.
 - Network Components
   - Switches :: used to connect devices on a network.
     - Operate at L2
     - Forward packets on to other devices
     - Pass data on only to devices that need to receive it.
   - Routers :: A network devices that connects multiple networks together
     - Are connected to two or more networks
     - Use routing table to determine which network to pass received packets to
   - Firewalls :: A network devices that controls the incomming and outgoing network traffic based on applied rules
   - Loadbalancer :: A network device that distributes network or application traffic accross a number of servers.
 - Tunnel Technologies
   - Allows a network protocol to encapsulate a payload protocol such that packets from the payload protocol are passed as data
   - Generic Routing Encapsulation
     - transmits IP packets with private IP addresses over the internet using delivery packets with public IP addresses
   - Virtual Extensible Local Area Network (VXLAN)
     - Encapsulates L2-ethernet frames over L4-UDP packates
     - Allows the creation of a logical network for virtual machines accross various networks.
 - Namespaces
   - Creates a scope identifier
   - Linux provices namespaces for networking and processes
   - Process Namespace
     - If a process is running within a process namespace then it can only detect and interact with other processes running within the same namespace
   - Network Namespace     
     - Scope identifier are network devices
       - eth0 would be in a particular namespace
       - There is a default namespace that all things are in if not explicitly declared to be in a different namespace.
     - Each namespace has its own routing table
       - A routing table is keyed by destination IP address.
       - Namespaces allow the same destination IP to have different routing rules from one namespace to another
       - This is how OpenStack is able to have a feature that allows overlapping IP addresses in different virtual networks.
     - Each namespace has its own IPTables
       - Allows for different security rules from one namespace to the next.
     - IP netns exec NETNS COMMAND
       - runs a command in the namespace NETNS
       - IP netns exec NETNS ping A.B.C.D will cause IP A.B.C.D to be looked up in the routing table scoped by NETNS and that will determine what network device the message is ultimately transmitted to.
 - Virtual Routing and Forwarding (VRF)
   - Allows multiple instances of a routing table to coexist on a single router.
   - It is another name for Network Namespace
 - Network Address Translation (NAT)
   - a method for modifying the source or destination IP address in the header of an IP-packet in transit
   - Neither the sender, nor the receiver are aware that the IP-packet headers are being manipulated
   - iptables provides NAT functionality
   - There are multiple variations of NAT
     - SNAT :: Source Network Address Translation
       - The NAT router modifies the IP-packet header sender address
       - Commonly used to enable hosts with private IP addresses to communicate with servers on a public network such as the internet.
       - Consider the senario where a user wants to access a website on the ineternet from a system with a private IP address.
         - If the request contains packet headers with a private source address then the webserver will not be able to send the response back to the sender.
         - SNAT solves the problem by modifying the source address to an IP address that is routable on the public internet.
         - There are different variations of SNAT
         - In the case of OpenStack SNAT
           - a NAT router between the sender and receiver replaces the source IP address in the IP-packet header with the router's public IP address.
           - modifies the source TCP or UDP port value
           - maintains a table of the source's true IP and port to the modified IP and port
           - When the router receives a packet destined to the modified IP and port, it translates the IP-packet header destination address and port to the corresponding private IP and port listed in the table and then forwards it.
           - This form of SNAT is sometimes refered to as Port Address Translation (PAT) or NAT overload.
           - OpenStack uses SNAT to allow applications running inside of instances to communicate out to the internet.
     - DNAT :: Destination Network Address Translation
       - The NAT router modifies the destination IP address in the header of an IP-packet in transit
       - OpenStack uses DNAT to route packets from applications running inside of instances to the OpenStack metadata-service.
       - Applications running inside of instances access the meta-data service by sending REST requests to IP address 169.254.169.254.
       - OpenStack uses DNAT to change the destination IP address from 169.254.169.254 to the IP address of the meta-data service
     - One-to-one NAT
       - maintains a one-to-one mapping between public and private IP addresses
       - OpenStack uses this to implement floating IP addresses       
 - Private Addresses
   - RFC 1918 :: https://tools.ietf.org/html/rfc1918
   - Reserves 3 subnets for private IPs
     - 10.0.0.0/8
     - 172.16.0.0/12
     - 192.168.0.0/16
   - Link-local 
     - 169.254.0.0/16
     - Used between two hosts on a single link when no IP address is specified.
     - Valid for communication within the network segmant (link) or broadcast domain that the host is connected to.
*** Routing
    - All IP networking is a permutation of three fundemental concepts of reachability:
      1. The IP Address is reachable on the machine itself.
         - Known as scope host.
         - used for IPs bound to any network device, including loop-back devices and the network range for the loop-back device.
         - Addresses of this nature are called local IPs or locally hosted IPs
      2. The IP address is reachable on the directly connected link layer medium.
         - Addresses of this type are called locally reachable or directly reachable IPs.
      3. The IP address is ultimately reachable through a router which is reachable on a directly connected link layer medium.
         - This class of IP addresses is only reachable through a gateway.
    - General IP Networking Terminology
      - Classess Inter-Domain Routing (CIDR), supernetting
      - Octet :: A single number between 0 and 255, hexadecimal 0x00 and 0xff.
        - An octet is a single byte in size.
        - examples: 140, 255, 0, 7
      - IP Address, IP :: A locally unique 4 octet logical identifier which a machine can use for communication using the IP protocol.
        - The address is determined by combining the network address and the host address.
        - The IP Address is a unique number identifying a host on a network.
        - examples: 192.168.99.35, 140.71.38.7, 205.254.86.91
      - Host Address Portion :: The right most bits (frequently octets) in an IP Address which are not part of the network address. The part of an IP address which identifies the computer on a network independent of the network.
        - examples: the portion within the curly braces - 192.168.1.{27}/24, 10.{10.17.24}/8, 172.20.{158.75}/16
      - network address, network, network prefix, subnetwork address :: A four octet address and network mask identifying the usable range of IP addresses.
        - Conventional and CIDR notations combine the four bare octets with the netmask or prefix length to define this address.
        - Briefly a network address is the first address in a range and is reserved to identify the entire network.
        - examples: 192.168.187.0/24, 205.254.211.192/26, 4.20.17.128/255.255.255.248, 10.0.0.0/255.0.0.0, 12.35.17.112/28.
      - Network mask, netmask, network bitmask :: A four-octet set of bits which, when AND'd with a particular IP Address produces the Network Address.
        - Combined with a network address or IP Address, the netmask identifies the range of IP addresses which are directly reachable.
        - examples: 255.255.255.0, 255.255.0.0, 255.255.192.0, 255.255.255.224, 255.0.0.0.
      - Prefix length :: An alternate representation of netmask, this is a single integer between 0 and 32, identifying the number of significant bits in an IP Address or Network Address.
        - This is the "slash-number" component of a CIDR address.
        - Examples: 4.20.17.0/24, 66.14.17.116/30, 10.158.42.72/29, 10.48.7.198/9, 192.168.154.64/26.
      - broadcast Address :: a four-octet address derived from an OR operation between the Host Address Portion of a Network Address and the full broadcast special 255.255.255.255.
        - The broadcast is the highest allowable address in a given network and is reserved for broadcast traffic.
        - Examples: 192.168.205.255/24, 172.18.255.255/16, 12.7.149.63/26.
      - Router :: Any machine which will accept and forward packets between two networks
    - Any IP address is defined by two sets of numbers: Network Address and Netmask.
    - There are two ways to express an IP address:
      - Netmask Notation: 4-octet-address/Netmask
      - CIDR Notation: 4-octet-address/Prefix length
    - Commands
      - ip route show table tablename :: view the routing entries of table named tablename
      - ip route show table all :: list all routing entries of all tables
*** OpenStack Networking (Neutron)
    - Overview and Components
      - OpenStack networking facilitates the creation and management of network object (networks, subnets, ports etc) which other OpenStack services can use.
      - Plug-ins are used to accomadate different networking devices.
      - Neutron provides an API that allows for the defining of network connectivity and addressing.
      - Neutron provides an API manage a variety of network services such as:
        - L3 forwarding
        - NAT
        - Load balancing
        - perimiter firewalls
        - VPNs
      - Neutron includes the following components
        - API Server
          - Supports L2 networking and IP Address Management (IPAM)
          - Extension for L3 router construct that enables routing between L2 networks and gateways to external networks
          - A growing list of plug-ins that enable interoperability with various networking technologies including: routers, switches, virtual switches, Software Defined Networking (SDN) controllers. 
        - OpenStack Networking plug-in and Agents
          - Plug-ins and unplugs ports, creates networks or subnets, provides IP addressing
          - The chosen plug-in and agent differ depending on the vendor and technologies used
          - Only one plug-in can be used at a time.                        
        - Messaging Queue
          - Accepts and routes RPC requests between agents and to complete API operations
          - Used by the ML2 plug-in for RPC between neutron server and neutron agents that run on each hypervisor
    - Concepts
      - You can create networks and subnets and instruct OpenStack services like Compute to attach virtual devices to ports on these networks.
      - OpenStack compute uses Neutron to provide connectivity for its instances
      - Tennants can have multiple private networks, choose their own addressing scheme independantly of other tenants.
      - Two types of networks:
        - Tenant
          - Used for connectivity within projects
          - By default they are fully isolated and not shared with other projects.
          - The following types of network isolation and overlay technologies are supported:
            - FLAT
              - All instances reside on the same network. This may include the hosts.
              - No VLAN tagging or network segregation takes place                
            - VLAN
              - Allows users to create multiple provider or tenant networks using VLAN IDs (802.1Q tagged) that correspond the VLANs present in the physical network
              - Instances can communicate accross the environment
              - Instances can communicate with dedicated servers, firewalls, load balancers and other networking infrastructure on the same layer 2 VLAN                
            - GRE & VXLAN
              - Encapsulation protocols that create overlay networks to activate and control communication between Compute instances
              - A networking router is required to allow traffic to flow outside of a GRE or VXLAN tenant network
              - A networking router is required to connect direct-connected tenant networks with external networks, including the internet.
              - A networking router is required to support ONE-to-one NAT floating IP addresses allowing the ability to connect to instances directly from an external network.
        - Provider
          - Creaated by an admin
          - Map to existing physical networks in the data centre.
          - Flat and VLAN make useful provider networks.
          - You can create networks and subnets and instruct OpenStack services like Compute to request to be connected to these networks by requesting virtual ports
          - Tennants can have multiple private networks, choose their own addressing scheme independantly of other tenants.
      - Subnets
        - A block of IP addresses known as native IPAM (IP Address Management)
        - Used to allocate IP addresses when new ports are created on a network
      - Ports
        - A connection point for attaching a single device to a virtual network
        - Describes the associated networking configuration, MAC and IP addresses to be used on that port
      - Routers
        - Forwards data-packets between networks
        - Provides L3 and NAT forwarding to provide external network access for VMs on tenant networks.
        - Required by certain plug-ins only
      - Security Groups
        - Acts as a virtual firewall for compute instances controlling inbound and outbound traffic.
        - Act at the port level (not subnet level)
        - Each network has a default security group
          - drops all ingress and allows all egress
          - Rules can be added to to this group
        - A container for security group rules
        - Groups and group rules give admins and tenants the ability to sepcify the type of traffic and direction that is allowed to pass through a port.
        - Ports are associated with a security group at creation time.
      - Extensions
        - Serve two purposes
          - Allow the introduction of new features to the API without requiring a version change
          - Allow the introduction of vendor specific functionality
          - Applications can list all available extensions by performing a GET /extensions request.
          - Extensions available in one API might not be available in another.
    - Service & Component Heirarchy
      - 
*** Virtual Network Interface
    - References:
      - https://wiki.gentoo.org/wiki/QEMU_with_Open_vSwitch_network
      - https://wiki.gentoo.org/wiki/QEMU/KVM_IPv6_Support    
    - edit /etc/conf.d/net
      - vlans-[interface-name]="1 2 .." :: declare numbered vlands for named interface
      - Read the docs:
        - =# less /usr/share/doc/netifrc-*/net.example.bz2=
    - Set up a vlan for our physical interface 
      - You will want to use the ipmi service for this
      - vlan 1 is default. When adding new vlans always pick a vlan-id > 1 <= 4096
      - vlans will be added in /proc/net/vlan/
      - vlan conf is in /proc/net/vlan/conf
      - To create a vlan link
    #+BEGIN_SRC bash
    $ sudo ip link set enp3s0f0 down
    $ sudo ip address add 0.0.0.0/32 dev enp3s0f0
    $ sudo ip link set enp3s0f0 up
    $ sudo ip link add link enp3s0f0 name enp3s0f0.2 type vlan id 2
    $ sudo ip addr add 198.27.74.225/24 brd 198.27.74.255 dev enp3s0f0.2
    $ sudo ip link set enp3s0f0.2 up
    $ sudo ip link add link enp3s0f0 name enp3s0f0.3 type vlan id 3
    $ sudo ip addr add 192.95.36.241/24 brd 198.95.36.255 dev enp3s0f0.3
    $ sudo ip link set enp3s0f0.3 up
    #+END_SRC    
  - ==$ ip -d link show enp3s0f0.2== :: shows the vlan id. Useful if the name does not include the id.
  - ==$ ip -d addr show == :: show full address details.
  - To remove a vlan link
    #+BEGIN_SRC /bin/bash
    $ sudo ip link set dev enp3s0f0.2 down
    $ sudo ip link delete enp3s0f0.2
    #+END_SRC
    - 

*** Protocols
    - LACP
      - part of the IEEE specification 802.3ad
      - Bundle several pysical ports to form a single logical channel
    - VRRP
      - Virtual Router Redundancy Protocol
        - automatic assignment of available IP routers to participating hosts
        - Increases availability and reliability of routing paths via atomic default gateway selections on IP subnetworks
** Profiling and Instrumentation
  - eBPF :: extended berkly packet filtering
    - Reference: http://www.brendangregg.com/blog/2015-05-15/ebpf-one-small-step.html
  - Kprobes
** dcron
   - /etc/crontab is the system crontab
   - uses crontab in conjunction with conrbase to run scripts in /etc/cron.{daily,hourly,weekly,monthly}
   - =# crontab /etc/crontab= :: Run everytime changes are made to the system crontab.
   - =# crontab -l= :: display a list of cron jobs
     - jobs schedualed in system crontab may not show up in this list
   - It is not necessary to use the system crontab simply by never running:
     - =# crontab /etc/crontab=
     - =# sed -i -e "s/^/#/" /etc/crontab= :: to be extra careful you can comment all lines out in /etc/crontab:
   - Schedualing Jobs
     - =# crontab -e= :: edit crontab
     - =# crontab -d [user]= :: delete crontab. If no user is supplied then it deletes the current user's crontab.
     - =# crontab file= :: new crontab
     - Each lineitem in a crontab has the following fields:
       | Minutes | Hours  | Day of Month | Month  | Day of week |
       | (0-59)  | (0-23) | (1-31)       | (1-12) | (0-7)       |
       - Monday is day 1, Sunday is day 0.
       - Days of week and months can be specified by 3 letter abbreviations or numbers.
       - Each field can specify a range or a comma seperated list. i.e. 1-5, mon-fri or 1,3,4.
       - Ranges can have a step i.e. 1-5/2 = 1,3,5 where 2 is the step to increment by.
       - Regarding Day of Month and Day of week. If * is used for one, then the other takes precendence. If * is used for both, this means every day.
   - Schedualing jobs with the system crontab
     - =# sudo crontab /etc/crontab= :: to replace root's current crontab
     - Then simply drop the scripts into /etc/con.{daily,hourly,weekly,monthly}
** Gentoo   
*** Use Variables
    - If a use variable has a * by it that means that it changed since the last build.
*** Hardened Profile
    - References:
      - https://wiki.gentoo.org/wiki/Project:Hardened :: Gentoo Hardened Project
**** PIC (Position Independent Code)
     - Functions and data are accessed through an indirect table called the Global Offset Table (GOT).
     - The purpose of indirect addressing is to fascillitate the access of functions and data independently of the corresponding load address. Only the symbols in the text segment exported in the GOT need updating at run-time deending on the current load address of the various shared libraries in the address space of the running process.
     - Similarly, procedure calls to globally defined functions are redirected through the "Procedure Linkage Table" (PLT) residing in the data segment of the core image. This avoids runtime modifications of the text segment.
     - The Linker-editor allocates the GOT and PLT when combining the PIC object files into an image for mapping into the process address space.
     - The Linker-editor collects all symbols that may be needed by the run-time link-editor and stores these along with the image's text and data bits.
     - Objects compiled as PIC allow the OS to load the object at any address in preperation for execution with slight overhead.
     - The libtool builds PIC objects for use in shared libraries and non-PIC objects for use in static libraries. PIC compilation is required for objects in a shared library.
     - libtool compiles PIC objects with '*.lo' extension and non-PIC objects with '*.o' extension.
     - In practice PIC objects can be linked into static archive and often non-PIC objects can be similarly linked into shared archives both with execution and load speed overhead.
     - If the shared object is built from code that is not PIC then the text segment will usually require a large number of relocations to be performed at runtime. The system overhead from the run-time linker required to handle this can cause serious performance degradation.
     - =# readelf -d foo= :: If the output contains a TEXTREL entry then text relocations exist.
**** Grsecurity
     - References: 
       - http://grsecurity.net/ :: project page
       - https://wiki.gentoo.org/wiki/Hardened/Grsecurity2_Quickstart :: quick start guid
       - http://en.wikibooks.org/wiki/Grsecurity/Appendix/Grsecurity_and_PaX_Configuration_Options :: Features Page
       - I follow the recommended PaX kernel configuration on this page rather than that of the PaX configuration Quick Start page. The reason being that this details how to set a configuration for the XATTR_PAX markings more clearly.
***** PaX
     - Purpose is to protect against a class of exploits that give an attacker arbitrary read / write access to the attacked task's address space. These exploits include buffer and heap overflows and similar attacks. PaX is the first line of defense offered by Hardened Gentoo.
     - Implements the least privilege protections for memory pages. i.e computer programs should only  be allowed to do what they have to do in order to be able to execute properly and nothing more.
     - The exploit techniques that PaX defends against include:
       1. Introduce / execute arbitrary code
       2. execute existing code out of original program order
       3. execute existing code in original program order with arbitrary data
     - References:
       - Gentoo Hardened Introduction :: https://wiki.gentoo.org/wiki/Hardened/Introduction_to_Hardened_Gentoo#Technologies_Offered
       - Project site :: http://pax.grsecurity.net/
       - Quickstart :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart
     - Adds security enhancement to the area between both kernel and userland.
     - Patch to the kernel that provides hardening in the following ways:
       1. Judicious enforcement of non-executable memory
       2. Address Space Layout Randomization (ASLR)
          - Compiling with Position Independent Executable (PIE) allows ASLR to randomaize even the base address.
       3. Miscellaneous hardening on stack and memory handling
          - Erases stack frame when returning from a system call
          - refusing to dereference user-land pointers in some context
          - detecting overflows of certain reference counters
          - correcting overflows of some integer counters
          - enforcing the size on copies between kernel and user land
          - providing extra entropy
     - PaX Modes
       - SOFTMODE
         - PaX protection will not be enforced by default for those features which can be turned on or off at runtime.
         - The "permit by default" mode.
         - The user must explicitly mark executables to enforce PaX protection.
       - non-SOFTMODE
         - PaX protections are immediately activated.
         - The "forbid by default" mode.
         - The user must explicitly mark binaries to relax PaX protection selectively.
     - PaX Configurable Features
       - Enforce non-executable pages :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Enforce_non-executable_pages
       - Enhanced Address Space Layout Randomization (ASLR) :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Enhanced_Address_Space_Layout_Randomization_.28ASLR.29
       - Miscellaneous Memory Protection :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Miscellaneous_Memory_Protection
     - PaX patches support three ways of doing PaX markings:
       1. EI_PAX
          - This option is nolonger supported
          - Places PaX flags in bytes 14 and 15 of the e_ident field of an ELF objects header.
       2. PT_PAX
          - Places the flags in an ELF object's program header called PAX_FLAGS.
          - Flags are in the body of the object and so if the object is moved or copied the flags are also.
          - The object must have the PAX_FLAGS program header to work. Most Linux distributions do not build their executables and libraries with this program header.
       3. XATTR_PAX
          - This is the preferred approach.
          - PaX flags are palced in the file system's extended attributes.
          - Does not modify the ELF object.
          - The file system and utilities used to copy, move and archive files must support xattrs.
            - Must support user.pax.* namespace in which the PaX flags are placed.
            - Do not enable the entire user.* namespace because it may open attackvectors.
            - Must support security.*, trusted.* namespaces.
     - Building a PaX Kernel
       - Reference :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Building_a_PaX_Kernel

                      
***** RBAC
      - gradm :: The administration program for the grsecurity RBAC system
      - 
*** ZFS 
**** Zpool Administration
    - References:
      - https://wiki.gentoo.org/wiki/ZFS/Features :: Detailed list of features
      - https://wiki.gentoo.org/wiki/ZFS :: Gentoo wiki guide
      - https://pthree.org/2012/12/04/zfs-administration-part-i-vdevs/ :: Administraiton Guide
      - http://docs.oracle.com/cd/E19253-01/819-5461/ :: Official Oracle Docs
    - ARC :: Adaptive Replacement Cache
    - ARC page replacement agolrithm is used instead of the Last Recently used page replacement algorithm.
    - Minumum and Maximum memory usage allocated to ARC varies based on system memory.
      - Default Min :: 1/32 of all memory, or 64 MB, whichever is more.
      - Default Max :: the larger of 1/2 of system memory or 64 MB.
      - Linux accounts for memory used by arc differently than memory used by the page cache. Memory used by ARC is included under "used" not "cached" in the output used by the 'free' program. This can give the impression that ARC will use all of system memory if given opportunity.
      - ARC memory usage is tunable via zfs_arc_min and zfs_arc_max. These properties may be set in 3 ways:
        1. at runtime.
           - =root # echo 536870912 >> /sys/module/zfs/parameters/zfs_arc_max=
           - Changes through sysfs do not persist across boots.
           - The value in sysfs will be 0 when the value has not been manually configured.
           - The current setting can be viewed by looking at c_max in /proc/spl/kstat/zfs/arcstats
        2. via /etc/modprobe.d/zfs.conf
           - =root # "options zfs zfs_arc_max=536870912" >> /etc/modprobe.d/zfs.conf=
        3. Kernel command line
           - =zfs.zfs_arc_max=536870912=
      - Zpool Version Update
        - When sys-fs/zfs is updated likely the version of ZFS has been incremented. The status of zpools will indicate a warning that a newer version is available and that the zpools can be upgraded.
        - =root # zpool upgrade -v= :: Display current version on zpool
        - =root # zpool upgrade zfs_test= :: upgrade the version of zpool zfs_test.
        - =root # zpool upgrade -a= :: upgrade the version of all zpools in the system.
***** Virtual Devices (VDEVs)
     - A meta-device representing one or more physical devices.
     - 7 types of VDEVs:
       - Disk (default) :: The physical drives in your system.
       - File :: The absolute path of pre-allocated files/images.
       - Mirror :: Standard software RAID-1 mirror.
       - Spare :: Hard drives marked as a "hot spare" for ZFS software RAID.
       - Cache :: Device used for a level 2 adaptive read cache (L2ARC).
       - Log :: A seperate log (SLOG) called the "ZFS Intent Log" or ZIL.
     - VDEVs are dynamically striped.
     - Caveats
       - Devices cannot be removed from a VDEV
       - RAID-(n) is faster than RAID-(n+1)
       - Hot spares are not dynamically added unless configured to.
       - A zpool will not dynamically resize when larger disks fill the pool unless you enable the setting (off by default) BEFORE the first disk replacement.
       - Will know about "advanced format" 4K sector drives iif the drive reports such.
       - Duplication is EXTREMELY EXPENSIVE, will cause performance degredation if not enough RAM is available.
       - Duplication is pool-wide, not local to the filesystem.
       - Compression is EXTREMLY CHEAP on the CPU, yet it is disabled by default.
       - ZFS suffers a great deal from fragmentation. Full ZPOOLS will experience performance degredation
     - Creation
       - A Simple pool
         #+BEGIN_SRC sh
           # zpool create tank sde sdf
           # zpool status tank
 pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  sde       ONLINE       0     0     0
	  sdf       ONLINE       0     0     0
	  sdg       ONLINE       0     0     0
	  sdh       ONLINE       0     0     0

errors: No known data errors
         #+END_SRC
       - A simple mirrored zpool
         #+BEGIN_SRC sh
           # zpool create tank mirror sde sdf sdg sdh
           # zpool status tank
 pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    sde     ONLINE       0     0     0
	    sdf     ONLINE       0     0     0
	    sdg     ONLINE       0     0     0
	    sdh     ONLINE       0     0     0

errors: No known data errors
         #+END_SRC
       - Nested VDEVs
         #+BEGIN_SRC sh
           # zpool create tank mirror sde sdf mirror sdg sdh
           # zpool status
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    sde     ONLINE       0     0     0
	    sdf     ONLINE       0     0     0
	  mirror-1  ONLINE       0     0     0
	    sdg     ONLINE       0     0     0
	    sdh     ONLINE       0     0     0

errors: No known data errors
         
         #+END_SRC
       - File VDEVs (useful for experiments)
         - When creating cannot use relative paths.
         - The image file must be preallocated, not sparse or thin provisioned.
         #+BEGIN_SRC sh
         # for i in {1..4}; do dd if=/dev/zero of=/tmp/file$i bs=1G count=4 &> /dev/null; done
         # zpool create tank /tmp/file1 /tmp/file2 /tmp/file3 /tmp/file4
         # zpool status tank
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME          STATE     READ WRITE CKSUM
	tank          ONLINE       0     0     0
	  /tmp/file1  ONLINE       0     0     0
	  /tmp/file2  ONLINE       0     0     0
	  /tmp/file3  ONLINE       0     0     0
	  /tmp/file4  ONLINE       0     0     0

errors: No known data errors
         
         #+END_SRC
       - Hybrid pools
         - "tank" pool is composed of "mirror-0" and "mirror-1" VDEVs for long-term persistent storage. 
         - Neither the "logs" pool nor the "cache" pool are long-term storage for the pool, thus creating a "hybrid pool" steup.
         #+BEGIN_SRC sh
           # zpool create tank mirror /tmp/file1 /tmp/file2 mirror /tmp/file3 /tmp/file4 log mirror sde sdf cache sdg sdh
           # zpool status tank
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME            STATE     READ WRITE CKSUM
	tank            ONLINE       0     0     0
	  mirror-0      ONLINE       0     0     0
	    /tmp/file1  ONLINE       0     0     0
	    /tmp/file2  ONLINE       0     0     0
	  mirror-1      ONLINE       0     0     0
	    /tmp/file3  ONLINE       0     0     0
	    /tmp/file4  ONLINE       0     0     0
	logs
	  mirror-2      ONLINE       0     0     0
	    sde         ONLINE       0     0     0
	    sdf         ONLINE       0     0     0
	cache
	  sdg           ONLINE       0     0     0
	  sdh           ONLINE       0     0     0

errors: No known data errors
         
         #+END_SRC
         - In practice, use the device id found in /dev/disk/by-id when identifying the devices for the "logs" and "cache" pools. They may be assigned different device names from one boot to another, unlike devices in the main pool.
***** RAIDZ
      - References:
        - http://en.wikipedia.org/wiki/RAID
      - Disk Striping
        - References
          - http://en.wikipedia.org/wiki/Data_striping
        - The process of segmenting a body of logically sequential data into data blocks so that consecutive data blocks are spread accross multiple storage devices.
        - Storage systems vary in the way striping is performed. Data may be stripped at the byte, block or partition level, and may be stripped accross all or some of the disks in a cluster.
        - Main advantage is higher performance. By spreading data accross multiple devices that can be accessed concurrently, total throughput is increased.
        - Balances I/O load accross an array of disks.
      - Standard Parity RAID (i.e. RAID-5)
        - References:
          - http://blog.open-e.com/how-does-raid-5-work/
        - Consists of block level striping with distributed parity. The stripe width is thus statically set at creation.
        - Minumum of 3 disks. Data is stripped accross two disks. A pairity bit is calculated such that the XOR of all three stripes in the set calculate to zero. The parity bit is then written to the chosen 3rd disk.
        - No single disk is dedicated to parity. Two disk are chosen to be striped and the third is chosen for parity such that the distribution of parity bits is even accross all drives.
        - Resilant against a failure of any one disk. The data on the failed disk can be recalculated via the remaining disks and thus restored.
        - RAID-5 write hole
          - Caused by inturrupted destaging of writes to disk, such as a power failure.
          - Solutions to the RAID-5 write hole are either slow (software based) or expensive (hardware based).
          - RAID-5 has fallen out of favor as a result.
        - If the data being written to the strip is less than the stripe-size, much time is wasted reading the data on the rest of the stripe to ensure that the parity satisfies the constraint that the XOR of all three stripes in the set calculate to zero.
          - Thus data is read and written that is not pertinent to the application doing the work.
          - Expensive NVRAM hardware RAID cards can hide the latency from the end user.
        - The RAID-5 write hole and the performance impact of writing data smaller than the stripe size to disk motivated the ZFS team to re-think parity-based RAID.
      - ZFS RAIDZ
        - Stripe width dynamically allocated.
          - Every block stransactionally flushed to disk is its own stripe width.
          - Every RAIDZ write is a full stripe write.
          - The parity bit is flushed with the stripe simultaneously. This completely eliminates the possability of a write hole.
            - In the event of a power failure, either the latest data was flushed to disk or it wasn't, but the disks will not be inconsistent.
          - Cannot calculate parity simply by the rule -- every disk XORs to zero because the stripe size is dynamic with respect to the size of the datablock being written to disk.
            - ZFS metadata is used to determine the RAIDZ geometry on every read.
            - Reading file system metadata to contruct the RAID stripe means reading live running data only, not dead inpertinent or unallocated data.
            - No need for expensive NVRAM to buffer writes or for battery backup to protect against write hole.
      - Self-healing RAID
        - ZFS can detect silent errors and fix them on the fly (Not possible if RAID and filesystem are seperate)
          - When an application requests data ZFS constructs the stripe and compares each block against a default checksum in the metadata. If the read stripe does not match the checksum, ZFS finds the bad block, reads the parity and fixes it through combinatorial reconstruction and returns good data to the application.
          - If the stripe width is longer than the disks in the array and there is a disk failure there would not be enough data in the parity for combinatorial reconstruction, thus ZFS can mirror data in the stripe to prevent this from happening. :: Bad wording. I re-worded the original to try to make sense of it. Is this what was meant???
      - RAIDZ-{1,2,3}
        - RAIDZ-n, where n is the number of parity bits distributed accross all the disks in the array.
        - Stripe width is variable. Disk Array Size < Stripe width || Disk Array Size == Stripe Width || Stripe Width < Disk Array Size.
        - RAIDZ-n allows for n disk failures.
        - RAIDZ-n requires Disk Array Size = n + 2.
        - RAIDZ-n capacity = Disk Array Size * Capacity of smallest disk - n parity storage
      - Hybrid RAIDZ
        - A stripe of multiple RAIDZ VDEVS.
        - Increase performance at the cost of available disk storage.
        - Stripe width is variable within each nested RAIDZ VDEV.
        - Each RAIDZ level follows the same rules outlined in RAIDZ-{1,2,3} above.
          - A stripe of 3 RAIDZ-n VDEVS can suffer a total of 3*n disks. n per VDEV.
      - RAIDZ Benchmark :: Recommended to use the tool IOZone 3 to benchmark and stress the array.
        - Mirrors always outperform RAIDZ levels.
        - performs(RAIDZ-n) > performs(RAIDZ-{n-1})
        - More parity bits mean longer read / write times.
      
***** ZFS Intent Log (ZIL and SLOG)
      - References:
       https://pthree.org/2012/12/06/zfs-administration-part-iii-the-zfs-intent-log/
      - The Zil and the Seperate Intent Log (SLOG) log what is currently in system memory so that the system may recover in the event of a power loss. This can be setup at anytime so I am going to skip it for now.
***** The Adjustable Replacement Cache (ARC)
      - The purpose of this is to cache data for quick retrieval to increase performance. This is a good idea, but can be added later so I am going to skip this for now.
***** Exporting and Importing Zpools
      - Exporting Storage Pools
        - When exporting the following happens:
          - Causes the kernel to flush all pending data to disk
          - Writes data to the disk acknowledging that the export was done
          - Removes all knowledge that the storage pool existed in the system
        - Exporting the storage pool is necessary before importing the storage pool into a new system. Also, unwritten data may not have been flushed to disk.
        - =root # zpool export tank=
          - Will attempt to unmount all ZFS datasets as well as the pool
          - If the zpool refuses to export you can add -f to force the export.
      - Importing Storage Pools
        - Once the drives have been physically installed they may be imported.
        - =root # zpool import tank= :: Imports the zpool tank
        - Once imported it is a good idea to check the status of the zpool
        - =root # zpool status tank=
          - A status of ONLINE means that everything is healthy
          - Status FAULTED :: means one or more drives appear faulty to the system
          - Refer to documentation for troubleshooting status codes
        - Import multiple zpools by listing them after the import command delimited by white space, or by passing the -a flag for importing all known zpools.
          - =root # zpool import tank1 tank2 tank3=
          - =root # zpool import -a=
      - Recovering a Destroyed Pool
        - Destroying a pool does not wipe the data on the disks. The pool can be discovered.
        - =(server A) root # zpool destroy tank= :: Does not wipe data
          =(server B) root # zpool import -D= :: Lists destroyed pools.
          - Run the import command again specifying the pool name to bring it fully online.
          - If more than one storage pool is found with the same name then the unique identifier of the storage pool must be used as the argument to import to bring it fully online.
      - Upgrading Storage Pools
        - Once a zpool has been upgraded servers running older versions of zpool will not be able to import it.
        - There is no way to downgrade.
        - =root # zpool upgrade -v= :: Outputs the version of ZFS pool that the system is currently running. Display a list of supported zpool versions and some descriptions for each.
        - =root # zpool upgrade -a= :: Upgrades the zpool and enables all supported features. See man page to upgrade to a specific version and feature set.
        - On shutdown the zfs init script may just unmount the pools and not export them in which case the zpools would not be able to be imported to by another system. If this is the case you will have to explicitly export the zpools first.

***** Scrub and Resilver
     - Standard Validation
       - integrity checking tools require the disks to be offline.
       - The filesystem knows nothing about the underlying data structures such as LVM or RAID.
       - Software RAID has no idea what disk contains good or bad data, so either could be served (silent data errors).
       - Nothing can be done about silent data errors.
     - ZFS Scrubbing
       - Scrubbing the disk is used to detect and correct silent data errors.
       - Scrubbing disks can be done on a live running system with no downtime.
       - The scrub involves checking every block in the storage pool against its known checksum using the "fletcher4" (default) 256-bit algorithm.
       - Must be performed explicitly.
       - Recommended that scrubbing is performed on a regularly scheduled interval (good job for chron).
       - =root # scrub tank= :: performs a scrub of the pool.
       - Can check status during a scrub.
       - Scrubs impact performance of disks.
       - =root # scrub -s tank= Stops a scrub in progress.
       - =0 2 * * 0 /sbin/zpool scrub tank= crontab to perform scrub every Sunday at 2 in the morning.
     - Self Healing Data
       - Requires redundancy. i.e. mirror
       - Will not only detect corrupt data on a scrub, but will correct the bad blocks if good data exists on a different disk.
       - I think that this zfs can self heal in response to realtime interaction with an application and is not limited to explicit scrubs. (verify this)
     - Resilvering Data
       - References:
         - http://docs.oracle.com/cd/E19082-01/817-2271/gbbvf/index.html :: regarding damaged devices with ZFS
       - Same concept as rebuilding or resyncing data onto a new disk in the array.
       - With Software RAID there is no distinction between which blocks are live and which are not. The rebuild starts at the begining of the disk and does not stop until it reaches the end of the disk.
       - ZFS knows about the RAID structure and has a smarter algorithm for rebuilding the data.
         - Does not sync the free disk blocks, only live blocks. If storage pool is only partially filled this can save significant time.
         - In ZFS the process of rebuilding, resyncing or reconstructing is kown is Resilvering.
         - If zpool status is "DEGRADED" then a disk needs to be replaced. Identify the disk that needs to be replaced with teh following command
           #+BEGIN_SRC 
           # for i in a b c d e f g; do echo -n "/dev/sd$i: "; hdparm -I /dev/sd$i | awk '/Serial Number/ {print $3}'; done
/dev/sda: OCZ-9724MG8BII8G3255
/dev/sdb: OCZ-69ZO5475MT43KNTU
/dev/sdc: WD-WCAPD3307153
/dev/sdd: JP2940HD0K9RJC
/dev/sde: /dev/sde: No such file or directory
/dev/sdf: JP2940HD0SB8RC
/dev/sdg: S1D1C3WR
           #+END_SRC
           The above example shows that /dev/sde needs to be replaced since the command could not find the device.
         - After identifying the dead disk go to the storage array and find which serial number was not printed. The one that was not printed needs to be replaced. Pull the disk and replace it with a new one.
         - Restart the system and see if /dev/sde is repopulated by running the command again.
         - Then issue the following command to replace sde with the new disk at /dev/sde (note the new disk may not be /dev/sde in which case the command would be slightly different to reflect that)
           =root # zpool replace tank sde sde= Rebuilds the data blocks on the new disk until it is in a completely healthy state. Check the status to know when the process has completed.
     - Identifying Pool Problems
       - =zpool status -x= :: the x flag only displays the status of pools that are exhibiting errors or are unavailable.
       - The zpool status fields:
         + pool- The name of the pool.
         + state- The current health of the pool. This information refers only to the ability of the pool to provide the necessary replication level.
         + status- A description of what is wrong with the pool. This field is omitted if no problems are found.
         + action- A recommended action for repairing the errors. This field is an abbreviated form directing the user to one of the following sections. This field is omitted if no problems are found.
         + see- A reference to a knowledge article containing detailed repair information. Online articles are updated more often than this guide can be updated, and should always be referenced for the most up-to-date repair procedures. This field is omitted if no problems are found.
         + scrub- Identifies the current status of a scrub operation, which might include the date and time that the last scrub was completed, a scrub in progress, or if no scrubbing was requested.
         + errors- Identifies known data errors or the absence of known data errors.
         + config- Describes the configuration layout of the devices comprising the pool, as well as their state and any errors generated from the devices. The state can be one of the following: ONLINE, FAULTED, DEGRADED, UNAVAILABLE, or OFFLINE. If the state is anything but ONLINE, the fault tolerance of the pool has been compromised.
       - The columns in the status output, "READ", "WRITE" and "CHKSUM" are defined as follows:

         + NAME- The name of each VDEV in the pool, presented in a nested order.
         + STATE- The state of each VDEV in the pool. The state can be any of the states found in "config" above.
         + READ- I/O errors occurred while issuing a read request.
         + WRITE- I/O errors occurred while issuing a write request.
         + CHKSUM- Checksum errors. The device returned corrupted data as the result of a read request.
**** Zpool Properties
     - As with many file systems available for GNU/Linux, ZFS has various flags that can be set to tune the file systems behavior.
     - Zpool properties can be set to modify the behavior of both the pool and the datasets that the pool contains.
     - Some properties are read-only.
     - See man zpool for details on properties.
     - Get properties
       - When properties are retrieved the following fields are displayed
         | name     | Name of storage pool                         |
         | property | property name                                |
         | value    | property value                               |
         | source   | Property source, either 'default' or 'local' |
       - =root # zpool get -p "all"= :: Retrieves a list of all properties 
       - =root # zpool get -p PropName1,PropName2= :: retrieves property details for the specified list of properties delimited by a comma with no white space i.e. PropName1,PropName2
     - Set properties
       - Properties that require a string argument do not have an easy way to get the value back to a default value. With other property types, if you try to set the property to an invalid argument then an error will print to the screen displaying available valid values, but it will not indicate which was the default value.
       - If the source column has "default" then the setting has not been user-defined, if it is "local", then it was user-defined.
       - =root # zpool set comment="Contact admins@example.com" tank= :: sets the comment property. This will also change the source from "default" to "local"
       - =root # zpool create -o ashift=12 tank raid1 sda sdb= :: an example of setting properties at zpool creation time.
     - Properties apply to the whole pool, consequently ZFS datasets inherit property settings from the pool.
     - Some properties that are set on the dataset also apply to the whole pool
     - Setting properties only applies to data moving forward and never backward.
     - Properties are not retroactive i.e. if you replace a drive with a larger drive and then set autoexpand property after it has been installed it will not autoexpand. It will instead appear as a smaller drive.
     - 
**** Best Practices & Caveats
     - The idea is to optimize space efficiency, performance and ensure maximum data integrity
     - Best Practices
       - Only run ZFS on 64-bit kernels :: It has 64-bit specific code
       - Install ZFS only on systems with lots of RAM :: ZFS will use 1/2 of the available RAM for the ARC
       - Use ECC RAM when possible for scrubbing data in registers and maintaining data consistency :: The ARC is an actual read-only data cache of valuable data in RAM
       - Use whole disks rather than partitions :: ZFS can make better use of the on-disk cache as a result. If you must use partitions, backup the partition table, and take care when reinstalling data into the other partitions, so you don't corrupt the data in your pool
       - Keep each VDEV in a storage pool the same size :: If VDEVs vary in size, ZFS will favor the larger VDEV, which could lead to performance bottlenecks.
       - Use redundancy when possible :: ZFS can and will want to correct data errors that exist in the pool. You cannot fix these errors if you do not have redundant good copy elsewhere in the pool. Mirrors and RAID-Z levels accomplish this.
       - Consider using RAIDZ-2 or RAIDZ-3 over RAIDZ-1 :: This will increase the probability that you have fully resilvered the necessary data before too many disks failures have occured.
       - Perform regular (at least weekly) backups of the full storage pool :: It's not a backup, unless you have multiple copies. Just because you have redundant disks, does not ensure live running data in the event of a power failure, hardware failure or disconnected cables.
       - Use host spares to quickly recover from a damaged device. Set the "autoreplace" property to on for the pool.
       - Consider using hybrid storage pool with fast SSDs or NVRAM drives :: Using FAST SLOG and L2ARC can greatly improve performance.
       - If using a hybrid storage pool with multiple devices, mirror the SLOG and stripe the L2ARC.
       - If using a hybrid storage pool, and partitioning the fast SSD or NVRAM drive, unless you know you will need it, 1GB is likely sufficient for your SLOG :: Use the rest of the SSD or NVRAM drive for the L2ARC. The more storage for the L2ARC, the better.
       - Keep pool capacity under 80% for best performance :: Due to the copy-on-write nature of ZFS, the filesystem gets hevily fragmented. Email reports of capacity at least monthly.
       - If possible, scrub consumer-grade SATA and SCISI disks weekly and enterprise-grade SAS and FC disks montly :: If not possible then scrub as frequently as possible.
       - Set "autoexpand" to on :: so you can expand the storage pool automatically after all disks in the pool have been replaced with larger ones.
       - Always export your storage pool when moving the disks from one physical system to another.
       - When considering performance, know that for sequential writes, mirrors will always outperform RAID-Z levels. For sequential reads, RAID-Z levels will perform more slowly than mmirrors on smaller data blocks and faster on larger data blocks. For random reads and writes, mirrors and RAID-Z seem to perform in similar manners. Striped mirrors will outperform mirros and RAID-Z in both sequential, and ramdom reads and writes.
       - Compression is disabled by default :: this doesn't make much sense with today's hardware. ZFS compression is extremly cheap, extremly fast, and barely adds any latency to the reads and writes. Also your data will consume less space.
     - Caveats
       - Your VDEVs determine the IOPS of the storage, and the slowest disk in the VDEV will determine the IOPS for the entire VDEV.
       - ZFS uses 1/64 of the available raw storage for metadata. so if you purchased a 1 TB drive, the actual raw size is 976 GiB. After ZFS uses it, you will have 961 GiB of available space. The "zfs list" command will show an accurate representation of your available storage. Plan your storage keeping this in mind.
       - ZFS wants to control the whole block stack. It checksums, resilvers live data instead of full disks, self-heals corrupted blocks, and a number of other unique features. If using a RAID card, make sure to configure it as a true JBOD (or "passthrough mode"), so ZFS can control the disks. If you can't do this with your RAID card, don't use it. Best to use a real HBA.
       - Do not use other volume management software beneath ZFS. ZFS will perform better, and ensure greater data integrity, if it has control of the whole block device stack. As such, avoid using dm-crypt, mdadm or LVM beneath ZFS.
       - Do not share a SLOG or L2ARC Device accross pools. Each pool should have its own physical device, not a logical drive, as is the case with some PCI-Express SSD cards. Use the full card for one pool, and a different physical card for another pool. If you share a physical device, you will create race conditions, and could end up with corrupted data.
       - Do not share a single storage pool accress different servers. ZFS is not a clustered filesystem. Use GlusterFS, Ceph, Lustre or some other clustered filesystem on top of the pool if you wish to have a shared storage backend.
       - Other than a spare, SLOG and L2ARC in your hybrid pool, do not mix VDEVs in a single pool. If one VDEV is a mirror, all VDEVs should be mirrors. If one VDEV is a RAIDZ-1, all VDEVs should be RAIDZ-1. Unless of course, you know what you are doing, and are willing to accpet the consequences. ZFS attempts to balance the data across VDEVs. Having a VDEV of a different redundancy can lead to performance issues and space efficiency concerns and make it very difficult to recover in the event of a failure.
       - Do not mix disk sizes or speeds in a single VDEV. Do mix fabrication dates, however, to prevent mass drive failure.
       - Do not mix disk sizes or speeds in your storage pool at all.
       - Do not mix disk counts accross VDEVs. If one VDEV uses 4 drives, all VDEVs should use 4 drives.
       - Do not put all of the drives from a single controller in one VDEV. Plan your storage, such that if a controller fails, it affects only the number of disks necessary to keep the data online.
       - When using advanced format disks, you must set the ashift value to 12 at pool creation. It cannot be changed after the fact. Use "zpool create -o ashift=12 tank mirror sda sdb" as an example
       - Hot spare disks will not be added to the VDEV to replace a failed drive by default. You MUST enable this feature. set the autoreplace feature to on. Use 'zpool set autoreplace=on tank' as an example.
       - The storage pool will not auto resize itself when all smaller drives in the pool have been replaced by larger ones. You must enable this feature, and you must enable it before replacing the first disk. Use "zpool set autoexpand=on tank' as an example.
       - ZFS does not restripe data in a VDEV nor across multiple VDEVs. Typically, when adding a new device to a RAID array, the RAID controller will rebuild the data, by creating a new stripe width. This will free up some space on the drives in the pool, as it copies data to the new disk. ZFS has no such mechanism. Eventually, over time, the disks will balance out due to the writes, but even a scrub will not rebuild the stripe width.
       - You cannot shrink a zpool, only grow it. This means you cannot remove VDEVs from a storage pool.
       - You can only remove drives from mirrored VDEV using the "zpool detach' command. You can replace drives with another drive in RAIDZ and mirror VDEVs however.
       - Do not create a storage pool of files or ZVOLs from an existing zpool. Race conditions will be present, and you will end up with corrupted data. Always keep multiple pools sperate.
       - The Linux kernel may not assign a drive the same drive letter at every  boot. Thus, you should use the /dev/disk/by-id/ convention for your SLOG and L2ARC. If you don't, your zpool devices could end up as a SLOG device, which would in turn clobber your ZFS data.
       - Don't create massive storage pools "just because you can". Even though ZFS can create 78-bit storage pool sizes, that doesn't mean you need to create one.
       - Don't put production directly into the zpool. Use ZFS datasets instead.
       - Don't commit production data to file VDEVs. Only use file VDEVs for testing scripts or learning the ins and outs of ZFS.
         
***** Zpool Properties
      - As with many file systems available for GNU/Linux, ZFS has various flags that can be set to tune the file systems behavior.
      - Zpool properties can be set to modify the behavior of both the pool and the datasets that the pool contains.
      - Some properties are read-only.
      - See man zpool for details on properties.
      - Get properties
        - When properties are retrieved the following fields are displayed
          | name     | Name of storage pool                         |
          | property | property name                                |
          | value    | property value                               |
          | source   | Property source, either 'default' or 'local' |
        - =root # zpool get -p "all"= :: Retrieves a list of all properties 
        - =root # zpool get -p PropName1,PropName2= :: retrieves property details for the specified list of properties delimited by a comma with no white space i.e. PropName1,PropName2
      - Set properties
        - Properties that require a string argument do not have an easy way to get the value back to a default value. With other property types, if you try to set the property to an invalid argument then an error will print to the screen displaying available valid values, but it will not indicate which was the default value.
        - If the source column has "default" then the setting has not been user-defined, if it is "local", then it was user-defined.
        - =root # zpool set comment="Contact admins@example.com" tank= :: sets the comment property. This will also change the source from "default" to "local"
        - =root # zpool create -o ashift=12 tank raid1 sda sdb= :: an example of setting properties at zpool creation time.
      - Properties apply to the whole pool, consequently ZFS datasets inherit property settings from the pool.
      - Some properties that are set on the dataset also apply to the whole pool
      - Setting properties only applies to data moving forward and never backward.
      - Properties are not retroactive i.e. if you replace a drive with a larger drive and then set autoexpand property after it has been installed it will not autoexpand. It will instead appear as a smaller drive.

***** Best Practices & Caveats
      - The idea is to optimize space efficiency, performance and ensure maximum data integrity
      - Best Practices
        - Only run ZFS on 64-bit kernels :: It has 64-bit specific code
        - Install ZFS only on systems with lots of RAM :: ZFS will use 1/2 of the available RAM for the ARC
        - Use ECC RAM when possible for scrubbing data in registers and maintaining data consistency :: The ARC is an actual read-only data cache of valuable data in RAM
        - Use whole disks rather than partitions :: ZFS can make better use of the on-disk cache as a result. If you must use partitions, backup the partition table, and take care when reinstalling data into the other partitions, so you don't corrupt the data in your pool
        - Keep each VDEV in a storage pool the same size :: If VDEVs vary in size, ZFS will favor the larger VDEV, which could lead to performance bottlenecks.
        - Use redundancy when possible :: ZFS can and will want to correct data errors that exist in the pool. You cannot fix these errors if you do not have redundant good copy elsewhere in the pool. Mirrors and RAID-Z levels accomplish this.
        - Consider using RAIDZ-2 or RAIDZ-3 over RAIDZ-1 :: This will increase the probability that you have fully resilvered the necessary data before too many disks failures have occured.
        - Perform regular (at least weekly) backups of the full storage pool :: It's not a backup, unless you have multiple copies. Just because you have redundant disks, does not ensure live running data in the event of a power failure, hardware failure or disconnected cables.
        - Use host spares to quickly recover from a damaged device. Set the "autoreplace" property to on for the pool.
        - Consider using hybrid storage pool with fast SSDs or NVRAM drives :: Using FAST SLOG and L2ARC can greatly improve performance.
        - If using a hybrid storage pool with multiple devices, mirror the SLOG and stripe the L2ARC.
        - If using a hybrid storage pool, and partitioning the fast SSD or NVRAM drive, unless you know you will need it, 1GB is likely sufficient for your SLOG :: Use the rest of the SSD or NVRAM drive for the L2ARC. The more storage for the L2ARC, the better.
        - Keep pool capacity under 80% for best performance :: Due to the copy-on-write nature of ZFS, the filesystem gets hevily fragmented. Email reports of capacity at least monthly.
        - If possible, scrub consumer-grade SATA and SCISI disks weekly and enterprise-grade SAS and FC disks montly :: If not possible then scrub as frequently as possible.
        - Set "autoexpand" to on :: so you can expand the storage pool automatically after all disks in the pool have been replaced with larger ones.
        - Always export your storage pool when moving the disks from one physical system to another.
        - When considering performance, know that for sequential writes, mirrors will always outperform RAID-Z levels. For sequential reads, RAID-Z levels will perform more slowly than mmirrors on smaller data blocks and faster on larger data blocks. For random reads and writes, mirrors and RAID-Z seem to perform in similar manners. Striped mirrors will outperform mirros and RAID-Z in both sequential, and ramdom reads and writes.
        - Compression is disabled by default :: this doesn't make much sense with today's hardware. ZFS compression is extremly cheap, extremly fast, and barely adds any latency to the reads and writes. Also your data will consume less space.
      - Caveats
        - Your VDEVs determine the IOPS of the storage, and the slowest disk in the VDEV will determine the IOPS for the entire VDEV.
        - ZFS uses 1/64 of the available raw storage for metadata. so if you purchased a 1 TB drive, the actual raw size is 976 GiB. After ZFS uses it, you will have 961 GiB of available space. The "zfs list" command will show an accurate representation of your available storage. Plan your storage keeping this in mind.
        - ZFS wants to control the whole block stack. It checksums, resilvers live data instead of full disks, self-heals corrupted blocks, and a number of other unique features. If using a RAID card, make sure to configure it as a true JBOD (or "passthrough mode"), so ZFS can control the disks. If you can't do this with your RAID card, don't use it. Best to use a real HBA.
        - Do not use other volume management software beneath ZFS. ZFS will perform better, and ensure greater data integrity, if it has control of the whole block device stack. As such, avoid using dm-crypt, mdadm or LVM beneath ZFS.
        - Do not share a SLOG or L2ARC Device accross pools. Each pool should have its own physical device, not a logical drive, as is the case with some PCI-Express SSD cards. Use the full card for one pool, and a different physical card for another pool. If you share a physical device, you will create race conditions, and could end up with corrupted data.
        - Do not share a single storage pool accress different servers. ZFS is not a clustered filesystem. Use GlusterFS, Ceph, Lustre or some other clustered filesystem on top of the pool if you wish to have a shared storage backend.
        - Other than a spare, SLOG and L2ARC in your hybrid pool, do not mix VDEVs in a single pool. If one VDEV is a mirror, all VDEVs should be mirrors. If one VDEV is a RAIDZ-1, all VDEVs should be RAIDZ-1. Unless of course, you know what you are doing, and are willing to accpet the consequences. ZFS attempts to balance the data across VDEVs. Having a VDEV of a different redundancy can lead to performance issues and space efficiency concerns and make it very difficult to recover in the event of a failure.
        - Do not mix disk sizes or speeds in a single VDEV. Do mix fabrication dates, however, to prevent mass drive failure.
        - Do not mix disk sizes or speeds in your storage pool at all.
        - Do not mix disk counts accross VDEVs. If one VDEV uses 4 drives, all VDEVs should use 4 drives.
        - Do not put all of the drives from a single controller in one VDEV. Plan your storage, such that if a controller fails, it affects only the number of disks necessary to keep the data online.
        - When using advanced format disks, you must set the ashift value to 12 at pool creation. It cannot be changed after the fact. Use "zpool create -o ashift=12 tank mirror sda sdb" as an example
        - Hot spare disks will not be added to the VDEV to replace a failed drive by default. You MUST enable this feature. set the autoreplace feature to on. Use 'zpool set autoreplace=on tank' as an example.
        - The storage pool will not auto resize itself when all smaller drives in the pool have been replaced by larger ones. You must enable this feature, and you must enable it before replacing the first disk. Use "zpool set autoexpand=on tank' as an example.
        - ZFS does not restripe data in a VDEV nor across multiple VDEVs. Typically, when adding a new device to a RAID array, the RAID controller will rebuild the data, by creating a new stripe width. This will free up some space on the drives in the pool, as it copies data to the new disk. ZFS has no such mechanism. Eventually, over time, the disks will balance out due to the writes, but even a scrub will not rebuild the stripe width.
        - You cannot shrink a zpool, only grow it. This means you cannot remove VDEVs from a storage pool.
        - You can only remove drives from mirrored VDEV using the "zpool detach' command. You can replace drives with another drive in RAIDZ and mirror VDEVs however.
        - Do not create a storage pool of files or ZVOLs from an existing zpool. Race conditions will be present, and you will end up with corrupted data. Always keep multiple pools sperate.
        - The Linux kernel may not assign a drive the same drive letter at every  boot. Thus, you should use the /dev/disk/by-id/ convention for your SLOG and L2ARC. If you don't, your zpool devices could end up as a SLOG device, which would in turn clobber your ZFS data.
        - Don't create massive storage pools "just because you can". Even though ZFS can create 78-bit storage pool sizes, that doesn't mean you need to create one.
        - Don't put production directly into the zpool. Use ZFS datasets instead.
        - Don't commit production data to file VDEVs. Only use file VDEVs for testing scripts or learning the ins and outs of ZFS.
          
**** ZFS Administration
***** Merkle Trees
      - Cryptographic hash trees invented by Ralph Merkle
      - Each datablock is hashed with SHA-256 algorithm.
      - Used by ZFS to verify integrity of entire file system.
      - The purpose of cryptographically hasing each block is to ensure data integrity. We can check to see if the hash block matches its parent hash block. If it does, then the data was not corrupt, otherwise the block is corrupt.
      - The tree is binary (but Merkle trees do not need to be binary)
      - Each parent node is the SHA-256-bit hash of concatonating its children hash blocks together.
      - The root of the tree (also called super block, super node or uber block) is a SHA-256 bit hash of its concatenated children.
      - The uber block is used to verify the integrity of the entire Merkle tree.
      - If a data block changes, all of the parent hash blocks change all of the way up to and including the uber block.
      - When a storage pool is scrubbed ZFS verifies every SHA-256 hash in the Merkle tree to make sure there is no corrupted data, or inconsistencies.
      - If the pool has redundancy and a corrupted data block is found, ZFS will look elsewhere in the pool for a good datablock at the same location to fix the corrupted one and then reverify the SHA-256 hash in the Merkle tree.
***** Copy-on-write (COW)
      - A data storage technique:
        1. The data-block targeted for modification is copied
        2. The copy is modified
        3. The file system pointers are updated to point to the new data-block location
        4. The original data-block is released for use by the applicaiton
      - A consequence is that the underlying data becomes severly fragmented.
      - COW Fascilitates taking snapshots
        - This is such a huge bennefit that it outways the consequence of fragmentation
        - As the file system is being written to COW makes its way through to the end of the disk. The original copy of each data-block ramains for a long time following a write.
        - A snapshot is treated as a first class filesystem.
        - If a data-block is overriden following a snapshot, then its copy will be placed in the snapshot filesystem; otherwise, it remains where it is marked as free for use by the applicaiton.
        - This is possible because a snapshot is a copy of the hash tree at that exact moment. Thus, unless the snapshotted data-blocks are overritten, they take up hardly any space.
      - When a data-block is updated, the hash tree is also updated starting with the child-block hash and moving up the tree through all of its ancestor node hashes until the super-node hash is finally updated.
      - Fragmentation can have a serious performance impact. To help mitigate fragmentation ZFS marks slabs of disk space for data-block copies so that they stay close to one another.
        - Typically file systems write ata in 4 KiB blocks where as ZFS writes data in 128 KiB blocks.
        - This minimizes fragmentation by an order of 32.
        - The SLAB allocator will allocate a SLAB and then partition that into 128 KiB blocks.
        - ZFS delays syncing data to disk every 5 seconds. All data remaining is flushed to disk every 30 seconds. That means a lot of data flushes to disk at once into the SLAB keeping data-blocks close and therefore increasing the chance that related data is kept together in the same SLAB. Thus reducing fragmentation.
      - COW is also used by VM images such as Qemu and many other file systems.
***** Creating File Systems
      - ZFS datasets are file systems
      - Storage pools are not meant to store data directly. Instead, multiple ZFS datasets sharing the same storage pool store the data.
      - By default each dataset has full access to the entire storage pool.
      - As files are placed in the dataset, then the pool marks that storage as unavailable to all datasets. Therefore, there is no need to create logical volumes of limited size.
      - Datasets can have quotas that limit there size.
      - The big advantage is that there is no need to worry about preallocated block devices because ZFS manages the entire stack and  therefore understands how much data has been occupied and how much is available.
      - Create a dataset
        - =root # zfs create pool-name/dataset-name= :: Creates a data-set named data-set in the pool named pool-name
      - Mounting DataSets
        - Datasets are not exportable block devices by default so there is nothing to directly mount; hence, there is nothing in /etc/fstab for persistence accross reboots.
        - Instead, the storage pool can be imported and zfs mount can be run for each dataset.
        - =root # zfs mount /<pool-name>/<dataset-name>= :: <dataset-name has bee mounted to the pool named <pool-name>
        - =root # zfs unmount /<pool-name>/<dataset-name>= :: <dataset-name has been umounted from <pool-name>
        - =root # zfs set mountpoint=/mnt/<data-setname> <pool-name>/<data-setname>= :: Sets the data-set mountpoint property
      - Nested Datasets
        - Nested datasets makes it possible to change properties to one dataset without affecting the other. i.e. setting compression on /var/log, but not on the parent data-set /var. 
        - =root # zfs create /<pool-name>/<data-setname>/log= :: creates a nested dataset named "log"
      - Destroy a dataset
        - Frees up the blocks used by the dataset.
        - Cannot be reverted without a previous snapshot.
        - =root # zfs destroy /<pool-name>/<data-setname>=
      - Rename a dataset
        - =root # zfs rename /<pool-name>/<data-setname1> /<pool-name>/<data-setname2>=
***** Compression and Duplication
      - Compression
        - is transparent. Zfs handles compression/decompression on the fly.
        - is enabled / disabled per dataset. The default is disabled.
        - Supported algorithms
          - LZJB
            - Default
            - A Lempel-Ziv algorithm written by author of ZFS Jeff Bonwick
            - Designed to be fast with tight compression ratios.
          - LZ4
            - New to ZFS. Offers tighter compression ratios than LZJB.
            - Is now preferred over LZJB.
          - ZLE
            - Super fast with very light compression ratios
          - Gzip{1-9}
            - Where 1 is as fast as possible and 9 is as compressed as possible.
            - The default is 6. This is the default for gnu linux.
        - Not retroactive. Applys only to newly commited or modified data.
        - There is next to know performance impact for enabling it so it is recommended to enable compression on all datasets.
        - Commands:
          - zfs set compression=lz4 tank/log :: To set compression to lz4 for dataset /tank/log
      - Duplication
        - Anotherway to save disk space in conjunction with compression
          - 3 types
            1. file
               - most performant and least costly on system resources.
               - each file is hashed with crypto hash agorithm. Files with matching hashes are only stored once in disk and metadata is used to lookup those files
               - saves significant disk space.
               - Drawback: if a single byte changes the entire modified file must be written to disk since it will no longer share the same hash as any other file in the file system (unless it does of course :-). For a large file that could have a serious performance impact.
            2. block
               - zfs uses this only
               - block duplication shares all the same blocks in a file, minus blocks that are different. Only unique blocks are written to disk and shared blocks can be referenced in RAM.
               - More efficient than byte duplication and more flexible than file duplication.
               - Drawback: Requires lots of RAM to keep track of shared blocks
               - Since file systems read and write data in block segments this option makes the most sense for modern filesystems.
            3. byte
               - most expensive because anchor points must be kept to determine the regions where duplicate and unique bytes start and end.
               - Works well for storage where file may be stored multiple times, but not necessarily under the same blocks. i.e. mail attachments.
        - Shared blocks are stored in a duplication table in RAM. The more shared blocks that exist on disk, the larger the duplication table becomes. If the system runs out of RAM then swap is used (performance hit).
        - It is turned off by default.
        - zfs get -p dedup :: shows if duplication is on/off on each filesystem, volume, snapshot
        - Because duplication requires a large amount of RAM as the size of the file system grows we will not consider it further.
***** Snapshots
      - First class read-only file system. It is a mirrored copy of the state of the file system at the time the snapshot was taken.
      - Can keep 2^64 snapshots in a pool.
      - Don't require additional backing store. They use the same storage pool as the rest of the data.
      - A snapshot is a readonly copy of the Merkle tree. Even the snapshots properties are readonly.
      - Snapshot creation is very fast.
      - As the current state diverges from the snapshot, the snapshot will begin to store the data of the things that change. So more the current state diverges from the snapshot the more space the snapshot consumes.
      - Creating Snapshots
        - Two types:
          - pool snapshots
            - pool@snapshot-name :: snapshot name syntax
            - =zfs snapshot tank@tuesday= :: takes a snapshot of the tank pool named tuesday
          - dataset snapshots
            - pool/dataset@snapshot-name :: snapshot name syntax
            - =zfs snapshot tank/test@tuesday= :: Takes a snapshot of the tank/test dataset named tuesday
      - Listing Snapshots
        - snapshots are stored in a hidden directory named .zfs. It is hidden even from ls -a command, but you can still cd to .zfs even though you cannot list it with ls -a.
        - The .zfs directory can be made visible by changing the snapdir property on the dataset.
          - =zfs set snapdir=visible tank test= makes the .zfs directory visible to the ls -a command. Valid values are hidden and visible.
        - =zfs list -t snapshot= :: will list the snapshots even if the snapdir is set to hidden.
        - =zfs list -r -t snapshot tank= :: this will list only snapshots under tank recursivly.
      - Destroying Snapshots
        - =zfs destroy tank/test@tuesday= :: Similar to destroying a storage pool or a dataset.
        - A snapshot of a dataset is a child file system of a dataset, therefore the dataset cannot be destroyed until all the snapshots (and nested datasets) of the dataset have been destroyed.
      - Renaming Snapshots
        - Must be renamed in the storage pool and ZFS dataset from which they were created.
        - =zfs rename tank/test@tuesday tank/test@tuesday-19:15=
      - Rolling Back to a Snapshot
        - Discards changes between the snapshot and the current state.
        - Can only roll back to most recent snapshot.
        - To roll back to a snapshot earlier than the most recent snapshot you must destroy all snapshots inbetween :: Yuck!
          #+BEGIN_SRC shell
          # zfs rollback tank/test@tuesday
          cannot rollback to 'tank/test@tuesday': more recent snapshots exist
          use '-r' to force deletion of the following snapshots:
          tank/test@wednesday
          tank/test@thursday
          #+END_SRC
        - The file system must be unmounted before the rollback can commence. :: This means downtime!
***** Clones
      - A writeable filesystem that was "upgraded" from a snapshot; hence, only snapshots can be cloned.
      - The clone depends on the snapshot to exist since it is a copy of the Merkle tree of the snapshot.
        - All clones must be destroyed before the snapshot that those clones depend on can be destroyed.
      - Clones do not take up additional space from their snapshot until they start to diverge at which point they only store the changes.
      - Creating a Clone
        - the clone does not need to reside in the same dataset as the snapshot, but it does need to reside in the same storage pool.
        - To clone tank/test@tuesday snapshot and name it tank/tuesday:
          #+BEGIN_SRC shell
          # zfs clone tank/test@tuesday tank/tuesday
          # dd if=/dev/zero of=/tank/tuesday/random.img bs=1M count=100
          # zfs list -r tank
          NAME           USED  AVAIL  REFER  MOUNTPOINT
          tank           161M  2.78G  44.9K  /tank
          tank/test     37.1M  2.78G  37.1M  /tank/test
          tank/tuesday   124M  2.78G   161M  /tank/tuesday
          #+END_SRC
      - Destroying a Clone
        - Cannot destroy a snapshot until all dependant clones have been destroyed.
        - =zfs destroy tank/tuesday= :: destroys the clone named tuesday in the storage pool tank. Just like destroying any other dataset.
      - Schedualing Snapshots
        - Snapshots are memory and time cheap, so it is recommended to take plenty of snapshots
        - consider creating a cron job to take snapshots hourly/daily/weekly/monthly maybe even by the minut.
        - A possible schedual actually used by Time Slider:
          - 15 min :: keeping 4 snapshots
          - hourly :: keeping 24 snapshots
          - daily :: keeping 31 snapshots
          - weekly :: keeping 7 snapshots
          - montly :: keeping 12 snapshots
      - Since both snapshots and clones are cheap, take advantage of them..
        - clones are useful to test deploying virtual machines, or development environments cloned from production environments for example.
        - When you are done with a clone you can easily destroy it without affecting the parent dataset.
***** ZFS Send / Receive
      - ZFS Send
        - Sending involves first taking a snapshot of the data and then sending the snapshot to ensure the data remains consistant over the duration of the transfer.
          - Consequently there is no need to take the file system offline to make a backup.
        - By default the data is sent to a single file that can be then moved like any other file.
        - Sending a snapshot creates an output stream that must be directed.
          - basic send to file
            #+BEGIN_SRC shell
            # zfs snapshot tank/test@tuesday
            # zfs send tank/test@tuesday > /backup/test-tuesday.img
            #+END_SRC
          - send to encrypted file. Note that xz compresses the snapshot before encryption.
            #+BEGIN_SRC shell
            # zfs snapshot tank/test@tuesday
            # zfs send tank/test@tuesday | xz | openssl en -aes-256-cbc -a -salt > /backup/test-tuesday.img.xz.asc
            #+END_SRC
      - ZFS Receive
        - receive also works with streams.
        - receive the snapshot image into any storage pool and it will create the necessary dataset.
          - =zfs receive tank-test2 < /backup/test-tuesday.img=
        - to recieve an encrypted and compressed snapshot we would do the following:
          - =openssl enc -d -aes-256-cbc -a -in /backup/test-tuesday.img.xz.asc | unxz | zfs receive tank/test2=
      - Combining Send and Recieve
        - =zfs send tank/test@tuesday | zfs receive pool/test= :: Send directly from snapshot to local storage pool
        - =zfs send tank/test@tuesday | ssh user@server@example.com "zfs receive pool/test= :: Send directly from local snapshot to remote storage pool
***** ZVOLs
      - A ZFS Volume that has been exported to the system as a block device.
      - Resides in a storage pool.
      - Takes advantage of the underlying things that ZFS offers (RAID, copy-on-write, scrubbing, duplication / compression etc).
      - Takes advantage of the ZIL and ARC.
      - ZVOLs are first class block devices and so you can do anything with them that you can do with any other block device.
      - Creating a ZFS Vol
        - uses the -V flag from zfs create command and requires a size.
          #+BEGIN_SRC shell
          # zfs create -V 1G tank/disk1
          # ls /dev/zvol/tank/disk1
          lrwxrwxrwx 1 root root 11 Dec 20 22:10 /dev/zvol/tank/disk1 -> ../../zd144
          # ls /dev/tank/disk1
          lrwxrwxrwx 1 root root 8 Dec 20 22:10 /dev/tank/disk1 -> ../zd144
          #+END_SRC
        - Nearly instantaneous regardless of size.
        - Compare creating a block device with gnu/linux from a file image and having /dev/loop0 represent the file. 
          - As with any other block device it can be formated and added to swap, but it is limited. 
          - By default there are only 8 loop back devices. This number can be changed of course.
          - In contrast with zfs you can create 2^64 ZVOLs.
          - Also, the gnu/linux way requires a pre-allocated image on top of the file system so there are three layers to manage. The block device, the file and the blocks on the file system.
          - In contrast ZVOL block devices are exported right off the storage pool just like any other dataset.
      - Swap on a ZVOL
        - create 1GB of swap on a ZVOL and add it to the kernel
          #+BEGIN_SRC shell
          # zfs create -V 1G tank/swap
          # mkswap /dev/zvol/tank/swap
          # swapon /dev/zvol/tank/swap          
          #+END_SRC
          - now running free will show the swap.
      - Ext4 on a ZVOL
        - it is possible to create a zvol, partition it and make multiple file systems on it.
        - The advantage is that you can enable compression, make snapshots, send the dataset to offsite back and do all the other things that zfs allows you to do with datasets that you can't do with those same file systems without zfs.
        - Here we will do just that:
          #+BEGIN_SRC shell
          # zfs create -V 100G tank/ext4
          # fdisk /dev/tank/ext4
          ( follow the prompts to create 2 partitions- the first 1 GB in size, the second to fill the rest )
          # fdisk -l /dev/tank/ext4

          Disk /dev/tank/ext4: 107.4 GB, 107374182400 bytes
          16 heads, 63 sectors/track, 208050 cylinders, total 209715200 sectors
          Units = sectors of 1 * 512 = 512 bytes
          Sector size (logical/physical): 512 bytes / 8192 bytes
          I/O size (minimum/optimal): 8192 bytes / 8192 bytes
          Disk identifier: 0x000a0d54

          Device Boot      Start         End      Blocks   Id  System
/dev/tank/ext4p1            2048     2099199     1048576   83  Linux
/dev/tank/ext4p2         2099200   209715199   103808000   83  Linux
 
          # mkfs.ext4 /dev/zd0p1
          # mkfs.ext4 /dev/zd0p2
          # mkdir /mnt/zd0p{1,2}
          # mount /dev/zd0p1 /mnt/zd0p1
          # mount /dev/zd0p2 /mnt/zd0p2
          #+END_SRC
        - Now we can enable compression, copy over some data, and take a snapshot
          #+BEGIN_SRC shell
           # zfs set compression=lzjb pool/ext4
           # tar -cf /mnt/zd0p1/files.tar /etc/
           # tar -cf /mnt/zd0p2/files.tar /etc /var/log/
           # zfs snapshot tank/ext4@001          
          #+END_SRC
      - ZVOL storage for VMs
        - It is common to use block devices as the backend storage for VMs.
        - You can attach the block device to a virtual machine and from its perspective the system will have a /dev/vda or /dev/sda depending on the setup.
        - The vm will get all of the bennefits that ZFS provides, such as snapshots, compression, deduplication, data integrity, drive redundancy, etc.
        - TODO: add snippet here of kvm configuration using block device.
      - caveat: ZFS is not a clustered file system. You cannot replicate zvols across a cluster.
***** ZFS Properties
      - datasets contain properties that can be retrieved and altered.
      - Many properties are read-only
      - There are lots of dataset properties.
      - properties can be inherited from parent datasets.
      - Custom property support.
      - ZFS Get
        - =# zfs get used,available,compressionratio tank/test= Retrieve multiple properties for a dataset via a comma seperated list.
        - =# zfs get all tank/test= Retrieve all of the properties for a dataset.
      - Inheritance
        - tank is a storage pool, but it is also a valid ZFS dataset. Any datasets nested under tank may inherit some properties.
        - We set the compression algorithm on the storage pool file system tank, so all nested datasets inherit this property from tank.
          #+BEGIN_SRC shell
          # zfs create -o compression=gzip tank/test/one
          # zfs get -r compression tank
          NAME           PROPERTY     VALUE     SOURCE
          tank           compression  lzjb      local
          tank/test      compression  lzjb      inherited from tank
          tank/test/one  compression  gzip      local
          #+END_SRC
        - zfs inherit command sets a properties value to be inherited from its parent.
          #+BEGIN_SRC shell
          # zfs inherit compression tank/test/one
          # zfs get -r compression tank
          NAME           PROPERTY     VALUE     SOURCE
          tank           compression  lzjb      local
          tank/test      compression  lzjb      inherited from tank
          tank/test/one  compression  lzjb      inherited from tank
          #+END_SRC
        - To set a parents property and have the children all inherit use the -r flag
          #+BEGIN_SRC shella
          # zfs set compression=gzip tank
          # zfs inherit -r compression tank/test
          # zfs get -r compression tank
          NAME           PROPERTY     VALUE     SOURCE
          tank           compression  gzip      local
          tank/test      compression  gzip      inherited from tank
          tank/test/one  compression  gzip      inherited from tank
          #+END_SRC
        - Caution: recursion can be dangerous. This next command turns off compression on all nested datasets of tank. This caution applies to datasets, volumes and snapshots. Anywhere recursion can be used.
          #+BEGIN_SRC shell
          # zfs inherit -r compression tank
          # zfs get -r compression tank
          NAME           PROPERTY     VALUE     SOURCE
          tank           compression  off       default
          tank/test      compression  off       default
          tank/test/one  compression  off       default
          #+END_SRC
      - User Dataset Properties
        - Motivation: To create custom properties for appliations designed specifically for ZFS. I am not making any, so I am not taking notes here.
      - Considerations:
        - Always read the man pages before altering properties.
        - Some dataset properties are not fully implemented on ZFS on Linux.
        - Some dataset properties apply to the whole pool, such as duplication.
        - Many properties only apply to newly written datap; hence, are not retroactive.
        - The parent storage pool is also a ZFS dataset, any child datasets will inherit non-default properties.
          - the same is true for nested datasets, snapshots and volumes.
***** Best Practices / Caveats
*** KVM
    - Kernel-based Virtual Machines
    - References:
      - https://wiki.gentoo.org/wiki/QEMU
      - https://wiki.gentoo.org/wiki/QEMU/Linux_guest
    - grep --color -E "vmx|svm" /proc/cpuinfo :: finds lines showing kvm support
    - /dev/kvm :: exists if kvm support is enabled
    - It is most efficient to create zvol and install the kvm onto the block device.
      - When creating a zvol for a kvm be sure to set the block size to a multiple of the OS page-size.
        - 1xpage-size is most efficient for writes
        - {2,4}xpage-size provides good efficiency and takes better advantage of transparent compression.
        - {>4}xpage-size provides better compression, but should only be used for volumes with large data that does not get modified frequently.
    - =sudo zfs create -p -b 8K -o compression=lz4 -o primarycache=none -o secondarycache=none -V 20G tank/vm/prod-web-1=
    - =sudo qemu-system-x86_64 -machine help= :: outputs a list of supported systems.
    - =sudo qemu-system-x86_64 -cpu help= :: outputs a list of supported cpu architectures
    - =gpasswd -a dustfinger kvm= :: allows me to start a vm without root.
    - The command to create a kvm on a block device
      #+BEGIN_SRC bash
      # qemu-system-x86_64 -enable-kvm -global ide-drive.physical_block_size=4096 -drive file=tank/vm/prod-web-1,if=ide,media=disk,aio=threads -machine pc,accel=kvm,iommu=on -cpu host -smp 4 -m 8G,slots=2,maxmem=16G -name prod-web-1 -usb -nographics

qemu-system-x86_64 -enable-kvm -global ide-drive.physical_block_size=4096 -drive file=tank/vm/prod-web-1,if=virtio,index=0,cach=none,format=raw -machine pc,accel=kvm,iommu=on -cpu host -smp 4 -m 8G,slots=2,maxmem=16G -name prod-web-1 -usb -nographics

 
      #+END_SRC
    - Install libvirt to manage our kvms
      - virt-manager or virsh commandline can be used to manage this.

*** sys-process/audit
    - References:
      - http://people.redhat.com/sgrubb/audit/visualize/index.html
    - TODO

*** linuxcontainers
    - Resources
      - https://linuxcontainers.org/
    - TODO:
      - Is this something I might like to take advantage of in the future?
*** OpenStack
    - Glossary
      - SLA :: Service Level Agreement
    - Cloud Concepts
      - On demand
        - A sence of immediate. No need to put in a request for permission.
      - Elastic
        - Grow and Shrink on demand
      - Self Service
        - Available through a service catalogue.
        - No need for complicated configuraitons or programatic setups
      - On-premis computing Service Stack
        - On-premises data center management model
          - Applicaitons
          - Data
          - Runtime
          - Middleware
          - O/S
          - Virtualizaiton
          - Servers
          - Storage
          - Networking
        - Managed by the IT Department. Not self service.
      - *-As-a-service
        - Service :: in the cloud computing environment means
          - used to describe control layers in the computing stack
          - underlying services are abstracted
          - Consumer does not need to worry about lower layers.
          - Focus on making "agnostic" service layers
        - IaaS :: Infrastructure-as-a-service
          - most common *-As-a-service model
          - Stack
            - Consumer Controlled
              - Applications
              - Data
              - Runtime
              - Middleware
              - O/S
            - Service Provider Controlled
              - Virtulization
              - Servers
              - Storage
              - Networking
          - Abstraction of hardware layers
            - Business consumer requests guest resources
          - No need to buy bare metal hardware
            - on-demand growth or reduction
        - PaaS :: Platform-as-a-service
          - Stack
            - Consumer Controlled
              - Applications
              - Data
            - Service Provider Controlled
              - Runtime
              - Middleware
              - O/S
              - Virtualization
              - Servers
              - Storage
              - Networking
          - Further abstraction up to application layer
            - Busniess consumer builds apps and manages data
          - Platform agnostic deployments
            - less time developing for rigit platforms
        - AaaS :: Anything-as-a-service
          - FWaaS :: Firewall-as-a-service
          - DBaaS :: Database-as-a-service
          - LBaaS :: Load balancing-as-a-service
          - DaaS :: Desktop-as-a-service
          - DRaaS :: Disaster recovery-as-a-service
      - Public, Private and Hybrid Cloud
        - Public Cloud :: Exposed to the internet
        - Private Cloud :: On-premis cloud environment
        - A Hybrid Cloud includes a vpn connecting the public and private cloud. The consumer then makes requests to the hybrid cloud and the resources are managed where they need to be i.e. in the public or in the private cloud. Also, resources can be moved from one environment to another as required.
      - APIs & Software Abstraction Layer
        - No need to install binary applications
        - Common methods to access resources
        - Open access regardless of platform
        - No language restrictions
        - REST :: Representational State Transfer
        - North Bound and South Bound
          - APIs exposed in two directions
          - Service Consumers use Northbound APIs
          - Southbound APIs allow for further abastraction into the hardware layers
          - Allows for programmatic control of hardware and software
          - Less regidity
        - Layering and API Access
          - Consumer makes api request northbound into the cloud. 
          - The Restful service makes southbound API calls to the compute resources
          - The Compute resources make southbound API calls to the storage resoruces
      - Modulare Infrastructure
        - Seperate Compute, Networking and Storage
        - Underneath is the shared service layer
        - Everything is accessed via an API
      - Service Dependencies
        - Host O/S
        - Database Service
          - MySQL is most common
        - Message Queuing Service (MQ)
          - Rabbid MQ (Common choice)
          - QPID
        - L2/L3 Networking
      - OpenStack Core Services
        - Minimal Required Services
          - An Identity Service Environment
          - A Comput Service Environment
          - An Image Service Environment (Virtual Machine Template Images)
          - A Networking Service Environment
        - Building OpenStack Core Services
          1. Install Host OS
          2. Install OpenStack Service Dependencies
             - Message Queuing, Database Services required to operate the other core services and how they interact with eachother
          3. Identity Environment
             - Provides Authorization and Authentication for our OpenStack cloud
          4. Compute Environment
             - Runs the virtual machines
          5. Image Environment
             - Delivers images / templates for our virtual machines
          6. Networking Environment
             - Public, Private and Hybrid
        - Dashboard
          - Named Horizon
          - Provides a web-based self-service portal to interact with underlying OpenStack services, such as launching an instance, assigning IP addresses and configuring access controls.
        - Compute
          - Named Nova
          - Manages the lifecycle of compute instances in an OpenStack environment. Responsibilities include spawning, scheduling and decommissioning of virtual machines on demand.
        - Networking
          - Named Neutron
          - Enables Network-Connectivity-as-a-Service for other OpenStack services, such as OpenStack Compute. Provides an API for users to define networks and the attachments into them. Has a pluggable architecture that supports many popular networking vendors and technologies.
        - Storage
          - Object Storage
            - Named Swift
            - Stores and retrieves arbitrary unstructured data objects via a RESTful, HTTP based API. It is highly fault tolerant with its data replication and scale-out architecture. Its implementation is not like a file server with mountable directories. In this case, it writes objects and files to multiple drives, ensuring the data is replicated across a server cluster.
          - Block Storage
            - Named Cinder
            - Provides persistent block storage to running instances. Its pluggable driver architecture facilitates the creation and management of block storage devices.
        - Shared Services
          - The Identity Service
            - Named Keystone
            - Authentication
              - Identifies who you are
            - Authorization
              - Determines if you are allowed to do what you requested to do.
            - Includes:
              - Users
              - Services measured for different authorization and authentication requirements
              - Provides a catalogue of end points for all OpenStack Services
            - Uses tokens to authenticate and maintain session state.
            - Workflow
              - Tenants
                - ************ GO BACK AND FILL THIS IN>>>
          - The Image Service
            - Named Glance
            - Stores and retrieves virtual machine disk images. OpenStack Compute makes use of this during instance provisioning.
          - Telemetry
            - Named Ceilometer
            - Monitors and meters the OpenStack cloud for billing, benchmarking, scalability, and statistical purposes.
        - Higher Level Services
          - Orchastration
            - Named Heat
            - Orchestrates multiple composite cloud applications by using either the native HOT template format or the AWS CloudFormation template format, through both an OpenStack-native REST API and a CloudFormation-compatible Query API.
    - Archetecture
      - Technical Considerations
        - Comput Resource Design
          - overcommit ratio :: the ratio of available virtual resources to available physical resources. This ratio is configurable for CPU and memory. The default CPU overcommit ratio is 16:1, and the default memory overcommit ratio is 1.5:1
        - Network Resource Design
          - Network resources are devided into segments. Each segment provides access to particular resources.
          - PLan for either a physical or logical seperation of network segments used by operators and tenants.
          - Additional network segments may be added to provide access to internal services such as the message bus and database used by various services. 
          - The network services require network communcation paths which should also be seperated from the other networks.
          - Segregating these services into different networks helps to protect sensitive data and protects against unauthorized access to services.
          - Neutron gives full control over the creation of virtual network resources to tenants.
          - Tunneling protocols are used to establish encapsulated communication paths over existing network infrastructure in order to segment tenant traffic.
            - It is common to tunnel over GRE and encapsulate with VXLAN and VLAN tags.
          - Three recommended Network Segments
            1. Public Network Segment: Used to access REST APIs by tenants and operators
               - Controller nodes and Swift proxies are the only devices connected to this segment.
               - Optionally serviced by hardware load balancers.
            2. Hardware Administration Segment:
               - Used by admins to administer the hardware
               - Configuration management tools use this segment to deploy software and services onto new hardware.
               - Communicates with every hardware node
               - Optionaly also used for internal services such as providing access to the message bus and database services
               - This segment must be properly secured.
            3. Application Network Segment:
               - Provides applications access to physical network and allows consumers to connect with appliations.
               - Compute resource nodes and network gateway services which allow application data to access the physical network from outside of the cloud must have access to this segment.
        - Object Storage Design
          - When designing hardware resources for storage the primary goal is to maximize storage in each resource node and keep the cost per terabyte to a minimum.
            - Often involved utilizing servers that can hold a large number of spinning disks.
            - Options include using 2u servers with directly attached storage or external chasis with a large number of drives.
            - The Swift Ring has the ablity to assign weights to drives allowing for the use of a wide range of drive technology. Take advantage of this by always purchasing the cheapest storage options at the time, then add an appropriate weight to the new drives so that their use is appropriately balanced.
        - Block Storage Design
          - It is recommended to create multiple storage pools of different types so that tenants can choose the an appropriate storage solution that meets the neads of their application.
          - It is possible to provide tenants with a large catalogue of storage services with a variety of performance levels and redundancy options.
        - Recommended Core Services
          - Compute, Networking, Image Service, Identity, Dashboard, Telemetry module.
          - Consider also including: Object Storage (swift) and Block Storage (cinder)
        - Supplemental Software
          - Includes databases and message queues, and may also involve software to provide high availability of the OpenStack environment. 
          - Design decisions around the underlying message queue might affect the required number of controller services, as well as the technology to provide highly resilient database functionality, such as MariaDB with Galera.
          - HAProxy is a software load balancer. It provides highly avaiable API access and SSL termination.
          - High Availability
            - Software with requirements such as HAProxy must be made highly available.
            - Keepalived or Pacemaker with Corosync provides high availability.
            - Pacemaker and Corosync can provide active-active or active-passive highly available configuration depending on the specific service in the OpenStack environment.
            - Assumes a two node controller infrastructure where one node may be running certain services in standby mode.
          - Aleviate load to the Identity Service
            - Memcached is a distributed memory object system.
              - used to cache tokens
              - Can help aleviate some bottle necks in the underlying authentication system.
            - Redis is a key-value store.
            - Deploy Redis and Memcache onto the infrastructure nodes providing OpenStack Services.
        - Controller Infrastructure Nodes
          - Provides management services for the end user.
          - Provides internal services for operating the cloud.
          - Run message queueing services that carry system messages between each services.
          - Performance
            - Performance issues in the message bus would lead to delays sending messages where they need to go.
              - This condition will cause delays in operation functions such as spinning up and deleting instances, provisioning new storage volumes and managing network resources.
              - Also advesly affects auto-scaling features.
              - Therefore it is important to design the hardware used to run the controller infrastructure as outlined above in the Hardware Selection section.
            - Issues can arrise serving concurrent users. Ensure that the APIs and Horizon services are load tested to ensure that you are able to serve your customers
              - Particular attention should be given to the Identity Service which provides authentication and authorization for all services both internally and to the end-user.
              - If the Identity Service is not sized appropriately it can lead to a degradation of overall performance.
        - Network Performance
          - It is possible to provide a mix of networking capabilities by utilizing different network interface speeds.
          - Performance can be boosted significantly with hardware load balancers.
          - hardware load balancers can also perform SSL termination.
          - It is important to understand the SSL offloading capabilities of the devices selected.
        - Compute Host
          - Performance is primarly affected by choice of hardware specifications for the compute nodes including CPU, Memory and disk type.
          - The overcommit ration applied to resources also has an impact on performance. The default is 16:1 of the CPU and 1.5 of the Memory.
          - Overcommit rations that are too high lead to a condition known as "noisy-neighbour".
          - It is fine to stick with the defaults, but be sure to monitor the environment as its usage increases for the noisy-neighbour activity.
        - Storage Performance
          - Block Storage
            - Hardware specification is an important factor for performance.
              - Can use enterprise storage solutions such as NetApp or EMC
              - Can use scale out storage such as GlusterFS or Ceph.
              - Can use directly attached storage to the nodes themselves.
            - Block Storage can be deployed so that traffic traverses the host network.
              - In this case the front side API traffic performance could adversly affect block storage performance.
              - Consider using a dedicated data storage network with dedicated interfaces on the Controller and Compute Hosts.
          - Object Storage performance is affected by a number of design choices.
            - User access to Object Storage is through the proxy services which site behind hardware load balancers.
            - Data replication affects performance.
            - 10 GbE or better networking is recommended throughout storage networking architecture.
        - Availability
          - Reference:
            - http://docs.openstack.org/ha-guide/ :: Openstack High Availability Guide
          - Design infrastructure so that no single point of failure exists.
          - Should consider the number of switches, routes and power redundancy as well as the associated bonding of networking providing diverse routes to your highly available switch infrastructure.
          - OpenStack services should be deployed accross multiple servers.
          - It is expected that at least 2 servers be utilized.
          - Monitoring and reporting and server utilization and response times, as well as load testing your system, will guide scale out decisions.
          - Consider that legacy networking (nova-network) provides a feature that removes a single point of failure when it comes to routing. This feature has not yet been impelemnted in OpenStack Networking (neutron).
            - i.e. Multi-host functionality restricts failure domains to the host running that instance.
          - OpenStack Networking the controller servers or seperate networking hosts handle routing. This restriction can be removed by using 3rd party software that maintain highly available L3 routes.
            - It is also possible to move routing to hardware routers and remove it from OpenStack Networking. The switching infrastructure must support L3 routing.
          - If Compute hosts do not provide seemless live migration capability, then when the comput host fails, that instance and any data local to that instance will be deleted.
            - Consider ways of eliminating any single point of failure when a comput host dissapears.
              - Consider shared file system on Enterprise storage or OpenStack block storage.
        - Security
          - Reference
            - http://docs.openstack.org/security-guide/ :: OpenStack Security Guide
          - Security Domain comprise of users, applications, servers or networks that share common trust requirements and expectations
          - Security Domains
            - Public
              - Entirely untrusted.
              - Includes the internet as a whole and all networks for which you have no authority.
            - Guest
              - Handles compute data generated by instances on the clound.
              - Does not handle data generated by services that support the operations of the cloud, such as API calls.
              - If no stringent controls on instance use, or if unrestricted internet access to instances is allowed, then this domain should be considered untrusted.
              - Private cloud providers may consider this network internal and therefore trusted only if they trust all of their tenants.
            - Management
              - Sometimes referred to as the control plane
                Where services interact.
              - The network transports confidential data such as configuration parameters, user names, passwords.
              - This domain is normally considered trusted.              
            - Data
              - Primarly concerned with information pertaining to the storage services.
              - Depending on type of deployment data crossing this network has high integrity, confidential requirements and may have high availability requirements.
              - Trust level is highly dependant on type of deployment.
          - May be mapped to OpenStack deployment individually or combined.
          - Should be mapped based on deployment topology.
          - Domains and trust requirements depend on whether the clound instance is Public, Private or hybrid.
          - API services should be protected by SSL.
      - Operational Considerations
        - Expectations set by the Service Level Agreement (SLA) directly affect knowing when and where you should impemented redundancy and high availability
        - SLA terms that affect design include:
          - API availability guarantees implying multiple infrastructure services and highly available load balancers.
          - Network uptime guarantees affecting switch design, which might require redundant switching and power.
          - Factor in networking security policy requirements in to your deployments.
        - Support and Maintainability
          - Monitoring
            - Specific meters that are critically important to monitor
              - Image Disk Utilization
              - Response time to Compute API
            - It is recommended to leverage existing monitoring systems.
          - Downtime
            - Must create processes and architectures that support:
              - Planned downtime (maintenance)
              - Unplanned downtime (system-faults)
            - To be dictated by the SLA
          - Capacity Planning
            - 
    - Networking
      - Every virtual machine instance has a unique MAC address which is different from the MAC of the compute host.
      - Compute Host must alter the default behavior of NICs because they may recieve frames with destination MAC addresses that match an instance.
        - The appropriate NICs can be configured for promiscuous mode to resolve this issue.
      - VLANs
        - OpenStack can use VLAN's to isolate the traffic of different tenants.
        - If using VLAN for tennant isolation then all switchports must be configured as trunk-ports.
        - It is important to select a VLAN range that the infrastructure is not using. I.E. if the infrastructure is to support 100 projects, then select VLAN range outside of that value such as 200-299. All physical infrastructure (and openstack) that handles tenant networks must support this range.
** xfreerdp
  - As of version 1.2.1 it fails to connect frequently and when it does it does not does maintain a stable connection.
  -  xfreerdp /v:computername.lan.local /u:username@lan.local /p:\!foo /g:tsgateway.lan /gd:lan /gu:username /gp:\!foo /gateway-usage-method:direct /cert-ignore +auto-reconnect
* TODO
  - [X] Install Cron    
  - [2/4] Things to Schedual with Cron
    - [X] Schedual Zpool scrub
    - [ ] Email reports of storage pool capacity monthly
    - [X] Schedual zfs snapshots
    - [ ] Schedual last snapshot of the night to be compressed, encrypted, and sent to remote storage.    
  - [X] Make a snapshot
  - [X] Enable compression
  - [ ] Do I need PAM? If not
    - [ ] If I need it then configure it.
    - [ ] If I don't, then remove it.
  - [ ] Address WARNING: 'portageq envvar PORTDIR' is deprecated. Use any of 'get_repos, get_repo_path, repos_config' instead.
  - [ ] Learn about prodigy-mode for services
  - [ ] Learn about company-statistics for company-mode
* JOTTED
- Kernel to look into:
  - [CONFIG_USELIB] :: do I need this for hardened?
    - apparently not!
  - 3.24.24-gentoo > General setup > RCU Subsystem: Build-Forced no-CBs CPUs :: What is this? It is not available in hardened.
    - This has to do with configuring the RCU callbacks for specific cpus.It seems that this optino has been removed and that all cpus are no-CBs cpus.
  - CONFIG_INTEGRITY
    - IMA: Integrity Measurement Module
    - EVM: Extended Verification Module
    - Right now this is only for dev purposes. It is not stable.
  - Is it advisable to install kprobes and eBPF on a production server?
    - CONFIG_BPF_SYSCALL
  - CONFIG_ZPOOL
    - What is zbud, zsmalloc
  - I was not able to find all the kernel config options. Must review PaX kernel config options more closely.
- How can I resolve the issue that I am having with pinentry?? How should I build ncurses so that it plays nicely?? I have tried building it with tinfo with no luck. I have also tried building ncurses with .
  - This seems to be related: https://lists.gnu.org/archive/html/tramp-devel/2008-02/msg00025.html
  - Found these USE flags for sys-libs/ncurses-5.9-r99:                                                                                                                                    

$ equery u ncurses
[ Legend : U - final flag setting for installation]
[        : I - package is installed with flag     ]
[ Colors : set, unset                             ]
 * Found these USE flags for sys-libs/ncurses-5.9-r99:
 U I
 - - ada         : Add bindings for the ADA programming language
 + + cxx         : Build support for C++ (bindings, extra libraries, code generation, ...)
 - - gpm         : Add support for sys-libs/gpm (Console-based mouse driver)
 - - static-libs : Build static versions of dynamic libraries as well
 - - tinfo       : Build curses library (libncurses) sep from the low-level terminfo library (libtinfo) -- usually needed only for binary packages -- but it
                   is binary compatible in either mode 
 + + unicode     : Add support for Unicode


$ equery u pinentry
[ Legend : U - final flag setting for installation]
[        : I - package is installed with flag     ]
[ Colors : set, unset                             ]
 * Found these USE flags for app-crypt/pinentry-0.9.5:
 U I
 - - caps          : Use Linux capabilities library to control privilege
 - - clipboard     : Enable clipboard integration
 + + emacs         : Add support for GNU Emacs
 - - gnome-keyring : Enable support for storing passwords via gnome-keyring
 - - gtk           : Add support for x11-libs/gtk+ (The GIMP Toolkit)
 + + ncurses       : Add ncurses support (console display library)
 - - qt4           : Add support for the Qt GUI/Application Toolkit version 4.x
 - - static        : !!do not set this during bootstrap!! Causes binaries to be statically linked instead of dynamically
