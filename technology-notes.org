* Node
- process.env.NODE_ENV :: environment variable for configuring what environment node should run in i.e. dev, prod etc.
** npm
   - node package-manager
   - npm help <command> :: brings up help documentation on the specified command
** Node Modules
*** Bower
  - Install: $npm install -g bower
  - Help:
    - $ bower help
    - help on specific command : $ bower help command_name
  - Bower Lookup:
   Lets you see the git uri for a specific package
    - $bower lookup lodash
  - Bower Install: 
    - current version: $ bower install lodash
    - specific version: $ bower install lodash#2.2.1
    - save package to bower.json dependencies: $ bower install lodash --save
    - install package to dev only: $ bower install lodash --save-dev
    - install from cache (offline mode): $ bower install lodash -o
    - install from local repository: $ bower install relative-path/ProjectDirectoryName
    - install regular dependencies: $ bower install --production
    - install to named folder: $ bower install named_dir=lodash
  - Bower Uninstall: 
    - $ bower uninstall lodash
    - $ Remove from dependencies: bower uninstall lodash --save 
    - $ Remove from dev dependencies: bower uninstall lodash --save-dev
  - Bower Package Info: $ bower info lodash
  - Bower Update:
    - Update all packages: $bower update
    - Update single package: Just use Bower Install. It will update already installed packages.
  - Bower List:
    - list already installed packages: $bower list
  - Prune:
    Removes all packages that are not indicated in the bower.json file or are not sub-dependencies of existing dependencies.
    - $ bower prune
  - Bower Registery Search:
    - find all packages with the specified word: $bower search lodash
    - find package by keyword: go to the bower site http://bower.io/search/
  - Bower Init:
    - Used to create the bower json file: $ bower init
  - Bower RC:
    - Create a file called .bowerrc that contains a json with a single property named "directory"
      {
        "directory": "js/lib"
      }
      This specifies where bower should install packages.
    - Have bower install to more than one directory by creating more than one init point. So you could have a sub directory called test and throw a bower.json and .bowerrc file in there for managing dependencies in directory test/js/lib.
  - Bower Cache
    - List what is in the cache: $bower cache list
    - Clean the cache: $bower cache clean
  - Bower register
    To register a repository
    - $ git register https://github.com/TrevorWilsonSS/[reponame]
    - It will then ask if it can register the package with bower.herokuapp.com.
*** nodemon
    - Run a node application reloads modified files on the fly.
    - usage :: $ nodemon main-file.js
*** Stylus
    - A CSS preprocessor
    - styl :: file extension
*** Jade
  - The view engine used by express applications
  - jade :: file extension
  - doctype :: specifies that the jade file will contain HTML5
  - // :: a single line comment
*** toastr
*** ExpressJS
*** Morgan
    - Http request logger middleware
*** Body-parser
    - Body parser middleware
*** Mongoose
  - Fascilitates implementation of mongo-db in node applications
  - makes implementation of mongo much easier.
  - works off of schemas. Since mongo-db is a schema list document database, implementing schemas with mongoose is often seen as a bad idea.
    
* Angular JS
** Isolate Scope
- Local Scope Properties: Used to allow information flow between isolate and parent scope. Note that the alternate name option applies to all local scope properties, but is only illustrated with the @ below.
  - @: one-way binding of string values
    - directive usage:
      scope: {
    name: '@'
    value: '@someOtherAttrName'
    }
    - consumer usage:
    <div my-isolate-scope-with-name name: '{{customer.name}}' someOtherAttrName='{{constomer.value}}'></div>
  - =: two-way binding of objects
    - directive usage:
   scope: {
   customer: '='    
   },
    template: '<ul><li ng-repeat="prop in customer">{{prop}}</li></ul>
    - consumer usage:
      <div my-isolate-scope-with-model customer="customer"></div>
  - &: function binding for call-backs
    - directive usage:
      scope{
      action:'&'
      }
    - consumer usage:
      <div my-isolate-scope-with-function action="doStuff()" />
* Mongo-db
  - A schema list document database
  - No schema to define.
  - No relationship between collections of objects.
  - Objects can be flat or structured.
  - Two documents in same collection can be different from each other since no schema governs the collection.
    + Scalability
      - Single document write scope. Documents live in a collection, but updating a document occurs one at a time.
      - No need to extend locks accross collections because there are no relationships to enforce.
      - Eventual consistency. Mongo does not lock accross multiple mongo servers. A repleca set in mongo contains a single server that will handle all writes and a collection of secondary servers that will be replecated to. There is a lag of time from when a write occurs in the Primary DB to when the value is made observable by others by being replicated in a secondary db; hence, eventual consistancy.
      - Can choose consistancy model: 
        - Can choose to wait for primary write server to persist data
        - To wait for all replica servers to sync with the primary server following the write.
        - To wait for a majority of replica servers to sync with the primary server following the write.
        - Choose to hand over document to primary and not care wether it persisted or not.
      - Capped Collections:
        - Fixed size :: no time to allocate space
        - Auto override all documents
    + Mongod
      - The daemon.
      - Default Port: 27017
      - mongod help :: help documentation for commandline options
    + Mongos
      - The sharding server.
  - Mongo Client
    - mongo :: starts the client
    - help :: brings up client help documentation
    - exit :: quits the client
    - show dbs :: lists existing dbs
    - db :: shows the current database
    - use foo :: switches to database few and creates it if it does not exist.
    - db.getMongo() :: returns host and port for that server instance.
  - Replica Sets
    - Advantages :: scalability and automatic recovery.
    - Types :: Primary, Secondary, Arbiter
    - Primary
      - One and only primary instance. 
    - Secondary
      - Readonly
      - one to many
      - Data is replicated from primary. Gaurantees eventual consistancy.
      - If Primary database fails, one of the seconary databases will take over and become the primary. This is descided by an election.
      - Nothing special happens if a secondary db fails. If one secondary fails and there are others than no big deal. Haveing multiple Secondaries protects against single server failure.
    - Arbiter
      - sole purpose is to break ties on primary db elections.
      - is not a database. It contains no data.
    - Minimal replica set :: Primary DB, 1 Secondary DB, 0 or 1 Arbiters
    - Dev can run a replica set on a single machine. Production should run each mongo server per machine
    - Creation of single machine replica set:
      - Each mongo server requires its own db directory. i.e. db{1,2,3}
      - Each mongo server must run on a different port
      - mongod --dbpath ./db{n} --port unique_port_num --replSet "<replicaSetName>"
** TODO Install Mongo-db Minimal ReplicaSet
   - mongod -f "e:\dev\experiments\MultiVision\conf\mongod1.conf" --replSet "Experiments" --install
   - Windows instructions to get the replica set running as a service.
* jquery
* git
** Steps to create a new Github Rep
 1. Login to github and create the repository
 2. Copy the ssh path to the new repository
 3. Follow this command pattern:
    - $ mkdir myProj
    - $ cd myProj
    - $ git init
    - touch README.md
    - touch .gitignore
    - git add -A
    - git commit -m "my first checkin"
    - git remote add origin [paste copied git repo uri here]
    - git push -u origin master :: //MUST BE IN INTERACTIVE SHELL TO PROVIDE CREDENTIALS
    
** Steps to tag and push a new release
 1. cd myProj
 2. command pattern:
    - git tag 0.0.1 :: Should be the same version as entered in bower.json "version" property
    - git push --tags
 3. Now visit github, click on the project and then click on the release link to view the release.

* Linux 
** Commands :: remember to always check the man pages.
  - lsblk :: list block devices.
  - lddtree --help :: there are no man pages for this command
  - lddtree --copy-to-tree=/source/path /target/path :: from app-misc/pax-utils USE="python"
  - find /usr/portage -name '*.ebuild' -o -name '*.eclass' | xargs grep MAKE_CONF_VARIABLE :: finds all package references to MAKE_CONF_VARIABLE
  - lspci | grep -i vga :: detect video controller
  - numactl --hardware :: check to see if the hardware has numa support. This will return the number of numa nodes.
*** Detect Motherboard, Bios and CPU
   - dmidecode -t 4 | grep ID :: The CPU ID
   - dmidecode -t 0 :: Bios info
   - dmidecode -t 4 :: Processor info
   - dmidecode -t 11 :: Original Equipment Manufacturer (OEM) info
** I/O Stream Numbers
   | Handle | Name   | Description     |
   |      0 | stdin  | Standard input  |
   |      1 | stdout | Standard output |
   |      2 | stderr | Standard error  |
   - =$ program-name 2> error.log= :: Redirect standard error stream to a file
   - =$ program-name &>file= :: Redirect the standard error (stderr) and stdout to file
     =$ program-name > file-name 2>&1= :: Alternate redirect the standard error (stderr) and stdout to file

** Telly Type Terminals TTYs
   - == Ctrl-Alt-Fx == :: switch to TTYx
   - == $ chvt x == :: switch to TTYx via command line. Good for ssh.
** Boot Process
  1. Boot loader loads Linux.
  2. Linux assumes control of the system.
  3. Linux prepares its memory structures and drivers
  4. Hands control to Init.
  5. Init makes sure that at the end of the boot process, all necessary services are running and the user is able to log in.
  6. Init launches udev daemon which will further load up and prepare the system based on the detected devices.
  7. Udev mounts the remaining file systems waiting to be mounted.
  8. Udev starts the remaining services waiting to be started.
      
** Initramfs
*** The Initial RAM File System
  - Based on tmpfs
  - Because it is a size-flexible, in-memory lightweight file system it does not use a seperate block device so no caching was done. It does not have the overhead of an entire file system.
  - Contains the tools and scripts needed to mount the file systems before the init binary on the real root file system is called.
    - The tools can be the decryption abstraction layers (for encrypted file systems), logical volume mangers, software raid, bluetooth driver based file system loaders, etc.
  - All files, tools, libraries, configuration settings (if applicable), etc are put into a cpio archive.
*** From creation to execution
  1. The cpio archive is compressed using gzip and stored in the /boot partition along side the linux kernel.
  2. The boot loader will let the linux kernel know where the cpio archive is at boot time so that the kernel can load the initramfs.
  3. The Linux kernel will create a tmpfs file system, extract the contents of the cpio archive into it, and then launch the init script located in the root of the tmpfs file system.
  4. The init script will then perform what ever tasks are necessary to ensure that it will be able to mount the real root file system.
    - It may have to decrypt the real root file system, other vital file systems, and mount them among possibly other things depending on what is needed.
  5. The init script from the initramfs will then switch the root towards the real root file system
  6. Lastly the initramfs init script will call /sbin/init (the init script on the real root file system)
  7. The boot process will continue as normal.
** Profiling and Instrumentation
   - eBPF :: extended berkly packet filtering
     - Reference: http://www.brendangregg.com/blog/2015-05-15/ebpf-one-small-step.html
   - Kprobes
** dcron
   - /etc/crontab is the system crontab
   - uses crontab in conjunction with conrbase to run scripts in /etc/cron.{daily,hourly,weekly,monthly}
   - == # crontab /etc/crontab == :: Run everytime changes are made to the system crontab.
   - == # crontab -l == :: display a list of cron jobs
     - jobs schedualed in system crontab may not show up in this list
   - It is not necessary to use the system crontab simply by never running:
     - == # crontab /etc/crontab ==
     - == # sed -i -e "s/^/#/" /etc/crontab == :: to be extra careful you can comment all lines out in /etc/crontab:
   - Schedualing Jobs
     - == # crontab -e == :: edit crontab
     - == # crontab -d [user] == :: delete crontab. If no user is supplied then it deletes the current user's crontab.
     - == # crontab file == :: new crontab
     - Each lineitem in a crontab has the following fields:
       | Minutes | Hours  | Day of Month | Month  | Day of week |
       | (0-59)  | (0-23) | (1-31)       | (1-12) | (0-7)       |
       - Monday is day 1, Sunday is day 0.
       - Days of week and months can be specified by 3 letter abbreviations or numbers.
       - Each field can specify a range or a comma seperated list. i.e. 1-5, mon-fri or 1,3,4.
       - Ranges can have a step i.e. 1-5/2 = 1,3,5 where 2 is the step to increment by.
       - Regarding Day of Month and Day of week. If * is used for one, then the other takes precendence. If * is used for both, this means every day.
   - Schedualing jobs with the system crontab
     - == # sudo crontab /etc/crontab == :: to replace root's current crontab
     - Then simply drop the scripts into /etc/con.{daily,hourly,weekly,monthly}
** Gentoo
*** Use Variables
    - If a use variable has a * by it that means that it changed since the last build.
*** Hardened Profile
**** PIC (Position Independent Code)
     - Functions and data are accessed through an indirect table called the Global Offset Table (GOT).
     - The purpose of indirect addressing is to fascillitate the access of functions and data independently of the corresponding load address. Only the symbols in the text segment exported in the GOT need updating at run-time deending on the current load address of the various shared libraries in the address space of the running process.
     - Similarly, procedure calls to globally defined functions are redirected through the "Procedure Linkage Table" (PLT) residing in the data segment of the core image. This avoids runtime modifications of the text segment.
     - The Linker-editor allocates the GOT and PLT when combining the PIC object files into an image for mapping into the process address space.
     - The Linker-editor collects all symbols that may be needed by the run-time link-editor and stores these along with the image's text and data bits.
     - Objects compiled as PIC allow the OS to load the object at any address in preperation for execution with slight overhead.
     - The libtool builds PIC objects for use in shared libraries and non-PIC objects for use in static libraries. PIC compilation is required for objects in a shared library.
     - libtool compiles PIC objects with '*.lo' extension and non-PIC objects with '*.o' extension.
     - In practice PIC objects can be linked into static archive and often non-PIC objects can be similarly linked into shared archives both with execution and load speed overhead.
     - If the shared object is built from code that is not PIC then the text segment will usually require a large number of relocations to be performed at runtime. The system overhead from the run-time linker required to handle this can cause serious performance degradation.
     - =# readelf -d foo= :: If the output contains a TEXTREL entry then text relocations exist.
**** PaX
     - Purpose is to protect against a class of exploits that give an attacker arbitrary read / write access to the attacked task's address space. These exploits include buffer and heap overflows and similar attacks. PaX is the first line of defense offered by Hardened Gentoo.
     - Implements the least privilege protections for memory pages. i.e computer programs should only  be allowed to do what they have to do in order to be able to execute properly and nothing more.
     - The exploit techniques that PaX defends against include:
       1. Introduce / execute arbitrary code
       2. execute existing code out of original program order
       3. execute existing code in original program order with arbitrary data
     - References:
       - Gentoo Hardened Introduction :: https://wiki.gentoo.org/wiki/Hardened/Introduction_to_Hardened_Gentoo#Technologies_Offered
       - Project site :: http://pax.grsecurity.net/
       - Quickstart :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart
     - Adds security enhancement to the area between both kernel and userland.
     - Patch to the kernel that provides hardening in the following ways:
       1. Judicious enforcement of non-executable memory
       2. Address Space Layout Randomization (ASLR)
          - Compiling with Position Independent Executable (PIE) allows ASLR to randomaize even the base address.
       3. Miscellaneous hardening on stack and memory handling
          - Erases stack frame when returning from a system call
          - refusing to dereference user-land pointers in some context
          - detecting overflows of certain reference counters
          - correcting overflows of some integer counters
          - enforcing the size on copies between kernel and user land
          - providing extra entropy
     - PaX Modes
       - SOFTMODE
         - PaX protection will not be enforced by default for those features which can be turned on or off at runtime.
         - The "permit by default" mode.
         - The user must explicitly mark executables to enforce PaX protection.
       - non-SOFTMODE
         - PaX protections are immediately activated.
         - The "forbid by default" mode.
         - The user must explicitly mark binaries to relax PaX protection selectively.
     - PaX Configurable Features
       - Enforce non-executable pages :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Enforce_non-executable_pages
       - Enhanced Address Space Layout Randomization (ASLR) :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Enhanced_Address_Space_Layout_Randomization_.28ASLR.29
       - Miscellaneous Memory Protection :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Miscellaneous_Memory_Protection
     - PaX patches support three ways of doing PaX markings:
       1. EI_PAX
          - This option is nolonger supported
          - Places PaX flags in bytes 14 and 15 of the e_ident field of an ELF objects header.
       2. PT_PAX
          - Places the flags in an ELF object's program header called PAX_FLAGS.
          - Flags are in the body of the object and so if the object is moved or copied the flags are also.
          - The object must have the PAX_FLAGS program header to work. Most Linux distributions do not build their executables and libraries with this program header.
       3. XATTR_PAX
          - This is the preferred approach.
          - PaX flags are palced in the file system's extended attributes.
          - Does not modify the ELF object.
          - The file system and utilities used to copy, move and archive files must support xattrs.
            - Must support user.pax.* namespace in which the PaX flags are placed.
            - Do not enable the entire user.* namespace because it may open attackvectors.
            - Must support security.*, trusted.* namespaces.
     - Building a PaX Kernel
       - Reference :: https://wiki.gentoo.org/wiki/Hardened/PaX_Quickstart#Building_a_PaX_Kernel

*** ZFS 
**** Zpool Administration
    - References:
      - https://wiki.gentoo.org/wiki/ZFS/Features :: Detailed list of features
      - https://wiki.gentoo.org/wiki/ZFS :: Gentoo wiki guide
      - https://pthree.org/2012/12/04/zfs-administration-part-i-vdevs/ :: Administraiton Guide
      - http://docs.oracle.com/cd/E19253-01/819-5461/ :: Official Oracle Docs
    - ARC :: Adaptive Replacement Cache
    - ARC page replacement agolrithm is used instead of the Last Recently used page replacement algorithm.
    - Minumum and Maximum memory usage allocated to ARC varies based on system memory.
      - Default Min :: 1/32 of all memory, or 64 MB, whichever is more.
      - Default Max :: the larger of 1/2 of system memory or 64 MB.
      - Linux accounts for memory used by arc differently than memory used by the page cache. Memory used by ARC is included under "used" not "cached" in the output used by the 'free' program. This can give the impression that ARC will use all of system memory if given opportunity.
      - ARC memory usage is tunable via zfs_arc_min and zfs_arc_max. These properties may be set in 3 ways:
        1. at runtime.
           - =root # echo 536870912 >> /sys/module/zfs/parameters/zfs_arc_max=
           - Changes through sysfs do not persist across boots.
           - The value in sysfs will be 0 when the value has not been manually configured.
           - The current setting can be viewed by looking at c_max in /proc/spl/kstat/zfs/arcstats
        2. via /etc/modprobe.d/zfs.conf
           - =root # "options zfs zfs_arc_max=536870912" >> /etc/modprobe.d/zfs.conf=
        3. Kernel command line
           - =zfs.zfs_arc_max=536870912=
      - Zpool Version Update
        - When sys-fs/zfs is updated likely the version of ZFS has been incremented. The status of zpools will indicate a warning that a newer version is available and that the zpools can be upgraded.
        - =root # zpool upgrade -v= :: Display current version on zpool
        - =root # zpool upgrade zfs_test= :: upgrade the version of zpool zfs_test.
        - =root # zpool upgrade -a= :: upgrade the version of all zpools in the system.
***** Virtual Devices (VDEVs)
     - A meta-device representing one or more physical devices.
     - 7 types of VDEVs:
       - Disk (default) :: The physical drives in your system.
       - File :: The absolute path of pre-allocated files/images.
       - Mirror :: Standard software RAID-1 mirror.
       - Spare :: Hard drives marked as a "hot spare" for ZFS software RAID.
       - Cache :: Device used for a level 2 adaptive read cache (L2ARC).
       - Log :: A seperate log (SLOG) called the "ZFS Intent Log" or ZIL.
     - VDEVs are dynamically striped.
     - Caveats
       - Devices cannot be removed from a VDEV
       - RAID-(n) is faster than RAID-(n+1)
       - Hot spares are not dynamically added unless configured to.
       - A zpool will not dynamically resize when larger disks fill the pool unless you enable the setting (off by default) BEFORE the first disk replacement.
       - Will know about "advanced format" 4K sector drives iif the drive reports such.
       - Duplication is EXTREMELY EXPENSIVE, will cause performance degredation if not enough RAM is available.
       - Duplication is pool-wide, not local to the filesystem.
       - Compression is EXTREMLY CHEAP on the CPU, yet it is disabled by default.
       - ZFS suffers a great deal from fragmentation. Full ZPOOLS will experience performance degredation
     - Creation
       - A Simple pool
         #+BEGIN_SRC sh
           # zpool create tank sde sdf
           # zpool status tank
 pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  sde       ONLINE       0     0     0
	  sdf       ONLINE       0     0     0
	  sdg       ONLINE       0     0     0
	  sdh       ONLINE       0     0     0

errors: No known data errors
         #+END_SRC
       - A simple mirrored zpool
         #+BEGIN_SRC sh
           # zpool create tank mirror sde sdf sdg sdh
           # zpool status tank
 pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    sde     ONLINE       0     0     0
	    sdf     ONLINE       0     0     0
	    sdg     ONLINE       0     0     0
	    sdh     ONLINE       0     0     0

errors: No known data errors
         #+END_SRC
       - Nested VDEVs
         #+BEGIN_SRC sh
           # zpool create tank mirror sde sdf mirror sdg sdh
           # zpool status
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    sde     ONLINE       0     0     0
	    sdf     ONLINE       0     0     0
	  mirror-1  ONLINE       0     0     0
	    sdg     ONLINE       0     0     0
	    sdh     ONLINE       0     0     0

errors: No known data errors
         
         #+END_SRC
       - File VDEVs (useful for experiments)
         - When creating cannot use relative paths.
         - The image file must be preallocated, not sparse or thin provisioned.
         #+BEGIN_SRC sh
         # for i in {1..4}; do dd if=/dev/zero of=/tmp/file$i bs=1G count=4 &> /dev/null; done
         # zpool create tank /tmp/file1 /tmp/file2 /tmp/file3 /tmp/file4
         # zpool status tank
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME          STATE     READ WRITE CKSUM
	tank          ONLINE       0     0     0
	  /tmp/file1  ONLINE       0     0     0
	  /tmp/file2  ONLINE       0     0     0
	  /tmp/file3  ONLINE       0     0     0
	  /tmp/file4  ONLINE       0     0     0

errors: No known data errors
         
         #+END_SRC
       - Hybrid pools
         - "tank" pool is composed of "mirror-0" and "mirror-1" VDEVs for long-term persistent storage. 
         - Neither the "logs" pool nor the "cache" pool are long-term storage for the pool, thus creating a "hybrid pool" steup.
         #+BEGIN_SRC sh
           # zpool create tank mirror /tmp/file1 /tmp/file2 mirror /tmp/file3 /tmp/file4 log mirror sde sdf cache sdg sdh
           # zpool status tank
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME            STATE     READ WRITE CKSUM
	tank            ONLINE       0     0     0
	  mirror-0      ONLINE       0     0     0
	    /tmp/file1  ONLINE       0     0     0
	    /tmp/file2  ONLINE       0     0     0
	  mirror-1      ONLINE       0     0     0
	    /tmp/file3  ONLINE       0     0     0
	    /tmp/file4  ONLINE       0     0     0
	logs
	  mirror-2      ONLINE       0     0     0
	    sde         ONLINE       0     0     0
	    sdf         ONLINE       0     0     0
	cache
	  sdg           ONLINE       0     0     0
	  sdh           ONLINE       0     0     0

errors: No known data errors
         
         #+END_SRC
         - In practice, use the device id found in /dev/disk/by-id when identifying the devices for the "logs" and "cache" pools. They may be assigned different device names from one boot to another, unlike devices in the main pool.
***** RAIDZ
      - References:
        - http://en.wikipedia.org/wiki/RAID
      - Disk Striping
        - References
          - http://en.wikipedia.org/wiki/Data_striping
        - The process of segmenting a body of logically sequential data into data blocks so that consecutive data blocks are spread accross multiple storage devices.
        - Storage systems vary in the way striping is performed. Data may be stripped at the byte, block or partition level, and may be stripped accross all or some of the disks in a cluster.
        - Main advantage is higher performance. By spreading data accross multiple devices that can be accessed concurrently, total throughput is increased.
        - Balances I/O load accross an array of disks.
      - Standard Parity RAID (i.e. RAID-5)
        - References:
          - http://blog.open-e.com/how-does-raid-5-work/
        - Consists of block level striping with distributed parity. The stripe width is thus statically set at creation.
        - Minumum of 3 disks. Data is stripped accross two disks. A pairity bit is calculated such that the XOR of all three stripes in the set calculate to zero. The parity bit is then written to the chosen 3rd disk.
        - No single disk is dedicated to parity. Two disk are chosen to be striped and the third is chosen for parity such that the distribution of parity bits is even accross all drives.
        - Resilant against a failure of any one disk. The data on the failed disk can be recalculated via the remaining disks and thus restored.
        - RAID-5 write hole
          - Caused by inturrupted destaging of writes to disk, such as a power failure.
          - Solutions to the RAID-5 write hole are either slow (software based) or expensive (hardware based).
          - RAID-5 has fallen out of favor as a result.
        - If the data being written to the strip is less than the stripe-size, much time is wasted reading the data on the rest of the stripe to ensure that the parity satisfies the constraint that the XOR of all three stripes in the set calculate to zero.
          - Thus data is read and written that is not pertinent to the application doing the work.
          - Expensive NVRAM hardware RAID cards can hide the latency from the end user.
        - The RAID-5 write hole and the performance impact of writing data smaller than the stripe size to disk motivated the ZFS team to re-think parity-based RAID.
      - ZFS RAIDZ
        - Stripe width dynamically allocated.
          - Every block stransactionally flushed to disk is its own stripe width.
          - Every RAIDZ write is a full stripe write.
          - The parity bit is flushed with the stripe simultaneously. This completely eliminates the possability of a write hole.
            - In the event of a power failure, either the latest data was flushed to disk or it wasn't, but the disks will not be inconsistent.
          - Cannot calculate parity simply by the rule -- every disk XORs to zero because the stripe size is dynamic with respect to the size of the datablock being written to disk.
            - ZFS metadata is used to determine the RAIDZ geometry on every read.
            - Reading file system metadata to contruct the RAID stripe means reading live running data only, not dead inpertinent or unallocated data.
            - No need for expensive NVRAM to buffer writes or for battery backup to protect against write hole.
      - Self-healing RAID
        - ZFS can detect silent errors and fix them on the fly (Not possible if RAID and filesystem are seperate)
          - When an application requests data ZFS constructs the stripe and compares each block against a default checksum in the metadata. If the read stripe does not match the checksum, ZFS finds the bad block, reads the parity and fixes it through combinatorial reconstruction and returns good data to the application.
          - If the stripe width is longer than the disks in the array and there is a disk failure there would not be enough data in the parity for combinatorial reconstruction, thus ZFS can mirror data in the stripe to prevent this from happening. :: Bad wording. I re-worded the original to try to make sense of it. Is this what was meant???
      - RAIDZ-{1,2,3}
        - RAIDZ-n, where n is the number of parity bits distributed accross all the disks in the array.
        - Stripe width is variable. Disk Array Size < Stripe width || Disk Array Size == Stripe Width || Stripe Width < Disk Array Size.
        - RAIDZ-n allows for n disk failures.
        - RAIDZ-n requires Disk Array Size = n + 2.
        - RAIDZ-n capacity = Disk Array Size * Capacity of smallest disk - n parity storage
      - Hybrid RAIDZ
        - A stripe of multiple RAIDZ VDEVS.
        - Increase performance at the cost of available disk storage.
        - Stripe width is variable within each nested RAIDZ VDEV.
        - Each RAIDZ level follows the same rules outlined in RAIDZ-{1,2,3} above.
          - A stripe of 3 RAIDZ-n VDEVS can suffer a total of 3*n disks. n per VDEV.
      - RAIDZ Benchmark :: Recommended to use the tool IOZone 3 to benchmark and stress the array.
        - Mirrors always outperform RAIDZ levels.
        - performs(RAIDZ-n) > performs(RAIDZ-{n-1})
        - More parity bits mean longer read / write times.
      
***** ZFS Intent Log (ZIL and SLOG)
      - References:
       https://pthree.org/2012/12/06/zfs-administration-part-iii-the-zfs-intent-log/
      - The Zil and the Seperate Intent Log (SLOG) log what is currently in system memory so that the system may recover in the event of a power loss. This can be setup at anytime so I am going to skip it for now.
***** The Adjustable Replacement Cache (ARC)
      - The purpose of this is to cache data for quick retrieval to increase performance. This is a good idea, but can be added later so I am going to skip this for now.
***** Exporting and Importing Zpools
      - Exporting Storage Pools
        - When exporting the following happens:
          - Causes the kernel to flush all pending data to disk
          - Writes data to the disk acknowledging that the export was done
          - Removes all knowledge that the storage pool existed in the system
        - Exporting the storage pool is necessary before importing the storage pool into a new system. Also, unwritten data may not have been flushed to disk.
        - == root # zpool export tank ==
          - Will attempt to unmount all ZFS datasets as well as the pool
          - If the zpool refuses to export you can add -f to force the export.
      - Importing Storage Pools
        - Once the drives have been physically installed they may be imported.
        - == root # zpool import tank == :: Imports the zpool tank
        - Once imported it is a good idea to check the status of the zpool
        - == root # zpool status tank
          - A status of ONLINE means that everything is healthy
          - Status FAULTED :: means one or more drives appear faulty to the system
          - Refer to documentation for troubleshooting status codes
        - Import multiple zpools by listing them after the import command delimited by white space, or by passing the -a flag for importing all known zpools.
          - == root # zpool import tank1 tank2 tank3
          - == root # zpool import -a
      - Recovering a Destroyed Pool
        - Destroying a pool does not wipe the data on the disks. The pool can be discovered.
        - == (server A) root # zpool destroy tank == :: Does not wipe data
          == (server B) root # zpool import -D == :: Lists destroyed pools.
          - Run the import command again specifying the pool name to bring it fully online.
          - If more than one storage pool is found with the same name then the unique identifier of the storage pool must be used as the argument to import to bring it fully online.
      - Upgrading Storage Pools
        - Once a zpool has been upgraded servers running older versions of zpool will not be able to import it.
        - There is no way to downgrade.
        - == root # zpool upgrade -v == :: Outputs the version of ZFS pool that the system is currently running. Display a list of supported zpool versions and some descriptions for each.
        - == root # zpool upgrade -a == :: Upgrades the zpool and enables all supported features. See man page to upgrade to a specific version and feature set.
        - On shutdown the zfs init script may just unmount the pools and not export them in which case the zpools would not be able to be imported to by another system. If this is the case you will have to explicitly export the zpools first.

***** Scrub and Resilver
     - Standard Validation
       - integrity checking tools require the disks to be offline.
       - The filesystem knows nothing about the underlying data structures such as LVM or RAID.
       - Software RAID has no idea what disk contains good or bad data, so either could be served (silent data errors).
       - Nothing can be done about silent data errors.
     - ZFS Scrubbing
       - Scrubbing the disk is used to detect and correct silent data errors.
       - Scrubbing disks can be done on a live running system with no downtime.
       - The scrub involves checking every block in the storage pool against its known checksum using the "fletcher4" (default) 256-bit algorithm.
       - Must be performed explicitly.
       - Recommended that scrubbing is performed on a regularly scheduled interval (good job for chron).
       - == root # scrub tank == :: performs a scrub of the pool.
       - Can check status during a scrub.
       - Scrubs impact performance of disks.
       - == root # scrub -s tank == Stops a scrub in progress.
       - == 0 2 * * 0 /sbin/zpool scrub tank == crontab to perform scrub every Sunday at 2 in the morning.
     - Self Healing Data
       - Requires redundancy. i.e. mirror
       - Will not only detect corrupt data on a scrub, but will correct the bad blocks if good data exists on a different disk.
       - I think that this zfs can self heal in response to realtime interaction with an application and is not limited to explicit scrubs. (verify this)
     - Resilvering Data
       - References:
         - http://docs.oracle.com/cd/E19082-01/817-2271/gbbvf/index.html :: regarding damaged devices with ZFS
       - Same concept as rebuilding or resyncing data onto a new disk in the array.
       - With Software RAID there is no distinction between which blocks are live and which are not. The rebuild starts at the begining of the disk and does not stop until it reaches the end of the disk.
       - ZFS knows about the RAID structure and has a smarter algorithm for rebuilding the data.
         - Does not sync the free disk blocks, only live blocks. If storage pool is only partially filled this can save significant time.
         - In ZFS the process of rebuilding, resyncing or reconstructing is kown is Resilvering.
         - If zpool status is "DEGRADED" then a disk needs to be replaced. Identify the disk that needs to be replaced with teh following command
           #+BEGIN_SRC 
           # for i in a b c d e f g; do echo -n "/dev/sd$i: "; hdparm -I /dev/sd$i | awk '/Serial Number/ {print $3}'; done
/dev/sda: OCZ-9724MG8BII8G3255
/dev/sdb: OCZ-69ZO5475MT43KNTU
/dev/sdc: WD-WCAPD3307153
/dev/sdd: JP2940HD0K9RJC
/dev/sde: /dev/sde: No such file or directory
/dev/sdf: JP2940HD0SB8RC
/dev/sdg: S1D1C3WR
           #+END_SRC
           The above example shows that /dev/sde needs to be replaced since the command could not find the device.
         - After identifying the dead disk go to the storage array and find which serial number was not printed. The one that was not printed needs to be replaced. Pull the disk and replace it with a new one.
         - Restart the system and see if /dev/sde is repopulated by running the command again.
         - Then issue the following command to replace sde with the new disk at /dev/sde (note the new disk may not be /dev/sde in which case the command would be slightly different to reflect that)
           == root # zpool replace tank sde sde == Rebuilds the data blocks on the new disk until it is in a completely healthy state. Check the status to know when the process has completed.
     - Identifying Pool Problems
       - == zpool status -x == :: the x flag only displays the status of pools that are exhibiting errors or are unavailable.
       - The zpool status fields:
         + pool- The name of the pool.
         + state- The current health of the pool. This information refers only to the ability of the pool to provide the necessary replication level.
         + status- A description of what is wrong with the pool. This field is omitted if no problems are found.
         + action- A recommended action for repairing the errors. This field is an abbreviated form directing the user to one of the following sections. This field is omitted if no problems are found.
         + see- A reference to a knowledge article containing detailed repair information. Online articles are updated more often than this guide can be updated, and should always be referenced for the most up-to-date repair procedures. This field is omitted if no problems are found.
         + scrub- Identifies the current status of a scrub operation, which might include the date and time that the last scrub was completed, a scrub in progress, or if no scrubbing was requested.
         + errors- Identifies known data errors or the absence of known data errors.
         + config- Describes the configuration layout of the devices comprising the pool, as well as their state and any errors generated from the devices. The state can be one of the following: ONLINE, FAULTED, DEGRADED, UNAVAILABLE, or OFFLINE. If the state is anything but ONLINE, the fault tolerance of the pool has been compromised.
       - The columns in the status output, "READ", "WRITE" and "CHKSUM" are defined as follows:

         + NAME- The name of each VDEV in the pool, presented in a nested order.
         + STATE- The state of each VDEV in the pool. The state can be any of the states found in "config" above.
         + READ- I/O errors occurred while issuing a read request.
         + WRITE- I/O errors occurred while issuing a write request.
         + CHKSUM- Checksum errors. The device returned corrupted data as the result of a read request.
**** Zpool Properties
     - As with many file systems available for GNU/Linux, ZFS has various flags that can be set to tune the file systems behavior.
     - Zpool properties can be set to modify the behavior of both the pool and the datasets that the pool contains.
     - Some properties are read-only.
     - See man zpool for details on properties.
     - Get properties
       - When properties are retrieved the following fields are displayed
         | name     | Name of storage pool                         |
         | property | property name                                |
         | value    | property value                               |
         | source   | Property source, either 'default' or 'local' |
       - == root # zpool get -p "all" == :: Retrieves a list of all properties 
       - == root # zpool get -p PropName1,PropName2 == :: retrieves property details for the specified list of properties delimited by a comma with no white space i.e. PropName1,PropName2
     - Set properties
       - Properties that require a string argument do not have an easy way to get the value back to a default value. With other property types, if you try to set the property to an invalid argument then an error will print to the screen displaying available valid values, but it will not indicate which was the default value.
       - If the source column has "default" then the setting has not been user-defined, if it is "local", then it was user-defined.
       - == root # zpool set comment="Contact admins@example.com" tank == :: sets the comment property. This will also change the source from "default" to "local"
       - == root # zpool create -o ashift=12 tank raid1 sda sdb == :: an example of setting properties at zpool creation time.
     - Properties apply to the whole pool, consequently ZFS datasets inherit property settings from the pool.
     - Some properties that are set on the dataset also apply to the whole pool
     - Setting properties only applies to data moving forward and never backward.
     - Properties are not retroactive i.e. if you replace a drive with a larger drive and then set autoexpand property after it has been installed it will not autoexpand. It will instead appear as a smaller drive.
     - 
**** Best Practices & Caveats
     - The idea is to optimize space efficiency, performance and ensure maximum data integrity
     - Best Practices
       - Only run ZFS on 64-bit kernels :: It has 64-bit specific code
       - Install ZFS only on systems with lots of RAM :: ZFS will use 1/2 of the available RAM for the ARC
       - Use ECC RAM when possible for scrubbing data in registers and maintaining data consistency :: The ARC is an actual read-only data cache of valuable data in RAM
       - Use whole disks rather than partitions :: ZFS can make better use of the on-disk cache as a result. If you must use partitions, backup the partition table, and take care when reinstalling data into the other partitions, so you don't corrupt the data in your pool
       - Keep each VDEV in a storage pool the same size :: If VDEVs vary in size, ZFS will favor the larger VDEV, which could lead to performance bottlenecks.
       - Use redundancy when possible :: ZFS can and will want to correct data errors that exist in the pool. You cannot fix these errors if you do not have redundant good copy elsewhere in the pool. Mirrors and RAID-Z levels accomplish this.
       - Consider using RAIDZ-2 or RAIDZ-3 over RAIDZ-1 :: This will increase the probability that you have fully resilvered the necessary data before too many disks failures have occured.
       - Perform regular (at least weekly) backups of the full storage pool :: It's not a backup, unless you have multiple copies. Just because you have redundant disks, does not ensure live running data in the event of a power failure, hardware failure or disconnected cables.
       - Use host spares to quickly recover from a damaged device. Set the "autoreplace" property to on for the pool.
       - Consider using hybrid storage pool with fast SSDs or NVRAM drives :: Using FAST SLOG and L2ARC can greatly improve performance.
       - If using a hybrid storage pool with multiple devices, mirror the SLOG and stripe the L2ARC.
       - If using a hybrid storage pool, and partitioning the fast SSD or NVRAM drive, unless you know you will need it, 1GB is likely sufficient for your SLOG :: Use the rest of the SSD or NVRAM drive for the L2ARC. The more storage for the L2ARC, the better.
       - Keep pool capacity under 80% for best performance :: Due to the copy-on-write nature of ZFS, the filesystem gets hevily fragmented. Email reports of capacity at least monthly.
       - If possible, scrub consumer-grade SATA and SCISI disks weekly and enterprise-grade SAS and FC disks montly :: If not possible then scrub as frequently as possible.
       - Set "autoexpand" to on :: so you can expand the storage pool automatically after all disks in the pool have been replaced with larger ones.
       - Always export your storage pool when moving the disks from one physical system to another.
       - When considering performance, know that for sequential writes, mirrors will always outperform RAID-Z levels. For sequential reads, RAID-Z levels will perform more slowly than mmirrors on smaller data blocks and faster on larger data blocks. For random reads and writes, mirrors and RAID-Z seem to perform in similar manners. Striped mirrors will outperform mirros and RAID-Z in both sequential, and ramdom reads and writes.
       - Compression is disabled by default :: this doesn't make much sense with today's hardware. ZFS compression is extremly cheap, extremly fast, and barely adds any latency to the reads and writes. Also your data will consume less space.
     - Caveats
       - Your VDEVs determine the IOPS of the storage, and the slowest disk in the VDEV will determine the IOPS for the entire VDEV.
       - ZFS uses 1/64 of the available raw storage for metadata. so if you purchased a 1 TB drive, the actual raw size is 976 GiB. After ZFS uses it, you will have 961 GiB of available space. The "zfs list" command will show an accurate representation of your available storage. Plan your storage keeping this in mind.
       - ZFS wants to control the whole block stack. It checksums, resilvers live data instead of full disks, self-heals corrupted blocks, and a number of other unique features. If using a RAID card, make sure to configure it as a true JBOD (or "passthrough mode"), so ZFS can control the disks. If you can't do this with your RAID card, don't use it. Best to use a real HBA.
       - Do not use other volume management software beneath ZFS. ZFS will perform better, and ensure greater data integrity, if it has control of the whole block device stack. As such, avoid using dm-crypt, mdadm or LVM beneath ZFS.
       - Do not share a SLOG or L2ARC Device accross pools. Each pool should have its own physical device, not a logical drive, as is the case with some PCI-Express SSD cards. Use the full card for one pool, and a different physical card for another pool. If you share a physical device, you will create race conditions, and could end up with corrupted data.
       - Do not share a single storage pool accress different servers. ZFS is not a clustered filesystem. Use GlusterFS, Ceph, Lustre or some other clustered filesystem on top of the pool if you wish to have a shared storage backend.
       - Other than a spare, SLOG and L2ARC in your hybrid pool, do not mix VDEVs in a single pool. If one VDEV is a mirror, all VDEVs should be mirrors. If one VDEV is a RAIDZ-1, all VDEVs should be RAIDZ-1. Unless of course, you know what you are doing, and are willing to accpet the consequences. ZFS attempts to balance the data across VDEVs. Having a VDEV of a different redundancy can lead to performance issues and space efficiency concerns and make it very difficult to recover in the event of a failure.
       - Do not mix disk sizes or speeds in a single VDEV. Do mix fabrication dates, however, to prevent mass drive failure.
       - Do not mix disk sizes or speeds in your storage pool at all.
       - Do not mix disk counts accross VDEVs. If one VDEV uses 4 drives, all VDEVs should use 4 drives.
       - Do not put all of the drives from a single controller in one VDEV. Plan your storage, such that if a controller fails, it affects only the number of disks necessary to keep the data online.
       - When using advanced format disks, you must set the ashift value to 12 at pool creation. It cannot be changed after the fact. Use "zpool create -o ashift=12 tank mirror sda sdb" as an example
       - Hot spare disks will not be added to the VDEV to replace a failed drive by default. You MUST enable this feature. set the autoreplace feature to on. Use 'zpool set autoreplace=on tank' as an example.
       - The storage pool will not auto resize itself when all smaller drives in the pool have been replaced by larger ones. You must enable this feature, and you must enable it before replacing the first disk. Use "zpool set autoexpand=on tank' as an example.
       - ZFS does not restripe data in a VDEV nor across multiple VDEVs. Typically, when adding a new device to a RAID array, the RAID controller will rebuild the data, by creating a new stripe width. This will free up some space on the drives in the pool, as it copies data to the new disk. ZFS has no such mechanism. Eventually, over time, the disks will balance out due to the writes, but even a scrub will not rebuild the stripe width.
       - You cannot shrink a zpool, only grow it. This means you cannot remove VDEVs from a storage pool.
       - You can only remove drives from mirrored VDEV using the "zpool detach' command. You can replace drives with another drive in RAIDZ and mirror VDEVs however.
       - Do not create a storage pool of files or ZVOLs from an existing zpool. Race conditions will be present, and you will end up with corrupted data. Always keep multiple pools sperate.
       - The Linux kernel may not assign a drive the same drive letter at every  boot. Thus, you should use the /dev/disk/by-id/ convention for your SLOG and L2ARC. If you don't, your zpool devices could end up as a SLOG device, which would in turn clobber your ZFS data.
       - Don't create massive storage pools "just because you can". Even though ZFS can create 78-bit storage pool sizes, that doesn't mean you need to create one.
       - Don't put production directly into the zpool. Use ZFS datasets instead.
       - Don't commit production data to file VDEVs. Only use file VDEVs for testing scripts or learning the ins and outs of ZFS.
         
***** Zpool Properties
      - As with many file systems available for GNU/Linux, ZFS has various flags that can be set to tune the file systems behavior.
      - Zpool properties can be set to modify the behavior of both the pool and the datasets that the pool contains.
      - Some properties are read-only.
      - See man zpool for details on properties.
      - Get properties
        - When properties are retrieved the following fields are displayed
          | name     | Name of storage pool                         |
          | property | property name                                |
          | value    | property value                               |
          | source   | Property source, either 'default' or 'local' |
        - == root # zpool get -p "all" == :: Retrieves a list of all properties 
        - == root # zpool get -p PropName1,PropName2 == :: retrieves property details for the specified list of properties delimited by a comma with no white space i.e. PropName1,PropName2
      - Set properties
        - Properties that require a string argument do not have an easy way to get the value back to a default value. With other property types, if you try to set the property to an invalid argument then an error will print to the screen displaying available valid values, but it will not indicate which was the default value.
        - If the source column has "default" then the setting has not been user-defined, if it is "local", then it was user-defined.
        - == root # zpool set comment="Contact admins@example.com" tank == :: sets the comment property. This will also change the source from "default" to "local"
        - == root # zpool create -o ashift=12 tank raid1 sda sdb == :: an example of setting properties at zpool creation time.
      - Properties apply to the whole pool, consequently ZFS datasets inherit property settings from the pool.
      - Some properties that are set on the dataset also apply to the whole pool
      - Setting properties only applies to data moving forward and never backward.
      - Properties are not retroactive i.e. if you replace a drive with a larger drive and then set autoexpand property after it has been installed it will not autoexpand. It will instead appear as a smaller drive.

***** Best Practices & Caveats
      - The idea is to optimize space efficiency, performance and ensure maximum data integrity
      - Best Practices
        - Only run ZFS on 64-bit kernels :: It has 64-bit specific code
        - Install ZFS only on systems with lots of RAM :: ZFS will use 1/2 of the available RAM for the ARC
        - Use ECC RAM when possible for scrubbing data in registers and maintaining data consistency :: The ARC is an actual read-only data cache of valuable data in RAM
        - Use whole disks rather than partitions :: ZFS can make better use of the on-disk cache as a result. If you must use partitions, backup the partition table, and take care when reinstalling data into the other partitions, so you don't corrupt the data in your pool
        - Keep each VDEV in a storage pool the same size :: If VDEVs vary in size, ZFS will favor the larger VDEV, which could lead to performance bottlenecks.
        - Use redundancy when possible :: ZFS can and will want to correct data errors that exist in the pool. You cannot fix these errors if you do not have redundant good copy elsewhere in the pool. Mirrors and RAID-Z levels accomplish this.
        - Consider using RAIDZ-2 or RAIDZ-3 over RAIDZ-1 :: This will increase the probability that you have fully resilvered the necessary data before too many disks failures have occured.
        - Perform regular (at least weekly) backups of the full storage pool :: It's not a backup, unless you have multiple copies. Just because you have redundant disks, does not ensure live running data in the event of a power failure, hardware failure or disconnected cables.
        - Use host spares to quickly recover from a damaged device. Set the "autoreplace" property to on for the pool.
        - Consider using hybrid storage pool with fast SSDs or NVRAM drives :: Using FAST SLOG and L2ARC can greatly improve performance.
        - If using a hybrid storage pool with multiple devices, mirror the SLOG and stripe the L2ARC.
        - If using a hybrid storage pool, and partitioning the fast SSD or NVRAM drive, unless you know you will need it, 1GB is likely sufficient for your SLOG :: Use the rest of the SSD or NVRAM drive for the L2ARC. The more storage for the L2ARC, the better.
        - Keep pool capacity under 80% for best performance :: Due to the copy-on-write nature of ZFS, the filesystem gets hevily fragmented. Email reports of capacity at least monthly.
        - If possible, scrub consumer-grade SATA and SCISI disks weekly and enterprise-grade SAS and FC disks montly :: If not possible then scrub as frequently as possible.
        - Set "autoexpand" to on :: so you can expand the storage pool automatically after all disks in the pool have been replaced with larger ones.
        - Always export your storage pool when moving the disks from one physical system to another.
        - When considering performance, know that for sequential writes, mirrors will always outperform RAID-Z levels. For sequential reads, RAID-Z levels will perform more slowly than mmirrors on smaller data blocks and faster on larger data blocks. For random reads and writes, mirrors and RAID-Z seem to perform in similar manners. Striped mirrors will outperform mirros and RAID-Z in both sequential, and ramdom reads and writes.
        - Compression is disabled by default :: this doesn't make much sense with today's hardware. ZFS compression is extremly cheap, extremly fast, and barely adds any latency to the reads and writes. Also your data will consume less space.
      - Caveats
        - Your VDEVs determine the IOPS of the storage, and the slowest disk in the VDEV will determine the IOPS for the entire VDEV.
        - ZFS uses 1/64 of the available raw storage for metadata. so if you purchased a 1 TB drive, the actual raw size is 976 GiB. After ZFS uses it, you will have 961 GiB of available space. The "zfs list" command will show an accurate representation of your available storage. Plan your storage keeping this in mind.
        - ZFS wants to control the whole block stack. It checksums, resilvers live data instead of full disks, self-heals corrupted blocks, and a number of other unique features. If using a RAID card, make sure to configure it as a true JBOD (or "passthrough mode"), so ZFS can control the disks. If you can't do this with your RAID card, don't use it. Best to use a real HBA.
        - Do not use other volume management software beneath ZFS. ZFS will perform better, and ensure greater data integrity, if it has control of the whole block device stack. As such, avoid using dm-crypt, mdadm or LVM beneath ZFS.
        - Do not share a SLOG or L2ARC Device accross pools. Each pool should have its own physical device, not a logical drive, as is the case with some PCI-Express SSD cards. Use the full card for one pool, and a different physical card for another pool. If you share a physical device, you will create race conditions, and could end up with corrupted data.
        - Do not share a single storage pool accress different servers. ZFS is not a clustered filesystem. Use GlusterFS, Ceph, Lustre or some other clustered filesystem on top of the pool if you wish to have a shared storage backend.
        - Other than a spare, SLOG and L2ARC in your hybrid pool, do not mix VDEVs in a single pool. If one VDEV is a mirror, all VDEVs should be mirrors. If one VDEV is a RAIDZ-1, all VDEVs should be RAIDZ-1. Unless of course, you know what you are doing, and are willing to accpet the consequences. ZFS attempts to balance the data across VDEVs. Having a VDEV of a different redundancy can lead to performance issues and space efficiency concerns and make it very difficult to recover in the event of a failure.
        - Do not mix disk sizes or speeds in a single VDEV. Do mix fabrication dates, however, to prevent mass drive failure.
        - Do not mix disk sizes or speeds in your storage pool at all.
        - Do not mix disk counts accross VDEVs. If one VDEV uses 4 drives, all VDEVs should use 4 drives.
        - Do not put all of the drives from a single controller in one VDEV. Plan your storage, such that if a controller fails, it affects only the number of disks necessary to keep the data online.
        - When using advanced format disks, you must set the ashift value to 12 at pool creation. It cannot be changed after the fact. Use "zpool create -o ashift=12 tank mirror sda sdb" as an example
        - Hot spare disks will not be added to the VDEV to replace a failed drive by default. You MUST enable this feature. set the autoreplace feature to on. Use 'zpool set autoreplace=on tank' as an example.
        - The storage pool will not auto resize itself when all smaller drives in the pool have been replaced by larger ones. You must enable this feature, and you must enable it before replacing the first disk. Use "zpool set autoexpand=on tank' as an example.
        - ZFS does not restripe data in a VDEV nor across multiple VDEVs. Typically, when adding a new device to a RAID array, the RAID controller will rebuild the data, by creating a new stripe width. This will free up some space on the drives in the pool, as it copies data to the new disk. ZFS has no such mechanism. Eventually, over time, the disks will balance out due to the writes, but even a scrub will not rebuild the stripe width.
        - You cannot shrink a zpool, only grow it. This means you cannot remove VDEVs from a storage pool.
        - You can only remove drives from mirrored VDEV using the "zpool detach' command. You can replace drives with another drive in RAIDZ and mirror VDEVs however.
        - Do not create a storage pool of files or ZVOLs from an existing zpool. Race conditions will be present, and you will end up with corrupted data. Always keep multiple pools sperate.
        - The Linux kernel may not assign a drive the same drive letter at every  boot. Thus, you should use the /dev/disk/by-id/ convention for your SLOG and L2ARC. If you don't, your zpool devices could end up as a SLOG device, which would in turn clobber your ZFS data.
        - Don't create massive storage pools "just because you can". Even though ZFS can create 78-bit storage pool sizes, that doesn't mean you need to create one.
        - Don't put production directly into the zpool. Use ZFS datasets instead.
        - Don't commit production data to file VDEVs. Only use file VDEVs for testing scripts or learning the ins and outs of ZFS.
          
**** ZFS Administration
***** Merkle Trees
      - Cryptographic hash trees invented by Ralph Merkle
      - Each datablock is hashed with SHA-256 algorithm.
      - Used by ZFS to verify integrity of entire file system.
      - The purpose of cryptographically hasing each block is to ensure data integrity. We can check to see if the hash block matches its parent hash block. If it does, then the data was not corrupt, otherwise the block is corrupt.
      - The tree is binary (but Merkle trees do not need to be binary)
      - Each parent node is the SHA-256-bit hash of concatonating its children hash blocks together.
      - The root of the tree (also called super block, super node or uber block) is a SHA-256 bit hash of its concatenated children.
      - The uber block is used to verify the integrity of the entire Merkle tree.
      - If a data block changes, all of the parent hash blocks change all of the way up to and including the uber block.
      - When a storage pool is scrubbed ZFS verifies every SHA-256 hash in the Merkle tree to make sure there is no corrupted data, or inconsistencies.
      - If the pool has redundancy and a corrupted data block is found, ZFS will look elsewhere in the pool for a good datablock at the same location to fix the corrupted one and then reverify the SHA-256 hash in the Merkle tree.
***** Copy-on-write (COW)
      - A data storage technique:
        1. The data-block targeted for modification is copied
        2. The copy is modified
        3. The file system pointers are updated to point to the new data-block location
        4. The original data-block is released for use by the applicaiton
      - A consequence is that the underlying data becomes severly fragmented.
      - COW Fascilitates taking snapshots
        - This is such a huge bennefit that it outways the consequence of fragmentation
        - As the file system is being written to COW makes its way through to the end of the disk. The original copy of each data-block ramains for a long time following a write.
        - A snapshot is treated as a first class filesystem.
        - If a data-block is overriden following a snapshot, then its copy will be placed in the snapshot filesystem; otherwise, it remains where it is marked as free for use by the applicaiton.
        - This is possible because a snapshot is a copy of the hash tree at that exact moment. Thus, unless the snapshotted data-blocks are overritten, they take up hardly any space.
      - When a data-block is updated, the hash tree is also updated starting with the child-block hash and moving up the tree through all of its ancestor node hashes until the super-node hash is finally updated.
      - Fragmentation can have a serious performance impact. To help mitigate fragmentation ZFS marks slabs of disk space for data-block copies so that they stay close to one another.
        - Typically file systems write ata in 4 KiB blocks where as ZFS writes data in 128 KiB blocks.
        - This minimizes fragmentation by an order of 32.
        - The SLAB allocator will allocate a SLAB and then partition that into 128 KiB blocks.
        - ZFS delays syncing data to disk every 5 seconds. All data remaining is flushed to disk every 30 seconds. That means a lot of data flushes to disk at once into the SLAB keeping data-blocks close and therefore increasing the chance that related data is kept together in the same SLAB. Thus reducing fragmentation.
      - COW is also used by VM images such as Qemu and many other file systems.
***** Creating File Systems
      - ZFS datasets are file systems
      - Storage pools are not meant to store data directly. Instead, multiple ZFS datasets sharing the same storage pool store the data.
      - By default each dataset has full access to the entire storage pool.
      - As files are placed in the dataset, then the pool marks that storage as unavailable to all datasets. Therefore, there is no need to create logical volumes of limited size.
      - Datasets can have quotas that limit there size.
      - The big advantage is that there is no need to worry about preallocated block devices because ZFS manages the entire stack and  therefore understands how much data has been occupied and how much is available.
      - Create a dataset
        - == root # zfs create pool-name/dataset-name == :: Creates a data-set named data-set in the pool named pool-namen
      - Mounting DataSets
        - Datasets are not exportable block devices by default so there is nothing to directly mount; hence, there is nothing in /etc/fstab for persistence accross reboots.
        - Instead, the storage pool can be imported and zfs mount can be run for each dataset.
        - == root # zfs mount /<pool-name>/<dataset-name> == :: <dataset-name has bee mounted to the pool named <pool-name>
        - == root # zfs unmount /<pool-name>/<dataset-name> == :: <dataset-name has been umounted from <pool-name>
        - == root # zfs set mountpoint=/mnt/<data-setname> <pool-name>/<data-setname> == :: Sets the data-set mountpoint property
      - Nested Datasets
        - Nested datasets makes it possible to change properties to one dataset without affecting the other. i.e. setting compression on /var/log, but not on the parent data-set /var. 
        - == root # zfs create /<pool-name>/<data-setname>/log == :: creates a nested dataset named "log"
      - Destroy a dataset
        - Frees up the blocks used by the dataset.
        - Cannot be reverted without a previous snapshot.
        - == root # zfs destroy /<pool-name>/<data-setname> ==
      - Rename a dataset
        - == root # zfs rename /<pool-name>/<data-setname1> /<pool-name>/<data-setname2> ==
***** Compression and Duplication
      - Compression
        - is transparent. Zfs handles compression/decompression on the fly.
        - is enabled / disabled per dataset. The default is disabled.
        - Supported algorithms
          - LZJB
            - Default
            - A Lempel-Ziv algorithm written by author of ZFS Jeff Bonwick
            - Designed to be fast with tight compression ratios.
          - LZ4
            - New to ZFS. Offers tighter compression ratios than LZJB.
            - Is now preferred over LZJB.
          - ZLE
            - Super fast with very light compression ratios
          - Gzip{1-9}
            - Where 1 is as fast as possible and 9 is as compressed as possible.
            - The default is 6. This is the default for gnu linux.
        - Not retroactive. Applys only to newly commited or modified data.
        - There is next to know performance impact for enabling it so it is recommended to enable compression on all datasets.
        - Commands:
          - zfs set compression=lz4 tank/log :: To set compression to lz4 for dataset /tank/log
      - Duplication
        - Anotherway to save disk space in conjunction with compression
          - 3 types
            1. file
               - most performant and least costly on system resources.
               - each file is hashed with crypto hash agorithm. Files with matching hashes are only stored once in disk and metadata is used to lookup those files
               - saves significant disk space.
               - Drawback: if a single byte changes the entire modified file must be written to disk since it will no longer share the same hash as any other file in the file system (unless it does of course :-). For a large file that could have a serious performance impact.
            2. block
               - zfs uses this only
               - block duplication shares all the same blocks in a file, minus blocks that are different. Only unique blocks are written to disk and shared blocks can be referenced in RAM.
               - More efficient than byte duplication and more flexible than file duplication.
               - Drawback: Requires lots of RAM to keep track of shared blocks
               - Since file systems read and write data in block segments this option makes the most sense for modern filesystems.
            3. byte
               - most expensive because anchor points must be kept to determine the regions where duplicate and unique bytes start and end.
               - Works well for storage where file may be stored multiple times, but not necessarily under the same blocks. i.e. mail attachments.
        - Shared blocks are stored in a duplication table in RAM. The more shared blocks that exist on disk, the larger the duplication table becomes. If the system runs out of RAM then swap is used (performance hit).
        - It is turned off by default.
        - zfs get -p dedup :: shows if duplication is on/off on each filesystem, volume, snapshot
        - Because duplication requires a large amount of RAM as the size of the file system grows we will not consider it further.
***** Snapshots
      - First class read-only file system. It is a mirrored copy of the state of the file system at the time the snapshot was taken.
      - Can keep 2^64 snapshots in a pool.
      - Don't require additional backing store. They use the same storage pool as the rest of the data.
      - A snapshot is a readonly copy of the Merkle tree. Even the snapshots properties are readonly.
      - Snapshot creation is very fast.
      - As the current state diverges from the snapshot, the snapshot will begin to store the data of the things that change. So more the current state diverges from the snapshot the more space the snapshot consumes.
      - Creating Snapshots
        - Two types:
          - pool snapshots
            - pool@snapshot-name :: snapshot name syntax
            - == zfs snapshot tank@tuesday == :: takes a snapshot of the tank pool named tuesday
          - dataset snapshots
            - pool/dataset@snapshot-name :: snapshot name syntax
            - == zfs snapshot tank/test@tuesday == :: Takes a snapshot of the tank/test dataset named tuesday
      - Listing Snapshots
        - snapshots are stored in a hidden directory named .zfs. It is hidden even from ls -a command, but you can still cd to .zfs even though you cannot list it with ls -a.
        - The .zfs directory can be made visible by changing the snapdir property on the dataset.
          - == zfs set snapdir=visible tank test == makes the .zfs directory visible to the ls -a command. Valid values are hidden and visible.
        - == zfs list -t snapshot == :: will list the snapshots even if the snapdir is set to hidden.
        - == zfs list -r -t snapshot tank == :: this will list only snapshots under tank recursivly.
      - Destroying Snapshots
        - == zfs destroy tank/test@tuesday == :: Similar to destroying a storage pool or a dataset.
        - A snapshot of a dataset is a child file system of a dataset, therefore the dataset cannot be destroyed until all the snapshots (and nested datasets) of the dataset have been destroyed.
      - Renaming Snapshots
        - Must be renamed in the storage pool and ZFS dataset from which they were created.
        - == zfs rename tank/test@tuesday tank/test@tuesday-19:15 ==
      - Rolling Back to a Snapshot
        - Discards changes between the snapshot and the current state.
        - Can only roll back to most recent snapshot.
        - To roll back to a snapshot earlier than the most recent snapshot you must destroy all snapshots inbetween :: Yuck!
          #+BEGIN_SRC shell
          # zfs rollback tank/test@tuesday
          cannot rollback to 'tank/test@tuesday': more recent snapshots exist
          use '-r' to force deletion of the following snapshots:
          tank/test@wednesday
          tank/test@thursday
          #+END_SRC
        - The file system must be unmounted before the rollback can commence. :: This means downtime!
***** Clones
      - A writeable filesystem that was "upgraded" from a snapshot; hence, only snapshots can be cloned.
      - The clone depends on the snapshot to exist since it is a copy of the Merkle tree of the snapshot.
        - All clones must be destroyed before the snapshot that those clones depend on can be destroyed.
      - Clones do not take up additional space from their snapshot until they start to diverge at which point they only store the changes.
      - Creating a Clone
        - the clone does not need to reside in the same dataset as the snapshot, but it does need to reside in the same storage pool.
        - To clone tank/test@tuesday snapshot and name it tank/tuesday:
          #+BEGIN_SRC shell
          # zfs clone tank/test@tuesday tank/tuesday
          # dd if=/dev/zero of=/tank/tuesday/random.img bs=1M count=100
          # zfs list -r tank
          NAME           USED  AVAIL  REFER  MOUNTPOINT
          tank           161M  2.78G  44.9K  /tank
          tank/test     37.1M  2.78G  37.1M  /tank/test
          tank/tuesday   124M  2.78G   161M  /tank/tuesday
          #+END_SRC
      - Destroying a Clone
        - Cannot destroy a snapshot until all dependant clones have been destroyed.
        - == zfs destroy tank/tuesday == :: destroys the clone named tuesday in the storage pool tank. Just like destroying any other dataset.
      - Schedualing Snapshots
        - Snapshots are memory and time cheap, so it is recommended to take plenty of snapshots
        - consider creating a cron job to take snapshots hourly/daily/weekly/monthly maybe even by the minut.
        - A possible schedual actually used by Time Slider:
          - 15 min :: keeping 4 snapshots
          - hourly :: keeping 24 snapshots
          - daily :: keeping 31 snapshots
          - weekly :: keeping 7 snapshots
          - montly :: keeping 12 snapshots
      - Since both snapshots and clones are cheap, take advantage of them..
        - clones are useful to test deploying virtual machines, or development environments cloned from production environments for example.
        - When you are done with a clone you can easily destroy it without affecting the parent dataset.
***** ZFS Send / Receive
      - ZFS Send
        - Sending involves first taking a snapshot of the data and then sending the snapshot to ensure the data remains consistant over the duration of the transfer.
          - Consequently there is no need to take the file system offline to make a backup.
        - By default the data is sent to a single file that can be then moved like any other file.
        - Sending a snapshot creates an output stream that must be directed.
          - basic send to file
            #+BEGIN_SRC shell
            # zfs snapshot tank/test@tuesday
            # zfs send tank/test@tuesday > /backup/test-tuesday.img
            #+END_SRC
          - send to encrypted file. Note that xz compresses the snapshot before encryption.
            #+BEGIN_SRC shell
            # zfs snapshot tank/test@tuesday
            # zfs send tank/test@tuesday | xz | openssl en -aes-256-cbc -a -salt > /backup/test-tuesday.img.xz.asc
            #+END_SRC
      - ZFS Receive
        - receive also works with streams.
        - receive the snapshot image into any storage pool and it will create the necessary dataset.
          - == zfs receive tank-test2 < /backup/test-tuesday.img ==
        - to recieve an encrypted and compressed snapshot we would do the following:
          - == openssl enc -d -aes-256-cbc -a -in /backup/test-tuesday.img.xz.asc | unxz | zfs receive tank/test2 ==
      - Combining Send and Recieve
        - == zfs send tank/test@tuesday | zfs receive pool/test == :: Send directly from snapshot to local storage pool
        - == zfs send tank/test@tuesday | ssh user@server@example.com "zfs receive pool/test == :: Send directly from local snapshot to remote storage pool
***** ZVOLs
      - A ZFS Volume that has been exported to the system as a block device.
      - Resides in a storage pool.
      - Takes advantage of the underlying things that ZFS offers (RAID, copy-on-write, scrubbing, duplication / compression etc).
      - Takes advantage of the ZIL and ARC.
      - ZVOLs are first class block devices and so you can do anything with them that you can do with any other block device.
      - Creating a ZFS Vol
        - uses the -V flag from zfs create command and requires a size.
          #+BEGIN_SRC shell
          # zfs create -V 1G tank/disk1
          # ls /dev/zvol/tank/disk1
          lrwxrwxrwx 1 root root 11 Dec 20 22:10 /dev/zvol/tank/disk1 -> ../../zd144
          # ls /dev/tank/disk1
          lrwxrwxrwx 1 root root 8 Dec 20 22:10 /dev/tank/disk1 -> ../zd144
          #+END_SRC
        - Nearly instantaneous regardless of size.
        - Compare creating a block device with gnu/linux from a file image and having /dev/loop0 represent the file. 
          - As with any other block device it can be formated and added to swap, but it is limited. 
          - By default there are only 8 loop back devices. This number can be changed of course.
          - In contrast with zfs you can create 2^64 ZVOLs.
          - Also, the gnu/linux way requires a pre-allocated image on top of the file system so there are three layers to manage. The block device, the file and the blocks on the file system.
          - In contrast ZVOL block devices are exported right off the storage pool just like any other dataset.
      - Swap on a ZVOL
        - create 1GB of swap on a ZVOL and add it to the kernel
          #+BEGIN_SRC shell
          # zfs create -V 1G tank/swap
          # mkswap /dev/zvol/tank/swap
          # swapon /dev/zvol/tank/swap          
          #+END_SRC
          - now running free will show the swap.
      - Ext4 on a ZVOL
        - it is possible to create a zvol, partition it and make multiple file systems on it.
        - The advantage is that you can enable compression, make snapshots, send the dataset to offsite back and do all the other things that zfs allows you to do with datasets that you can't do with those same file systems without zfs.
        - Here we will do just that:
          #+BEGIN_SRC shell
          # zfs create -V 100G tank/ext4
          # fdisk /dev/tank/ext4
          ( follow the prompts to create 2 partitions- the first 1 GB in size, the second to fill the rest )
          # fdisk -l /dev/tank/ext4

          Disk /dev/tank/ext4: 107.4 GB, 107374182400 bytes
          16 heads, 63 sectors/track, 208050 cylinders, total 209715200 sectors
          Units = sectors of 1 * 512 = 512 bytes
          Sector size (logical/physical): 512 bytes / 8192 bytes
          I/O size (minimum/optimal): 8192 bytes / 8192 bytes
          Disk identifier: 0x000a0d54

          Device Boot      Start         End      Blocks   Id  System
/dev/tank/ext4p1            2048     2099199     1048576   83  Linux
/dev/tank/ext4p2         2099200   209715199   103808000   83  Linux
          
          # mkfs.ext4 /dev/zd0p1
          # mkfs.ext4 /dev/zd0p2
          # mkdir /mnt/zd0p{1,2}
          # mount /dev/zd0p1 /mnt/zd0p1
          # mount /dev/zd0p2 /mnt/zd0p2
          #+END_SRC
        - Now we can enable compression, copy over some data, and take a snapshot
          #+BEGIN_SRC shell
           # zfs set compression=lzjb pool/ext4
           # tar -cf /mnt/zd0p1/files.tar /etc/
           # tar -cf /mnt/zd0p2/files.tar /etc /var/log/
           # zfs snapshot tank/ext4@001          
          #+END_SRC
      - ZVOL storage for VMs
        - It is common to use block devices as the backend storage for VMs.
        - You can attach the block device to a virtual machine and from its perspective the system will have a /dev/vda or /dev/sda depending on the setup.
        - The vm will get all of the bennefits that ZFS provides, such as snapshots, compression, deduplication, data integrity, drive redundancy, etc.
        - TODO: add snippet here of kvm configuration using block device.
      - caveat: ZFS is not a clustered file system. You cannot replicate zvols across a cluster.
***** ZFS Properties
      - datasets contain properties that can be retrieved and altered.
      - Many properties are read-only
      - There are lots of dataset properties.
      - properties can be inherited from parent datasets.
      - Custom property support.
      - ZFS Get
        - == # zfs get used,available,compressionratio tank/test == Retrieve multiple properties for a dataset via a comma seperated list.
        - == # zfs get all tank/test == Retrieve all of the properties for a dataset.
      - Inheritance
        - tank is a storage pool, but it is also a valid ZFS dataset. Any datasets nested under tank may inherit some properties.
        - We set the compression algorithm on the storage pool file system tank, so all nested datasets inherit this property from tank.
          #+BEGIN_SRC shell
          # zfs create -o compression=gzip tank/test/one
          # zfs get -r compression tank
          NAME           PROPERTY     VALUE     SOURCE
          tank           compression  lzjb      local
          tank/test      compression  lzjb      inherited from tank
          tank/test/one  compression  gzip      local
          #+END_SRC
        - zfs inherit command sets a properties value to be inherited from its parent.
          #+BEGIN_SRC shell
          # zfs inherit compression tank/test/one
          # zfs get -r compression tank
          NAME           PROPERTY     VALUE     SOURCE
          tank           compression  lzjb      local
          tank/test      compression  lzjb      inherited from tank
          tank/test/one  compression  lzjb      inherited from tank
          #+END_SRC
        - To set a parents property and have the children all inherit use the -r flag
          #+BEGIN_SRC shella
          # zfs set compression=gzip tank
          # zfs inherit -r compression tank/test
          # zfs get -r compression tank
          NAME           PROPERTY     VALUE     SOURCE
          tank           compression  gzip      local
          tank/test      compression  gzip      inherited from tank
          tank/test/one  compression  gzip      inherited from tank
          #+END_SRC
        - Caution: recursion can be dangerous. This next command turns off compression on all nested datasets of tank. This caution applies to datasets, volumes and snapshots. Anywhere recursion can be used.
          #+BEGIN_SRC shell
          # zfs inherit -r compression tank
          # zfs get -r compression tank
          NAME           PROPERTY     VALUE     SOURCE
          tank           compression  off       default
          tank/test      compression  off       default
          tank/test/one  compression  off       default
          #+END_SRC
      - User Dataset Properties
        - Motivation: To create custom properties for appliations designed specifically for ZFS. I am not making any, so I am not taking notes here.
      - Considerations:
        - Always read the man pages before altering properties.
        - Some dataset properties are not fully implemented on ZFS on Linux.
        - Some dataset properties apply to the whole pool, such as duplication.
        - Many properties only apply to newly written datap; hence, are not retroactive.
        - The parent storage pool is also a ZFS dataset, any child datasets will inherit non-default properties.
          - the same is true for nested datasets, snapshots and volumes.
***** Best Practices / Caveats
** xfreerdp
  - As of version 1.2.1 it fails to connect frequently and when it does it does not does maintain a stable connection.
  -  xfreerdp /v:2ua4041FTF.lan.local /u:trevor.wilson@lan.local /p:\!foo /g:tsgateway.lan /gd:lan /gu:trevor.wilson /gp:\!foo /gateway-usage-method:direct /cert-ignore +auto-reconnect
* TODO
  - [0/0] Install Cron    
  - [0/4] Things to Schedual with Cron
    - [ ] Schedual Zpool scrub
    - [ ] Email reports of storage pool capacity monthly
    - [ ] Schedual zfs snapshots
    - [ ] Schedual last snapshot of the night to be compressed, encrypted, and sent to remote storage.    
  - [ ] Make a snapshot
  - [ ] Enable compression

* JOTTED
- Kernel to look into:
  - [CONFIG_USELIB] :: do I need this for hardened?
    - apparently not!
  - 3.24.24-gentoo > General setup > RCU Subsystem: Build-Forced no-CBs CPUs :: What is this? It is not available in hardened.
    - This has to do with configuring the RCU callbacks for specific cpus.It seems that this optino has been removed and that all cpus are no-CBs cpus.
  - CONFIG_INTEGRITY
    - IMA: Integrity Measurement Module
    - EVM: Extended Verification Module
    - Right now this is only for dev purposes. It is not stable.
  - Is it advisable to install kprobes and eBPF on a production server?
    - CONFIG_BPF_SYSCALL
  - CONFIG_ZPOOL
    - What is zbud, zsmalloc
